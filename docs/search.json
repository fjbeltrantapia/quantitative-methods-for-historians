[
  {
    "objectID": "speeches.html",
    "href": "speeches.html",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "",
    "text": "This second case-study explores how computational methods help extracting information from unstructured texts. We are only going to introduce very basic tools that basically count words. There are however more sophisticated tools, so take this just a first taste into this topic."
  },
  {
    "objectID": "speeches.html#preliminary-steps",
    "href": "speeches.html#preliminary-steps",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "Preliminary steps",
    "text": "Preliminary steps\nAs explained in the previous section, we need to include some preliminary commands in our script so we (1) get rid of other objects that could be in the R environment from previous sessions, (2) set the working directory, (3) load (and install if necessary) the packages we plan to use, and (4) import the dataset. Apart from the tidyverse, we will also make use of the tidytext package that contains many of the functions to treat textual corpuses. Importin the .csv file containing the corpus into R involves using the command read_csv() which is part of the tidyverse.\n\n# Clear de \"Global Environment\"\nrm(list=ls()) \n\n# Sets the working directory\nsetwd(\"~/Documents/quants\") \n\n# Install/load packages\ninstall.packages(\"tidytext\")\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Importing the data\nspeeches &lt;- read_csv(\"data/state-of-the-union-texts.csv\")"
  },
  {
    "objectID": "speeches.html#inspecting-the-data",
    "href": "speeches.html#inspecting-the-data",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "Inspecting the data",
    "text": "Inspecting the data\nLet’s start by having a first look at the data itself typing the name of the object we just created (speeches). As shown below, the contents of the tidyverse have been nicely structured into a data frame containing 4 columns (different pieces of information) and 235 rows (one for each speech).\n\nspeeches\n\n# A tibble: 219 × 4\n   President          Year Title                              Text              \n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                              &lt;chr&gt;             \n 1 George Washington  1790 First State of the Union Address   \"['I embrace with…\n 2 George Washington  1790 Second State of the Union Address  \"['Fellow-Citizen…\n 3 George Washington  1791 Third State of the Union Address   \"['Fellow-Citizen…\n 4 George Washington  1792 Fourth State of the Union Address  \"['Fellow-Citizen…\n 5 George Washington  1793 Fifth State of the Union Address   \"['Fellow Citizen…\n 6 George Washington  1794 Sixth State of the Union Address   \"['Fellow Citizen…\n 7 George Washington  1795 Seventh State of the Union Address \"['Fellow Citizen…\n 8 George Washington  1796 Eighth State of the Union Address  \"['Fellow Citizen…\n 9 by John Adams      1797 First State of the Union Address   \"['I was for some…\n10 by John Adams      1798 Second State of the Union Address  \"['Gentlemen of t…\n# ℹ 209 more rows\n\n\nWe can use the functions we are already familiar with to continue exploring the data. We can see for instance that two speeches were delivered in 1790 (there is only one speech each year from then on), something we will need to take into account later.\n\nspeeches |&gt;\n  count(Year)\n\n# A tibble: 216 × 2\n    Year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  1790     2\n 2  1791     1\n 3  1792     1\n 4  1793     1\n 5  1794     1\n 6  1795     1\n 7  1796     1\n 8  1797     1\n 9  1798     1\n10  1799     1\n# ℹ 206 more rows\n\n\nLooking at the column President (and sorting it out by those with higher counts) indicates that\n\nspeeches |&gt; \n  count(President, sort = TRUE)\n\n# A tibble: 41 × 2\n   President                     n\n   &lt;chr&gt;                     &lt;int&gt;\n 1 Franklin Delano Roosevelt    12\n 2 Dwight D. Eisenhower          9\n 3 Andrew Jackson                8\n 4 Barack Obama                  8\n 5 Bill Clinton                  8\n 6 George W. Bush                8\n 7 George Washington             8\n 8 Grover Cleveland              8\n 9 Harry S. Truman               8\n10 James Monroe                  8\n# ℹ 31 more rows\n\n\nIt is also possible to have a look at the text of the speeches themselves. The code below for instance, takes the object data and prints the contents of fourth row of the field Text.\n\nspeeches$Text[4] \n\n[1] \"['Fellow-Citizens of the Senate and of the House of Representatives:', 'It is some abatement of the satisfaction with which I meet you on the present occasion that, in felicitating you on a continuance of the national prosperity generally, I am not able to add to it information that the Indian hostilities which have for some time past distressed our Northwestern frontier have terminated.', 'You will, I am persuaded, learn with no less concern than I communicate it that reiterated endeavors toward effecting a pacification have hitherto issued only in new and outrageous proofs of persevering hostility on the part of the tribes with whom we are in contest. An earnest desire to procure tranquillity to the frontier, to stop the further effusion of blood, to arrest the progress of expense, to forward the prevalent wish of the nation for peace has led to strenuous efforts through various channels to accomplish these desirable purposes; in making which efforts I consulted less my own anticipations of the event, or the scruples which some considerations were calculated to inspire, than the wish to find the object attainable, or if not attainable, to ascertain unequivocally that such is the case.', 'A detail of the measures which have been pursued and of their consequences, which will be laid before you, while it will confirm to you the want of success thus far, will, I trust, evince that means as proper and as efficacious as could have been devised have been employed. The issue of some of them, indeed, is still depending, but a favorable one, though not to be despaired of, is not promised by anything that has yet happened.', 'In the course of the attempts which have been made some valuable citizens have fallen victims to their zeal for the public service. A sanction commonly respected even among savages has been found in this instance insufficient to protect from massacre the emissaries of peace. It will, I presume, be duly considered whether the occasion does not call for an exercise of liberality toward the families of the deceased.', 'It must add to your concern to be informed that, besides the continuation of hostile appearances among the tribes north of the Ohio, some threatening symptoms have of late been revived among some of those south of it.', 'A part of the Cherokees, known by the name of Chickamaugas, inhabiting five villages on the Tennessee River, have long been in the practice of committing depredations on the neighboring settlements.', 'It was hoped that the treaty of Holston, made with the Cherokee Nation in July, 1791, would have prevented a repetition of such depredations; but the event has not answered this hope. The Chickamaugas, aided by some banditti of another tribe in their vicinity, have recently perpetrated wanton and unprovoked hostilities upon the citizens of the United States in that quarter. The information which has been received on this subject will be laid before you. Hitherto defensive precautions only have been strictly enjoined and observed.', 'It is not understood that any breach of treaty or aggression whatsoever on the part of the United States or their citizens is even alleged as a pretext for the spirit of hostility in this quarter.', 'I have reason to believe that every practicable exertion has been made (pursuant to the provision by law for that purpose) to be prepared for the alternative of a prosecution of the war in the event of a failure of pacific overtures. A large proportion of the troops authorized to be raised have been recruited, though the number is still incomplete, and pains have been taken to discipline and put them in condition for the particular kind of service to be performed. A delay of operations (besides being dictated by the measures which were pursuing toward a pacific termination of the war) has been in itself deemed preferable to immature efforts. A statement from the proper department with regard to the number of troops raised, and some other points which have been suggested, will afford more precise information as a guide to the legislative consultations, and among other things will enable Congress to judge whether some additional stimulus to the recruiting service may not be advisable.', 'In looking forward to the future expense of the operations which may be found inevitable I derive consolation from the information I receive that the product of the revenues for the present year is likely to supersede the necessity of additional burthens on the community for the service of the ensuing year. This, however, will be better ascertained in the course of the session, and it is proper to add that the information alluded to proceeds upon the supposition of no material extension of the spirit of hostility.', 'I can not dismiss the subject of Indian affairs without again recommending to your consideration the expediency of more adequate provision for giving energy to the laws throughout our interior frontier and for restraining the commission of outrages upon the Indians, without which all pacific plans must prove nugatory. To enable, by competent rewards, the employment of qualified and trusty persons to reside among them as agents would also contribute to the preservation of peace and good neighborhood. If in addition to these expedients an eligible plan could be devised for promoting civilization among the friendly tribes and for carrying on trade with them upon a scale equal to their wants and under regulations calculated to protect them from imposition and extortion, its influence in cementing their interest with ours could not but be considerable.', 'The prosperous state of our revenue has been intimated. This would be still more the case were it not for the impediments which in some places continue to embarrass the collection of the duties on spirits distilled within the United States. These impediments have lessened and are lessening in local extent, and, as applied to the community at large, the contentment with the law appears to be progressive.', 'But symptoms of increased opposition having lately manifested themselves in certain quarters, I judged a special interposition on my part proper and advisable, and under this impression have issued a proclamation warning against all unlawful combinations and proceedings having for their object or tending to obstruct the operation of the law in question, and announcing that all lawful ways and means would be strictly put in execution for bringing to justice the infractors thereof and securing obedience thereto.', 'Measures have also been taken for the prosecution of offenders, and Congress may be assured that nothing within constitutional and legal limits which may depend upon me shall be wanting to assert and maintain the just authority of the laws. In fulfilling this trust I shall count entirely upon the full cooperation of the other departments of the Government and upon the zealous support of all good citizens.', 'I can not forbear to bring again into the view of the Legislature the subject of a revision of the judiciary system. A representation from the judges of the Supreme Court, which will be laid before you, points out some of the inconveniences that are experienced. In the course of the execution of the laws considerations arise out of the structure of the system which in some cases tend to relax their efficacy. As connected with this subject, provisions to facilitate the taking of bail upon processes out of the courts of the United States and a supplementary definition of offenses against the Constitution and laws of the Union and of the punishment for such offenses will, it is presumed, be found worthy of particular attention.', 'Observations on the value of peace with other nations are unnecessary. It would be wise, however, by timely provisions to guard against those acts of our own citizens which might tend to disturb it, and to put ourselves in a condition to give that satisfaction to foreign nations which we may sometimes have occasion to require from them. I particularly recommend to your consideration the means of preventing those aggressions by our citizens on the territory of other nations, and other infractions of the law of nations, which, furnishing just subject of complaint, might endanger our peace with them; and, in general, the maintenance of a friendly intercourse with foreign powers will be presented to your attention by the expiration of the law for that purpose, which takes place, if not renewed, at the close of the present session.', 'In execution of the authority given by the Legislature measures have been taken for engaging some artists from abroad to aid in the establishment of our mint. Others have been employed at home. Provision has been made of the requisite buildings, and these are now putting into proper condition for the purposes of the establishment. There has also been a small beginning in the coinage of half dimes, the want of small coins in circulation calling the first attention to them.', 'The regulation of foreign coins in correspondency with the principles of our national coinage, as being essential to their due operation and to order in our money concerns, will, I doubt not, be resumed and completed.', 'It is represented that some provisions in the law which establishes the post office operate, in experiment, against the transmission of news papers to distant parts of the country. Should this, upon due inquiry, be found to be the fact, a full conviction of the importance of facilitating the circulation of political intelligence and information will, I doubt not, lead to the application of a remedy.', 'The adoption of a constitution for the State of Kentucky has been notified to me. The Legislature will share with me in the satisfaction which arises from an event interesting to the happiness of the part of the nation to which it relates and conducive to the general order.', 'It is proper likewise to inform you that since my last communication on the subject, and in further execution of the acts severally making provision for the public debt and for the reduction thereof, three new loans have been effected, each for 3,000,000 florins - one at Antwerp, at the annual interest of 4.5%, with an allowance of 4% in lieu of all charges, in the other 2 at Amsterdam, at the annual interest of 4%, with an allowance of 5.5% in one case and of 5% in the other in lieu of all charges. The rates of these loans and the circumstances under which they have been made are confirmations of the high state of our credit abroad.', 'Among the objects to which these funds have been directed to be applied, the payment of the debts due to certain foreign officers, according to the provision made during the last session, has been embraced.', '\\\\nGentlemen of the House of Representatives:', 'I entertain a strong hope that the state of the national finances is now sufficiently matured to enable you to enter upon a systematic and effectual arrangement for the regular redemption and discharge of the public debt, according to the right which has been reserved to the Government. No measure can be more desirable, whether viewed with an eye to its intrinsic importance or to the general sentiment and wish of the nation.', 'Provision is likewise requisite for the reimbursement of the loan which has been made of the Bank of the United States, pursuant to the eleventh section of the act by which it is incorporated. In fulfilling the public stipulations in this particular it is expected a valuable saving will be made.', 'Appropriations for the current service of the ensuing year and for such extraordinaries as may require provision will demand, and I doubt not will engage, your early attention.', '\\\\nGentlemen of the Senate and of the House of Representatives:', 'I content myself with recalling your attention generally to such objects, not particularized in my present, as have been suggested in my former communications to you.', 'Various temporary laws will expire during the present session. Among these, that which regulates trade and intercourse with the Indian tribes will merit particular notice.', 'The results of your common deliberations hitherto will, I trust, be productive of solid and durable advantages to our constituents, such as, by conciliating more and more their ultimate suffrage, will tend to strengthen and confirm their attachment to that Constitution of Government upon which, under Divine Providence, materially depend their union, their safety, and their happiness.', 'Still further to promote and secure these inestimable ends there is nothing which can have a more powerful tendency than the careful cultivation of harmony, combined with a due regard to stability, in the public councils.']\"\n\n\nBut, how do we extract information from this type of unstructured data?"
  },
  {
    "objectID": "speeches.html#word-counts",
    "href": "speeches.html#word-counts",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "Word counts",
    "text": "Word counts\nOne possibility is to look at the number of times a particular term is mentioned in the corpus (or a set of terms). Let’s for instance count how many times de words “woman” and “women” show up in the presidential speeches. The code below uses mutate() to create two new variables woman and women indicating how many times those terms appear in the column Text. The actual computation is performed by the function str_count() (from the tidytext package). R goes through the text in that column and search for the string of characters indicated there. Although this is a very simplistic way of searching for terms, it serves as an illustration. Given that we are interested in both terms simultaneously, we can aggregate this information by constructing another column (sum_wom) summing both columns. We modify the existing object using the operator &lt;-.\n\nspeeches &lt;- speeches |&gt;\n  mutate(woman = str_count(Text, \"woman\"),\n         women = str_count(Text, \"women\"),\n         sum_wom = woman + women)\n\nTyping the name of the object shows the results. We have basically added columns indicating how many times those terms show up in each speech.\n\nspeeches\n\n# A tibble: 219 × 7\n   President          Year Title                       Text  woman women sum_wom\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;chr&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n 1 George Washington  1790 First State of the Union A… \"['I…     0     0       0\n 2 George Washington  1790 Second State of the Union … \"['F…     0     0       0\n 3 George Washington  1791 Third State of the Union A… \"['F…     0     0       0\n 4 George Washington  1792 Fourth State of the Union … \"['F…     0     0       0\n 5 George Washington  1793 Fifth State of the Union A… \"['F…     0     0       0\n 6 George Washington  1794 Sixth State of the Union A… \"['F…     0     0       0\n 7 George Washington  1795 Seventh State of the Union… \"['F…     0     1       1\n 8 George Washington  1796 Eighth State of the Union … \"['F…     0     0       0\n 9 by John Adams      1797 First State of the Union A… \"['I…     0     0       0\n10 by John Adams      1798 Second State of the Union … \"['G…     0     0       0\n# ℹ 209 more rows\n\n\nThis kind of information is a numerical variable that can be treated the same way as the ones we explored in the previous case-study. Imagine, for instance that you want to compute the total number of times those terms are mentioned in the whole corpus (all the speeches) plus the mean value (how many times they show up for speech, on average).\n\nspeeches |&gt;\n  summarize(sum = sum(sum_wom),\n            mean = mean(sum_wom, na.rm = TRUE))\n\n# A tibble: 1 × 2\n    sum  mean\n  &lt;int&gt; &lt;dbl&gt;\n1   350  1.60\n\n\nAlternatively, we can explore the evolution of the use of these words over time, that is, how often they show up by year. The peculiar structure of this corpus makes this a bit more complicated than it actually is: we have one speech by year, except in 1790 when we have two. This means that for that year, we have two values in the column sum_wom (one for each speech). If we want to show the information for that year, we need to make a decision: either we sum those values, average them or something. Alternatively, to make things simpler, we can just drop that year from the analysis using filter(). While ggplot() defines which columns are shown in the x- and y-axes (Year and sum_women, respectively), geom_line() indicates that we want to plot a line graph.\n\nspeeches %&gt;%\n  filter(Year&gt;1790) |&gt;\n  ggplot(aes(x = Year, y = sum_wom)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThe previous analysis is a bit naive (among other issues). What if some speeches are longer than others? Having “women” mentioned more often in some of them may therefore not reflect the attention given to women but just the fact that those speeches are longer and therefore have more room for talking about more things. One way of dealing with this issue is to relativise the previous value depending on how long the speech is. The code below uses again mutate() and str_count() to create a column counting how many words each speech has. [\\\\w]+ is a regular expression (regex or regexp), that is, a sequence of characters that specifies a match pattern in text. Given that we don’t have time to explain this, just trust me on this (useful tools for regular expressions can be found here or here.\n\nspeeches &lt;- speeches |&gt;\n  mutate(word_count = str_count(Text, \"[\\\\w]+\"))\n\nWe can graph the results so you have a sense of what this exercise is doing. As shows below, speeches were very short during the first years and became longer over time.\n\nspeeches |&gt;\n  filter(Year&gt;1790) |&gt;\n  ggplot(aes(x = Year, y = word_count)) + \n  geom_point() + \n  geom_line()\n\n\n\n\n\n\n\n\nComing back to our original purpose, we are now in the position of relativising how many times the terms “woman” and “women” are mentioned depending on the length of the text. The code below does this operation (basically dividing the column sum_women between word_count) and plots the results.\n\nspeeches %&gt;% \n  mutate(sum_rel = sum_wom/word_count) |&gt;\n  filter(Year&gt;1790) %&gt;%\n  ggplot(aes(x = Year, y = sum_rel)) + \n  geom_point() + \n  geom_line()"
  },
  {
    "objectID": "speeches.html#top-frequencies",
    "href": "speeches.html#top-frequencies",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "Top frequencies",
    "text": "Top frequencies\nInstead of searching for particular words (or set or words), we may want to adopt a more agnostic position and ask which words are most common in the speeches. This can be achieved by tokenizing the texts. This tool is actually used in many other applications, so it is important to see how it works.\nBasically, tokenizing splits the text into individual words (it also removes all of the punctuation and converts everything into lowercase characters). This is achieved with the function unnest_tokens() (which is part of the tidytext package). Apart from the object that contains the corpus we are exploring, this command requires two arguments: the column we want to tokenise (Text) and the name of the new column that will contain all the tokens (words in this case but it is up to you).\n\ndata_token &lt;- speeches |&gt; \n  unnest_tokens(output = words, input = Text)\n\nAs evident below, this tool transform the original corpus into a new dataframe when each row refers to each token, while preserving the metadata associated to them (speech, president, year, etc.). In total, we have almost 1.8 million tokens in this corpus.\n\ndata_token\n\n# A tibble: 1,766,436 × 8\n   President          Year Title            woman women sum_wom word_count words\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;\n 1 George Washington  1790 First State of …     0     0       0       1069 i    \n 2 George Washington  1790 First State of …     0     0       0       1069 embr…\n 3 George Washington  1790 First State of …     0     0       0       1069 with \n 4 George Washington  1790 First State of …     0     0       0       1069 great\n 5 George Washington  1790 First State of …     0     0       0       1069 sati…\n 6 George Washington  1790 First State of …     0     0       0       1069 the  \n 7 George Washington  1790 First State of …     0     0       0       1069 oppo…\n 8 George Washington  1790 First State of …     0     0       0       1069 which\n 9 George Washington  1790 First State of …     0     0       0       1069 now  \n10 George Washington  1790 First State of …     0     0       0       1069 pres…\n# ℹ 1,766,426 more rows\n\n\nOnce the data is expressed in this way, the column words contain all tokens mentioned in the speeches. These categories are basically qualitative information and can therefore be treated with the same tools we learned in the previous session. For instance, we can simply count() the number of times each category (token) is mentioned. Sorting the results by those with the highest frequencies (sort = TRUE) immediately indicates which tokens are the most common.\n\ndata_token |&gt; \n  count(words, sort = TRUE) %&gt;% \n  print(n = 20)\n\n# A tibble: 29,713 × 2\n   words      n\n   &lt;chr&gt;  &lt;int&gt;\n 1 the   149869\n 2 of     96455\n 3 to     60068\n 4 and    60026\n 5 in     38369\n 6 a      27709\n 7 that   21490\n 8 for    18917\n 9 be     18541\n10 is     16993\n11 our    16933\n12 it     14967\n13 by     14793\n14 which  12197\n15 as     12081\n16 this   12062\n17 with   11816\n18 have   11800\n19 we     11781\n20 i       9356\n# ℹ 29,693 more rows\n\n\nThe problem with this approach is that the most common words have little meaning (at least in terms of illustrating which topics are mentioned). Luckily, there is a simple solution, namely to get rid of those words that are so common that we are not interested in them, known as stop words. In fact there is a list of stop words already built within the tidy environment. Calling this object helps clarifying what stop words actually are: articles, prepositions, etc. We are not covering it but not only are there different lists of stop words (and in many different languages), but you can also modify them (add/remove terms) or create your own list from scratch).\n\nstop_words |&gt;   \n  print(n = 25)\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n11 afterwards  SMART  \n12 again       SMART  \n13 against     SMART  \n14 ain't       SMART  \n15 all         SMART  \n16 allow       SMART  \n17 allows      SMART  \n18 almost      SMART  \n19 alone       SMART  \n20 along       SMART  \n21 already     SMART  \n22 also        SMART  \n23 although    SMART  \n24 always      SMART  \n25 am          SMART  \n# ℹ 1,124 more rows\n\n\nThe trick now is to take this list of stop words and use it to exclude those terms from the tokenised version of the presidential speeches. In order to do so, we take the corpus and use anti_join() to drop the terms that match with the list of stop words. Note that the argument by defines which fields contain the terms to be matched (word in the object data_toke and words in the object stop_words).\n\ndata_token &lt;- data_token |&gt;\n  anti_join(stop_words, by = c(\"words\" = \"word\"))\ndata_token\n\n# A tibble: 696,002 × 8\n   President          Year Title            woman women sum_wom word_count words\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;\n 1 George Washington  1790 First State of …     0     0       0       1069 embr…\n 2 George Washington  1790 First State of …     0     0       0       1069 sati…\n 3 George Washington  1790 First State of …     0     0       0       1069 oppo…\n 4 George Washington  1790 First State of …     0     0       0       1069 cong…\n 5 George Washington  1790 First State of …     0     0       0       1069 favo…\n 6 George Washington  1790 First State of …     0     0       0       1069 pros…\n 7 George Washington  1790 First State of …     0     0       0       1069 publ…\n 8 George Washington  1790 First State of …     0     0       0       1069 affa…\n 9 George Washington  1790 First State of …     0     0       0       1069 rece…\n10 George Washington  1790 First State of …     0     0       0       1069 acce…\n# ℹ 695,992 more rows\n\n\nThe resulting object no longer contains those stop words. We can now proceed counting these categories again. The most common terms are now much more informative.\n\ndata_token |&gt; \n  count(words, sort = TRUE) |&gt;  \n  print(n = 20)\n\n# A tibble: 29,031 × 2\n   words          n\n   &lt;chr&gt;      &lt;int&gt;\n 1 government  6830\n 2 congress    5083\n 3 united      4784\n 4 people      3884\n 5 country     3374\n 6 public      3000\n 7 time        2820\n 8 war         2732\n 9 american    2640\n10 world       2307\n11 law         2124\n12 national    2122\n13 power       1938\n14 act         1860\n15 nation      1831\n16 peace       1796\n17 nations     1782\n18 citizens    1780\n19 service     1742\n20 system      1650\n# ℹ 29,011 more rows\n\n\nWe could easily graph this or even compare the most common words in different periods (or mentioned by different presidents, etc.). We are not going to enter in the details of the code below, but it basically depics which were the most common words before and after 1900.\n\ndata_token |&gt; \n  mutate(period = ifelse(Year &lt;= 1900, \"19th c.\", \"20th c.\")) |&gt; \n  group_by(period) |&gt; \n  count(words, sort=TRUE) |&gt; \n  mutate(proportion = n/sum(n)*1000) |&gt; \n  slice_max(order_by=proportion, n = 15) |&gt; \n  mutate(words = reorder(words, desc(proportion))) |&gt; \n  ggplot(aes(reorder_within(x = words, \n                            by = proportion, within = period),\n             proportion, fill = period)) + \n    geom_col() +\n    scale_x_reordered() +\n    scale_fill_manual(values = c(\"blue\", \"#56B4E9\")) +\n    coord_flip() +\n    facet_wrap(~period, ncol = 2, scales = \"free\") +\n    xlab(\"Word\")"
  },
  {
    "objectID": "speeches.html#n-grams",
    "href": "speeches.html#n-grams",
    "title": "Case-Study 2: The State of the Union Presidential Speeches",
    "section": "n-grams",
    "text": "n-grams\nAs mentioned above, tokenising goes beyond extracting what are the most commons words. It is also quite simple to count particular words as we did at the beginning of this session. The code below creates another column that assigns each observation the value 1 or 0 depending whether the token in the field words is “war” or not. It then computes the fraction of those tokens by year and plots the results.\n\ndata_token |&gt; \n  mutate(war = ifelse(words == \"war\", 1, 0)) |&gt;  \n  group_by(Year) |&gt;\n  summarize(fr_war = mean(war, na.rm = TRUE)) |&gt;\n  ggplot() +\n  geom_col(aes(x = Year, y = fr_war))\n\n\n\n\n\n\n\n\nLikewise, instead of splitting the corpus into 1-gram tokens, tokenising may involve creating multiple-word tokens: 2-grams, 3-grams, etc. As above, the code below uses unnest_tokens() again but it now indicates that we want 2-gram tokens (n = 2). The output shows how the corpus has not been split into.\n\ndata_token2 &lt;- speeches |&gt;\n  unnest_tokens(twogram, Text, token = \"ngrams\", n = 2)\ndata_token2\n\n# A tibble: 1,766,217 × 8\n   President          Year Title          woman women sum_wom word_count twogram\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;  \n 1 George Washington  1790 First State o…     0     0       0       1069 i embr…\n 2 George Washington  1790 First State o…     0     0       0       1069 embrac…\n 3 George Washington  1790 First State o…     0     0       0       1069 with g…\n 4 George Washington  1790 First State o…     0     0       0       1069 great …\n 5 George Washington  1790 First State o…     0     0       0       1069 satisf…\n 6 George Washington  1790 First State o…     0     0       0       1069 the op…\n 7 George Washington  1790 First State o…     0     0       0       1069 opport…\n 8 George Washington  1790 First State o…     0     0       0       1069 which …\n 9 George Washington  1790 First State o…     0     0       0       1069 now pr…\n10 George Washington  1790 First State o…     0     0       0       1069 presen…\n# ℹ 1,766,207 more rows\n\n\nThis type of multiple-word tokens are very useful to identify the terms that are mentioned accompanying particular words. Imagine, for instance, that we are not only interested in how many times the word “women” is mentioned, but also in which context. The code below implements such analysis.\n\nwomen &lt;- data_token2 |&gt; \n  separate_wider_delim(cols = twogram, delim = \" \", \n                       names = c(\"g1\", \"g2\")) |&gt;\n  filter(g1 == \"women\" | g2 == \"women\") |&gt;\n  pivot_longer(g1:g2) |&gt;\n  select(!name) |&gt;\n  rename(words = value) |&gt;\n  filter(words!=\"women\") |&gt;\n  anti_join(stop_words, by = c(\"words\" = \"word\")) \nwomen\n\n# A tibble: 84 × 8\n   President          Year Title            woman women sum_wom word_count words\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;\n 1 George Washington  1795 Seventh State o…     0     1       1       1988 inno…\n 2 Ulysses S. Grant   1873 Fifth State of …     0     1       1      10064 amer…\n 3 Ulysses S. Grant   1873 Fifth State of …     0     1       1      10064 marr…\n 4 Ulysses S. Grant   1874 Sixth State of …     0     2       2       9788 chin…\n 5 Ulysses S. Grant   1874 Sixth State of …     0     2       2       9788 amer…\n 6 Ulysses S. Grant   1875 Seventh State o…     0     3       3      12324 amer…\n 7 Ulysses S. Grant   1875 Seventh State o…     0     3       3      12324 chin…\n 8 Ulysses S. Grant   1876 Eighth State of…     0     1       1       6860 amer…\n 9 Grover Cleveland   1888 Fourth State of…     0     1       1      13200 170  \n10 Grover Cleveland   1888 Fourth State of…     0     1       1      13200 70   \n# ℹ 74 more rows\n\n\n\nwomen |&gt; \n  count(words, sort=TRUE)\n\n# A tibble: 65 × 2\n   words          n\n   &lt;chr&gt;      &lt;int&gt;\n 1 pregnant       7\n 2 american       6\n 3 serving        4\n 4 act            2\n 5 chinese        2\n 6 indian         2\n 7 infants        2\n 8 minorities     2\n 9 11             1\n10 170            1\n# ℹ 55 more rows\n\n\n\nwomen |&gt; \n  filter(words == \"pregnant\") |&gt;\n  count(Year)\n\n# A tibble: 3 × 2\n   Year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  1981     4\n2  1989     1\n3  1995     2\n\n\nWe are going to stop here. We don’t have time for more but there is so much more. I hope this session has given you a sense of how computational methods allow extracting information from historical sources in a powerful way, regardless whether the information is qualitative, numerical or purely textual. For those who want to know more, see you in HIST2025 Computational History."
  },
  {
    "objectID": "backg-speeches.html",
    "href": "backg-speeches.html",
    "title": "Case-study 2: The State of the Union Presidential Speeches",
    "section": "",
    "text": "The second case-study involves exploring the State of the Union Addresses that the president of the United States delivers annually since 1790. Each of these speeches seeks to set the political agenda.\nAs an illustration, please read the text included in the link below that records the address that Woodrow Wilson gave in December of 1913:\nState of the Union Presidential Address - Woodrow Wilson - December 2, 1913\nThe full corpus contains 235 texts (speeches; with a total of almost 1.7 million words) and constitutes an important source of information about the US political agenda and the wider socio-economic and cultural context surrounding them. This information has been gathered together into a .csv file that looks like Figure 1 below. Instead of columns, comma-separate (.csv) files separate the different pieces of information using commas as delimiters: name of the president delivering the speech (President), the year the speech was delivered (Year), the title of the speech (Title) and the (whole) text of the speech itself (Text).1 The first row displays the name of these variables and the remaining rows are devoted to each observation (speech) in the dataset.\nComputational text analysis can provide important insights about the contents of these speeches, including how they have changed over time or how they differ between Democrat and Republican presidents. What kind of words are used more often: “war” or “peace”, “justice” or “freedom”? Is education an important topic in these speeches? What about the economy (“business”, “debt”, “dollar”)? Which locations (including countries) are mentioned more often? Are women present in these speeches? What is the context in which these terms appear?\nIn this case-study, we will explore very simple tools that allow counting the number of particular words appearing in those speeches. For those eager to learning more after our session, you can read more about applying computational methods to this corpus here, here, here, here or here."
  },
  {
    "objectID": "backg-speeches.html#footnotes",
    "href": "backg-speeches.html#footnotes",
    "title": "Case-study 2: The State of the Union Presidential Speeches",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn extended dataset including party affiliation is available here: https://www.kaggle.com/datasets/rtatman/state-of-the-union-corpus-1989-2017↩︎"
  },
  {
    "objectID": "backg-paisley.html",
    "href": "backg-paisley.html",
    "title": "Case-study 1: The Paisley House of Correction Dataset",
    "section": "",
    "text": "The fist case-study deals with the admission records from the Paisley prison, an institution located in a village near Glasgow (The Scottish National Archives). Kept in the National Archives of Scotland, these registers provide individual information on those prisoners who were admitted into this institution between 1841 and 1883.\nFor an illustration on how this type of source looks like, see the register included below of John Hearn, a 12-year old convicted in 1873 for stealing 11 pieces of leather who was sentenced to one month of hard labour. Interestingly, prison records provide info on both males and females, a feature that is relatively uncommon for this period (when most of the height data come from military conscripts).\nThe previous picture comes from the H.M. Prison Wandsworth, near London. The Paisley records are not as fancy. Figure 2 below shows a sample of the original source. While each column records different pieces of information (case number, date of admission, name, etc.), each row refers to each inmate in the data set. The data set was originally collected by Hamish Maxwell-Stewart, James Bradley and Tamsin O’Connor who systematically sample every fourth double page of all registers, resulting in a total of 13,879 observations. For practical reasons, we will rely on a subset of the full data set, containing a thousand prisoners.1 This source presents information in a very structured form, that can be easily transferred to a digital version.\nAs well as the case number, the source reports name, sex, age, place (and country) of birth, place of residence, height, weight, occupation and literacy level, etc. As well demographic and socio-economic information, the registers also include the offence committed and the sentence they were punished with (see image below). The education, occupations, and types of crimes (mostly petty crimes) suggest that these prisoners were drawn from the low working classes, which constituted a large proportion of the full population at the time. Comparing the prisoners’ registers with those of the 1861 population census also confirms that they were “ordinary if vulnerable workers” (Meredith and Oxley 2015, 209).\nInputing the raw data into an Excel spreadsheet results in Figure 3 below. Each column, known as field or variable presents a piece of information. As well as the case number (casen) and the date of admission (information that is split in two fields: month and year), the source records several pieces of information about these inmates, such name and surname, sex, age, place of birth (born) and country of birth (countryb), the place where the were living before being imprisoned (reside), height (in feet and inches) and weight, occupation (occup) and whether they were employed or not. It also reports their literacy, the marks that were visible in their bodies, the offence they committed and the sentence they received. While the first row displays the name of these variables, the remaining rows are devoted to each individual in the dataset.\nWho were these prisoners? Where they were coming from? Did prisoners’ occupations differ significantly from the rest of the population? What about literacy rates? Did men and women commit different crimes? Did judges treat everyone equally or did particular groups suffer harsher sentences? What explains the variation in stature and weights observed across prisoners? How did theses dimensions change during the period? The range of historical questions that this source can address is almost endless. Sarah Horrell, David Meredith and Deborah Oxley have relied on this information to significantly contribute to our understanding of 19th-century British society, especially regarding the biological living standards of the working classes and the gender dynamics that drove the allocation of resources within these families [Horrell and Oxley (2013); Meredith and Oxley (2015)).2 We strongly encourage reading those pieces to get to know more about the source and its possibilities. Bear in mind that, for practical reasons, we will rely on a subset of the full data set, containing a thousand prisoners.3\nThis type of source allows exploring many historical questions:\nSarah Horrell, David Meredith and Deborah Oxley have relied on this information to significantly contribute to our understanding of 19th-century British society, especially regarding the living standards of the working classes and gender dynamics that drove the allocation of resources within these families (Horrell and Oxley 2013; Meredith and Oxley 2015). In a seminal article, Sarah Horrell and Deborah Oxley had previously addressed these issues using similar registers from the Wandsworth prison, near London (Horrell, Meredith, and Oxley 2009). Those who are interested in knowing more about the source and its possibilities are encouraged to read those articles."
  },
  {
    "objectID": "backg-paisley.html#footnotes",
    "href": "backg-paisley.html#footnotes",
    "title": "Case-study 1: The Paisley House of Correction Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI am extremely thankful to Deborah Oxley and Hamish Maxwell-Stewart for kindly sharing this material. I am especially indebted to Deb whose course materials I inherited when I first started teaching a similar course at the University of Oxford in 2012.↩︎\nIn a seminal paper, Sarah Horrell and Deborah Oxley had previously addressed these issues using similar registers from the Wandsworth prison, near London (Horrell, Meredith, and Oxley 2009).↩︎\nWe are extremely grateful to Deborah Oxley and Hamish Maxwell-Stewart for kindly sharing the Paisley dataset.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods for Historians - HIKU8863",
    "section": "",
    "text": "September 15-19, 2025 – 09:15-16:00 – Room U201 (Akrinn, Kalvskinnet)\nInstructor: Francisco J. Beltrán Tapia\nThis course, offered by the Faculty of Humanities at the Norwegian University of Science and Technology, provides a practical introduction on how historians can use computational tools to learn about the past.",
    "crumbs": [
      "Overview",
      "Contents"
    ]
  },
  {
    "objectID": "index.html#welcome-to-quantification-in-history",
    "href": "index.html#welcome-to-quantification-in-history",
    "title": "Quantitative Methods for Historians - HIKU8863",
    "section": "",
    "text": "September 15-19, 2025 – 09:15-16:00 – Room U201 (Akrinn, Kalvskinnet)\nInstructor: Francisco J. Beltrán Tapia\nThis course, offered by the Faculty of Humanities at the Norwegian University of Science and Technology, provides a practical introduction on how historians can use computational tools to learn about the past.",
    "crumbs": [
      "Overview",
      "Contents"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Quantitative Methods for Historians - HIKU8863",
    "section": "Description",
    "text": "Description\nDigital access to historical records is now available on an unprecedent scale. Millions of newspapers, government documents, letters and diaries, among other sources, are only one click away and completely searchable. Similarly, complete population censuses, birth, death and marriage records, military and prisons registers, and other sources have been digitised and made available online.\nHow can we make sense of this ever-increasing wealth of information? Computational methods permit extracting and analysing huge amounts of information, both textual and numerical, that would be impossible otherwise. Supplementing traditional qualitative methods with computing methods not only allows shedding new light into old questions, but also addressing new ones. Likewise, digital tools help visualising information in innovative and powerful ways, thus producing compelling arguments and stories.\nThis course provides an in-depth introduction to quantitative methods, covering some of the techniques most widely used in research in the historical and social sciences. This hands-on course shows how to apply quantitative methods to historical information, both textual and numerical, keeping statistical theory and mathematics to a minimum. The goal is to provide students with the tools to critically engage with the literature relying on quantitative methods and to be able to conduct original research using those tools either in academia, the public, or the business sector. In the process, students will master R, a free statistical software widely used by practitioners in many different fields.\nThe course is taught intensively during one week (morning and afternoon sessions; 30 hours in total). Each session combines lectures and computer practicals using R. Students will learn by applying the different concepts to real data used by historians. No previous background in computing or statistics is required.\n\nImportant warning\nPlease follow the Instructions section to prepare for the course in advance. As well as reading the article Quantifying history, this involves downloading the course materials, installing and getting familiar with R and RStudio and going through the background information on the case-studies we will be exploring.",
    "crumbs": [
      "Overview",
      "Contents"
    ]
  },
  {
    "objectID": "r-scripts/networks.html",
    "href": "r-scripts/networks.html",
    "title": "1.6. Social networks",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n###### Social networks ######\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# intro - simulated networks (run the following code: up to line 48)\n### nodes, edges, etc.\n### network structure\n### agents: centrality, bridges, etc.\nlibrary(tidyverse)\nlibrary(igraph)      \nlibrary(tidygraph)   \nlibrary(ggraph)\n\nset.seed(123)\n# Random (Erdős–Rényi): lower p increases sparsity\ng_random &lt;- sample_gnp(n = 50, p = 0.03, directed = FALSE) |&gt;\n  as_tbl_graph() |&gt;\n  mutate(degree = centrality_degree(),\n         betweenness = centrality_betweenness(),\n         closeness = centrality_closeness()) |&gt;\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(alpha = 0.2) +\n  geom_node_point(size = 2, color = \"steelblue\") +\n  theme_void()\n\n\n# Scale-free (Barabási–Albert)\ng_scalefree &lt;- sample_pa(n = 50, power = 2, m = 1, directed = FALSE) |&gt;\n  as_tbl_graph() |&gt;\n  mutate(degree = centrality_degree(),\n         betweenness = centrality_betweenness(),\n         closeness = centrality_closeness()) |&gt;\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(alpha = 0.2) +\n  geom_node_point(size = 2, color = \"steelblue\") +\n  theme_void()\n\n# Clustered (high transitivity)\ng_clustered &lt;- sample_k_regular(no.of.nodes = 50, k = 4, directed = FALSE) |&gt;\n  as_tbl_graph() |&gt;\n  mutate(degree = centrality_degree(),\n         betweenness = centrality_betweenness(),\n         closeness = centrality_closeness()) |&gt;\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(alpha = 0.2) +\n  geom_node_point(size = 2, color = \"steelblue\") +\n  theme_void()\n\nlibrary(patchwork)\ng_random + g_scalefree + g_clustered\n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# upload basic packages\nlibrary(tidyverse)\n\n# import the data\nletters &lt;- read_csv(\"data/tnp/tnp_letters.csv\")\nletters\n\n# exploring the data (basic desc stats)\nletters |&gt;\n  count(from, sort = TRUE) |&gt;\n  mutate(perc = 100*n/sum(n)) |&gt;\n  mutate(cum_perc = cumsum(perc))\n\nsenders &lt;- letters |&gt; count(from) |&gt; rename(n_out = n, agent = from)\nreceivers &lt;- letters |&gt; count(to) |&gt; rename(n_in = n, agent = to)\nlibrary(ggrepel)\nsenders |&gt; full_join(receivers, by = \"agent\") |&gt;\n  ggplot(aes(x = n_out, y = n_in)) +\n  geom_point() + \n  geom_text_repel(aes(label = ifelse(n_out &gt; 10 | n_in &gt; 25, agent, \"\")),\n                  vjust = -0.5, size = 2)\n\n### Exploring networks: all links simultaneously\n\n## convert the object into a network\n\n# install.packages(\"igraph\")\nlibrary(igraph)\n\nnetwork &lt;- graph_from_data_frame(\n  d = letters, \n  # vertices = people,      \n  directed = TRUE) \n\nnetwork\n\n## use a different package so it is \"tidy\"\nlibrary(tidygraph)\n\nnetwork_tidy &lt;- letters |&gt;\n  as_tbl_graph()\nnetwork_tidy\n\n### visualising the network\n\nlibrary(ggraph)\n\nset.seed(345) # so we see the same every time\nnetwork_tidy |&gt;\n  ggraph(\"fr\") + \n  geom_edge_link0(alpha = 0.3) + \n  geom_node_point(size = 0.5, alpha = 0.3, \n                  color = \"blue\") + \n  theme_graph()\n\n## connected vs disconnected nodes\ncomps &lt;- components(network_tidy)\n  # three elements:\n    # 1. a vector assigning each node to a component (comps\\$membership); \n    # 2. the size of each component, that is, the number of nodes (comps\\$csize); \n    # 3. the total number of components (comp\\$no).\n\ncomps$csize\n\n## extract the biggest component\nmain_network &lt;- network_tidy |&gt;\n  induced_subgraph(\n    which(comps$membership == which(comps$csize&gt;=10))) |&gt;\n  as_tbl_graph()\nmain_network\n\nset.seed(456)\nmain_network |&gt; # visualise it\n  ggraph(\"fr\") + \n  geom_edge_link0(alpha = 0.3) + \n  geom_node_point(size = 0.5, alpha = 0.3,\n                  color = \"blue\") + \n  theme_graph()\n\n##### Network metrics: global and local\n\n## Global metrics\n  # characterising the network itself\n    # compared to other networks or the same network over time\n  # size, density, diameter, average path length, connectedness...\n\n#### Extract two networks: before and during Elizabeth I's reign (1558-1603)\nn1 &lt;- network_tidy |&gt;\n  activate(\"edges\") |&gt;\n  mutate(year = date_from %/% 10000) |&gt;\n  filter(year&lt;1558)\ncomps &lt;- components(n1)\nn1 &lt;- n1 |&gt;\n  induced_subgraph(which(comps$membership == which.max(comps$csize))) |&gt;\n  as_tbl_graph()\n\nn2 &lt;- network_tidy |&gt;\n  activate(\"edges\") |&gt;\n  mutate(year = date_from %/% 10000) |&gt;\n  filter(year&gt;=1558)\ncomps &lt;- components(n2)\nn2 &lt;- n2 |&gt;\n  induced_subgraph(which(comps$membership == which.max(comps$csize))) |&gt;\n  as_tbl_graph()\n\nset.seed(567)\ns &lt;- 3\nx &lt;- 11\ny &lt;- 26\ng1 &lt;- n1 |&gt;\n  ggraph(\"fr\") + \n  geom_edge_link0(alpha = 0.3) + \n  geom_node_point(size = 0.5, alpha = 0.3,\n                  color = \"blue\") + \n  annotate(\"text\", \n           x = x, y = y,  \n           label = \"1509-1557\",\n           size = s, fontface = \"bold\") +\n  theme_graph() \ng2 &lt;- n2 |&gt;\n  ggraph(\"fr\") + \n  geom_edge_link0(alpha = 0.3) + \n  geom_node_point(size = 0.5, alpha = 0.3,\n                  color = \"blue\") + \n  annotate(\"text\", \n           x = x, y = y,  \n           label = \"1558-1603\",\n           size = s, fontface = \"bold\") +\n  theme_graph() \nlibrary(patchwork)\ng1 + g2\n\n## compute the measures: gorder(), gsize(), diameter(), ...\n  # and construct a table reporting them\ntibble(\n  period = c(\"1509-1557\", \"1558-1603\"),\n  nodes  = c(gorder(n1), gorder(n2)),\n  edges  = c(gsize(n1), gsize(n2)),\n  density  = c(edge_density(n1), edge_density(n2)),\n  diameter  = c(diameter(n1, directed = TRUE), \n                diameter(n2, directed = TRUE)),  \n  avg_path  = c(mean_distance(n1, directed = TRUE), \n                mean_distance(n2,  directed = TRUE)))\n\n## Local metrics\n### discriminating between the nodes: allows identifying important actors\n  # yields a metric for each node (agent)\n\n  # - Degree: number of connections (neighbours).\n  # - Closeness: distance to all other nodes.\n  # - Betweenness: how a node facilitates accessing other nodes. \n  # - Eigenvector: connectedness to other influential nodes. \n\nn2 &lt;- n2 |&gt;\n  activate(\"nodes\") |&gt; # probably not needed\n  mutate(degree_out = centrality_degree(mode = \"out\"),\n         degree_in = centrality_degree(mode = \"in\"),\n         degree_tot = centrality_degree(mode = \"total\"),\n         closeness = centrality_closeness(), \n         betweenness = centrality_betweenness(), \n         eigen = centrality_eigen(directed = TRUE))\n\nn2 |&gt;\n  activate(\"nodes\") |&gt;\n  arrange(desc(degree_out)) |&gt; \n  as_tibble()\n\nn2 |&gt;\n  arrange(desc(closeness)) |&gt; \n  as_tibble()\n\n## visualising local metrics\nset.seed(678)\nn2 |&gt;\n  ggraph(\"fr\") + \n  geom_edge_link0(alpha = 0.3) +\n  geom_node_point(aes(size = degree_out,\n                      color = closeness), \n                  shape = 16, alpha = 0.6) +\n  geom_node_text(aes(label = if_else(closeness&gt;0.5 |\n                                       degree_out&gt;10,\n                                     name, NA)), size = 1.5) +\n  scale_color_gradient(low=\"#104E8B\",high=\"#CD2626\") +\n  scale_size(range = c(0.5, 4)) +\n  labs(color = \"Closeness\", \n       size = \"Degree\")\ntheme_graph()\n\n### Community (cluster) detection\nnetwork_tidy # components\n\n# identifying connected/unconnected nodes\nset.seed(345)\nnetwork_tidy |&gt;\n  mutate(component = group_components(),\n         component = as_factor(component)) |&gt;\n  ggraph(\"fr\") + \n  geom_edge_link0(alpha = 0.3) + \n  geom_node_point(aes(color = component), size = 0.5) + \n  scale_color_viridis_d() +\n  theme_graph() + \n  theme(legend.position = \"none\") \n\n## find clusters\n  # groups of nodes that are densely connected between them\n  # many different algorithms: lovain, leiden...\n\nset.seed(456)\nmain_network |&gt;\n  as_undirected(mode = \"collapse\") |&gt; \n  as_tbl_graph() |&gt; \n  mutate(cluster = group_louvain(),\n         cluster = as_factor(cluster)) |&gt;\n  ggraph(\"fr\") + \n  geom_edge_link0(alpha = 0.3) + \n  geom_node_point(aes(color = cluster), size = 0.5) + \n  theme_graph() + \n  scale_color_viridis_d() +\n  theme(legend.position = \"none\") \n\n## explore clusters' properties\nmain_network |&gt;\n  as_undirected(mode = \"collapse\") |&gt; \n  as_tbl_graph() |&gt; \n  mutate(cluster = group_louvain(),\n         cluster = as_factor(cluster)) |&gt;\n  activate(nodes) |&gt;\n  as_tibble() |&gt;\n  count(cluster, sort = TRUE) \n\n## spatial networks (map them to \"real\" locations)"
  },
  {
    "objectID": "r-scripts/desc_text.html",
    "href": "r-scripts/desc_text.html",
    "title": "1.4. Textual information (counting words)",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM-2024/session\") \n\n# Install packages\n# install.packages(\"tidyverse\")\n# install.packages(\"tidytext\")\n# install.packages(\"tokenizers\")\n\n# Open packages that we will be using\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Set the encoding (optional)\nSys.setlocale(\"LC_ALL\", \"en_US.utf8\")\n  # ensure that all our computers read the characters/symbols the same way\n  # you’re telling R to use U.S. English conventions with a UTF-8 codeset \n  # it also depends on the language of the data your are using\n\n# Importing the data\ndata &lt;- read_csv(\"data/state_of_the_union_texts.csv\",\n                 locale = locale(encoding = \"UTF-8\"))\n  # we use same encoding as above \n\n# Having a look at the data\ndata\nprint(data)\nprint(data, n = 20)\nprint(data, n = Inf)\nView(data)\n\ndata |&gt; \n  count(President)\n\ndata |&gt;\n  ggplot(aes(x = Year) +\n  geom_histogram(bindwidth = 1)         \n\n\n# Reporting a particular speech (number 4 in this case)\ndata$Text[4] \n  # will show the fourth line of the text column of the \"data\" dataframe\n\n# or\nspeech_1912 &lt;- data |&gt; \n  filter(Year==1912)\nspeech_1912$Text\n\n\n#### n-grams (number of particular terms in the texts)\ndata &lt;- data |&gt;\n  mutate(peace = str_count(Text, \"[Pp]eace\")) |&gt;\n  mutate(war = str_count(Text, \"[Ww]ar\")) \n\n# summary statistics\ndata |&gt;\n  summarize(obs = sum(!is.na(war)),\n            sum = sum(war),\n            mean = mean(war, na.rm = TRUE)) \n\ndata |&gt;\n  group_by(Year) |&gt;\n  summarize(sum = sum(war), \n            mean = mean(war, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = Year, y = mean)) +\n  geom_line()\n\ndata |&gt;\n  filter(Year&gt;1790) |&gt;\n  ggplot(aes(x = Year, y = war)) +\n  geom_line()\n\ndata |&gt;\n  filter(war&gt;150)\n\n# aggregate the data into periods\ndata |&gt;\n  mutate(period = case_when(\n    Year&lt;1914 ~ \"Pre-1914\",\n    Year&gt;=1914 & Year&lt;=1945 ~ \"World Wars\",\n    Year&gt;1945 ~ \"Post-1945\")) |&gt;\n  mutate(period = factor(period, \n                         levels = c(\"Pre-1914\", \"World Wars\", \"Post-1945\"),\n                         ordered = TRUE)) |&gt;\n  group_by(period) |&gt;\n  summarize(sum = sum(war), \n            mean = mean(war, na.rm = TRUE)) \n  \n    # factor ranks them (instead of alphabetically, as with strings)\n\n# Plots\ndata |&gt;\n  group_by(Year) |&gt;\n  summarize(peace = sum(peace), war = sum(war)) |&gt;\n  pivot_longer(c(\"peace\", \"war\"), names_to = \"word\", values_to = \"counts\") |&gt;\n  ggplot(aes(x = Year, y = counts, color = word)) +\n  geom_line()\n\ndata |&gt;\n  filter(Year&gt;1790) |&gt;\n  pivot_longer(c(\"peace\", \"war\"), names_to = \"word\", values_to = \"counts\") |&gt;  \n  ggplot(aes(x = Year, y = counts, color = word)) +\n  geom_line()\n\ndata |&gt;\n  filter(Year&gt;1840 & Year&lt;1855 & war&gt;50)\n\n## Regular expressions:\n # \"^A\" those starting with A\n # \"e$\" those ending with e\n # [ae] both a and e  \n # ^x except x\n # | or: (war|conflict)\n # Check for more: https://regexr.com/\n\n  # do it with \"[Ww]omen\"\ndata &lt;- data |&gt; mutate(women = str_count(Text, \"[Ww]om[ae]n\"))\n\ndata |&gt;\n  group_by(Year) |&gt;\n  summarize(women = sum(women)) |&gt;\n  ggplot(aes(x = Year, y = women)) +\n    geom_line()\n\n\ndata |&gt;\n  filter(Year&gt;1790) |&gt;\n  ggplot(aes(x = Year, y = women)) +\n  geom_line()\n  # no need to group by year if you exclude the first year (with 2 speeches)\n\n\n\n#### Compute \"word counts\"\n\n# Counting all words or numbers that are separated by spaces on either side\ndata &lt;- data |&gt;\n  mutate(wc = str_count(Text, \"[\\\\w]+\"))\n  # [\\\\w]+ is a regular expression (regex or regexp)\n  # [:alpha:]+ would also work: groups of alphanumeric characters without a space, punctuation or line break in between\n    # a sequence of characters that specifies a match pattern in text\n  # more on this here: https://regexr.com\n  # this other handy tool provides a quick references to regular expressions \n    # and also allows testing them: https://regex101.com\n\n# Graph it\ndata |&gt;\n  ggplot(aes(x = Year, y = wc)) + \n  geom_point() + \n  geom_line()\n# just line graph\n# we might add geom_point() to underscore that we have missing years\n\ndata |&gt; \n  filter(Year&gt;1940 & wc&gt;20000)\n\ndata |&gt; \n  mutate(war_rel = war/wc) |&gt;\n  mutate(war_rel2 = 1000*war/wc) |&gt;\n  filter(Year&gt;1790) |&gt;\n  ggplot(aes(x = Year, y = war_rel)) + \n  geom_point() + \n  geom_line()\n\n\n## the package ngramr allows doing this in the google books database\n# install.packages(ngramr)\nlibrary(ngramr)\nngrams &lt;- ngram(c(\"women\", \"peace\", \"war\"), \n                year_start = 1700)\nngrams\n\nngrams |&gt; \n  ggplot(aes(x = Year, y = Frequency, colour = Phrase)) +\n  geom_line()\n\n\n\n#### Top word frequencies (the most common words)\n\n###### \"tokenizing\" first \n  # removes all of the punctuation, \n  # splits the text into individual words, and \n  # converts everything into lowercase characters\n\ndata_token &lt;- data |&gt;\n  unnest_tokens(output = words, input = Text)\n\ndata_token # almost 1.8 million entries\n\ndata_token |&gt;\n  count(words, sort = TRUE) |&gt; \n  print(n = 20) # \"Inf\" display all rows\n  # the most common words have no meaning\n\n#### stop words \n  # words that are so common that we are not interested in them \n  # already built within the tidy environment\n\nstop_words |&gt; print(n = 25)\n  # note that the stop words are in the \"word\" column\n\n# Exclude stop_words from the tokens\ndata_token_stop &lt;- data_token |&gt;\n  anti_join(stop_words, by = c(\"words\" = \"word\"))\n  # anti_join drops the words matching in both \"datasets\"\n  # \"words\" & \"word\" are the names of the fields in each \"dataset\"\n\ndata_token_stop |&gt;\n  count(words, sort = TRUE) |&gt; \n  print(n = 20)\n\n# Create our own stop words (or add more to the existing list)\nstop_words\nmy_stop_words &lt;- c(\"peace\", \"war\")\nmy_stop_words\ncustom_stop_words &lt;- tibble(word = my_stop_words, lexicon = \"my_customization\")\ncustom_stop_words\nstop_words_custom &lt;- rbind(stop_words, custom_stop_words)\nstop_words_custom\ntail(stop_words_custom) # view the end of the tibble, look like our words were added correctly\n\ndata_token_stop # reduce to almost 700,000 entries\n\n# Graphing - Top word frequencies\ndata_token_stop |&gt;\n  count(words, sort=TRUE) |&gt;\n  top_n(15) |&gt;                     # selecting to show only top 15 words\n  mutate(words = reorder(words, n)) |&gt;  # highest frequency words appear first\n  ggplot(aes(words, n)) +\n    geom_col() +\n    coord_flip()\n\n# Focusing on a particular period (1850-1900)\ndata_token_stop |&gt;\n  filter(Year &gt;1850 & Year &lt; 1900) |&gt;\n  count(words, sort=TRUE) |&gt;\n  top_n(15) |&gt;                     # selecting to show only top 15 words\n  mutate(words = reorder(words, n)) |&gt;  # this will ensure that the highest frequency words appear to the left\n  ggplot(aes(words, n)) +\n  geom_col() +\n  coord_flip()\n\n# Comparing periods\ndata_token_stop &lt;- data_token_stop |&gt;\n  mutate(period = ifelse(Year &lt;= 1900, \"19th c.\", \"20th c.\"))\n\ndata_token_stop |&gt;\n  group_by(period) |&gt;\n  count(words, sort=TRUE) |&gt;\n  mutate(proportion = n / sum(n) * 1000) |&gt;  # word freq- per 1000 words instead of counts\n  slice_max(order_by=proportion, n = 15) |&gt;  # selecting to show only top 15 words\n  mutate(words = reorder(words, desc(proportion))) |&gt;  # this will ensure that the highest frequency words appear to the left\n  ggplot(aes(reorder_within(x = words, by = proportion, within = period), proportion, fill = period)) +    # reordering is a bit tricky, see                                                                                                     ?reorder_within()\n    geom_col() +\n    scale_x_reordered() +\n    scale_fill_manual(values = c(\"blue\", \"#56B4E9\")) +\n    coord_flip() +\n    facet_wrap(~period, ncol = 2, scales = \"free\") +\n    xlab(\"Word\")\n\n?ggplot\n\n# Focusing on particular words\ndata_token_stop &lt;- data_token_stop |&gt;\n  mutate(war = ifelse(words == \"war\", 1, 0))\n\ndata_token |&gt;\n  filter(Year&gt;=1900 & Year&lt;2000) |&gt;\n  mutate(women = ifelse(words == \"women\", 1, 0)) |&gt;\n  count(women)\n\n# Evolution over time\ndata_token_stop |&gt;   \n  group_by(Year) |&gt;\n  summarize(fr_war = mean(war, na.rm = TRUE)) |&gt;\n  ggplot() +\n    geom_col(aes(x = Year, y = fr_war))\n\n\n# Stemming\n  # looking at the common root of similar words\n\n# install.packages(SnowballC)\nlibrary(SnowballC)\n\ndata_token_stop_stem &lt;- data_token_stop |&gt;\n  mutate(word_stem = wordStem(words))\ndata_token_stop_stem\n\ndata_token_stop_stem |&gt;\n  count(word_stem, sort=TRUE) |&gt; \n  print(n = 30)\n\ndata_token_stop_stem |&gt;\n  count(word_stem, sort=TRUE) |&gt;\n  top_n(15) |&gt;                     # selecting to show only top 15 words\n  mutate(word_stem = reorder(word_stem, n)) |&gt;  # this will ensure that the highest frequency words appear to the left\n  ggplot(aes(word_stem, n)) +\n  geom_col() + coord_flip()\n\n\n# We could do the whole process simultaneously\ndata_adj &lt;- data |&gt;\n  unnest_tokens(output = words, input = Text) |&gt;       # tokenize\n  anti_join(stop_words, by = c(\"words\" = \"word\")) |&gt;   # drop stop words\n  mutate(word_stem = wordStem(words))\n\ndata_adj\n\ndata_adj &lt;- data_adj |&gt;\n  mutate(war = ifelse(words == \"war\", 1, 0)) |&gt;\n  mutate(peace = ifelse(words == \"peace\", 1, 0))\n\ndata_adj |&gt;   \n  group_by(Year) |&gt;\n  summarize(fr_war = mean(war, na.rm = TRUE),\n            fr_peace = mean(peace, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = Year)) +\n    geom_line(aes(y = fr_war), color = \"red\") +\n    geom_line(aes(y = fr_peace), color = \"blue\")\n\n## dictionaries\n\nwar_words &lt;- c(\"war\", \"conflict\", \"hostilities\",    \"agression\", \n               \"armies\", \"army\", \"weapon\")\nrights_words &lt;- c(\"rights\", \"democracy\", \"freedom\")\n\nwar_dict &lt;- tibble(word = war_words, dictionary = \"war\")\nrights_dict &lt;- tibble(word = rights_words, dictionary = \"rights\")\n\ndict &lt;- rbind(rights_dict, war_dict) # putting them together\n\n  # or import these lists from an txt, csv, excel file\n\ndata |&gt;\n  unnest_tokens(output = words, input = Text) |&gt; \n  inner_join(dict, by = c(\"words\" = \"word\")) |&gt; \n  count(Year, dictionary) |&gt; \n  ggplot(aes(x = Year, y = n, color = dictionary)) +\n  geom_line() +\n  geom_smooth(se = FALSE)\n\n#### n-grams: multiple-word tokens\n\ndata_token2 &lt;- data |&gt;\n  unnest_tokens(twogram, Text, token = \"ngrams\", n = 2)\ndata_token2\n\n# in separate columns\ndata_token2 &lt;- data_token2 |&gt;\n  separate_wider_delim(cols = twogram, delim = \" \", names = c(\"g1\", \"g2\"))\ndata_token2\n\n## we could identify the most common bigrams and include them as unique words\n### \"united nations\"\n\n# identify words accompanying particular words\nwomen &lt;- data_token2 |&gt;\n  filter(g1 == \"women\" | g2 == \"women\") |&gt;\n  pivot_longer(g1:g2) |&gt; # put both columns in the same one\n  select(!name) |&gt; # drop the variable we are not using\n  rename(words = value) |&gt; # rename the new column we created\n  filter(words!=\"women\") |&gt; # drop the word women (we are interested in those around)\n  anti_join(stop_words, by = c(\"words\" = \"word\")) \n\nwomen |&gt;\n  count(words, sort=TRUE) |&gt;\n  print(n = 25)\n\nwomen |&gt; \n  filter(words == \"pregnant\") |&gt;\n  count(Year) \n\n\n# Exercise: \n  # think about a potential topic (word) \n  # explore the words around it (2 in each side)\n\n\n# Further topics:\n  # TF-IDF Term frequency - Inverse document frequency\n  # POS (Part of Speech) - nouns, verbs, adjectives...\n  # Sentiment analysis\n  # Topic models\n  # Co-occurrence\n\n\n\n# check women / men\n\ndata |&gt;\n  mutate(women = str_count(Text, \"[Ww]omen\")) |&gt;\n  summarize(women = sum(women)) # 300 \n\ndata_token |&gt;\n  mutate(women = if_else(words, \"women\")) |&gt;\n  summarize(women = sum(women)) # 302 \n\ndata_token2 |&gt;\n  mutate(women = str_count(g2, \"[Ww]omen\")) |&gt;\n  summarize(women = sum(women)) # 302 in either g1 and g2 \n\n### keywords-in-context\n# install.packages(\"quanteda\")\nlibrary(quanteda)\n\ntokens &lt;- corpus(data, text_field = \"Text\") |&gt;  \n  quanteda::tokens()\n\nkeywords &lt;- kwic(tokens, pattern = \"women\", window = 5)\n\nhead(keywords)"
  },
  {
    "objectID": "r-scripts/text_advanced.html",
    "href": "r-scripts/text_advanced.html",
    "title": "5. Advanced automated textual analysis",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# Install packages\n# install.packages(\"tidyverse\")\n# install.packages(\"tidytext\")\n# install.packages(\"tokenizers\")\n# install.packages(\"stm\")\n# install.packages(\"SnowballC\")\n\nif (!requireNamespace(\"xfun\")) install.packages(\"xfun\")\nxfun::pkg_attach2(\"tidyverse\", \"lubridate\", \"rvest\", \"stringr\", \"readtext\", \"tesseract\", \"tidytext\", \"SnowballC\", \"wordcloud\", \"wordcloud2\", \"widyr\", \"quanteda\", \"quanteda.textstats\", \"magrittr\", \"pdftools\", \"devtools\", \"tsne\", \"topicmodels\", \"readtext\")\n# The following two packages have Java dependencies that might give some (especially Windows) machines trouble. No worries if they don't load on your computer, I'll demonstrate running them on RStudio Cloud.\nxfun::pkg_attach2(\"tabulizer\", \"openNLP\")\ninstall.packages(\"devtools\")\ndevtools::install_github(\"bmschmidt/wordVectors\")\n\n# Open packages that we will be using\nlibrary(tidyverse)\nlibrary(tidytext)\n\nlibrary(SnowballC)\n\n# Importing the data\ndata &lt;- read_csv(\"data/state_of_the_union_texts.csv\")\ndata\n\n#### Tagging - Expanding on counting words (script 4) \n  # tagging the corpus into \"parts of speech\" and \"named entities\"\n\n## Part-of-speech tagging\n### identify verbes, nouns, etc.\n\n# install.packages(\"udpipe\")\nlibrary(udpipe)\n\n  # first download and load the language library (English have several libraries)\npos_model &lt;- udpipe_download_model(\"english-gum\")\npos_model &lt;- udpipe_load_model(file = pos_model$file_model)\n\n  # example with the first sentence of the first speech\nlibrary(tokenizers)\nsample_sentence &lt;- tokenize_sentences(data$Text[1])[[1]][[1]]\ntagged_pos &lt;- udpipe_annotate(pos_model, x = sample_sentence)\nas_tibble(tagged_pos)\n\n### upos: universal part of speech tag\n### xpos: language- (or corpus-) specific part of speech tag\n| Tag         | Category                  |\n  |-------------|---------------------------|\n  | ADJ         | adjective                 |\n  | ADP         | adposition                |\n  | ADV: adverb | adverb                    |\n  | AUX         | auxiliary                 |\n  | CCONJ       | coordinating conjunction  |\n  | DET         | determiner                |\n  | INTJ        | interjection              |\n  | NOUN        | noun                      |\n  | NUM         | numeral                   |\n  | PART        | particle                  |\n  | PRON        | pronoun                   |\n  | PROPN       | proper noun               |\n  | PUNCT       | punctuation               |\n  | SCONJ       | subordinating conjunction |\n  | SYM         | symbol                    |\n  | VERB        | verb                      |\n  | X           | other                     |\n\n## finding the most commonly occuring nouns\ndata_pos &lt;- udpipe_annotate(pos_model,\n                            x = data$Text,\n                            doc_id = data$Year)    \ndata_pos &lt;- as_tibble(data_pos) |&gt; \n  mutate(Year = as.numeric(doc_id)) # more coherent with our dat\ndata_pos |&gt; \n  filter(upos == \"NOUN\") |&gt; \n  count(lemma, sort=TRUE) |&gt;  \n  top_n(15) |&gt;                 \n  mutate(lemma = reorder(lemma,n)) |&gt;  \n  ggplot(aes(lemma, n)) + \n  geom_col() + \n  coord_flip()\n  \n## another example more for language/literary analyses: \n  # chart the number of adjectives, nouns and verbs (as a percentage of total words)\n\ndata_pos |&gt; \n  group_by(Year) |&gt; \n  summarize(word_count = n(), \n            nouns = sum(upos == \"NOUN\"), \n            verbs = sum(upos == \"VERB\"),\n            adj = sum(upos == \"ADJ\")) |&gt; \n  mutate(nouns = nouns/word_count, \n         verbs = verbs/word_count, \n         adj = adj/word_count) |&gt; \n  select(!word_count) |&gt; \n  pivot_longer(!Year, names_to = \"POS\", values_to = \"Count\") |&gt; \n  ggplot() +\n  geom_line(aes(x = Year, y = Count, group = POS, color = POS)) +\n  scale_y_continuous(labels = scales::label_percent()) + # labels as %\n  scale_x_continuous(breaks = seq(1905, 2021, 10))\n\n\n## Named entity recognition (NER)\ninstall.packages(\"entity\") # wrapped around NLP and openNLP\n  # download the zip ball here: \n  # https://github.com/trinker/entity/?tab=readme-ov-file#installation\n  # and run:\n  if (!require(\"pacman\")) install.packages(\"pacman\")\n  pacman::p_load_gh(\"trinker/entity\")\n\nlibrary(entity)\n  \n  # entity can annotate six kinds of names: \n  ## persons, locations, organizations, dates, mentions of money, and dates\n  ## person_entity(), location_entity(), organization_entity(), ... \n\n## example with locations (what type before and after 1900?)\nlocation_entity(data$Text[5]) \n\nlocations &lt;- data |&gt; \n  mutate(locations = location_entity(Text)) \nlocations |&gt;   \n  mutate(Period = ifelse(Year&lt;1900, \"XIX c.\", \"XX c.\"))  |&gt;  \n  mutate(Period = factor(Period, levels = c(\"XIX c.\", \"XX c.\"))) |&gt; \n  unnest(locations)|&gt; # instead of unnest_tokens (unnesting a list now)\n  group_by(Period) |&gt;                                            \n  count(locations, sort=TRUE) |&gt; \n  mutate(proportion = 1000*n/sum(n)) |&gt;                    \n  top_n(15) |&gt; \n  ggplot(aes(x = reorder_within(x = locations, \n                                by = proportion, \n                                within = Period), \n             y = proportion, \n             fill = Period)) +    \n  geom_col() +\n  scale_x_reordered() +\n  coord_flip() +\n  facet_wrap(~Period, ncol = 2, scales = \"free\") +\n  labs(x = \"Word\", \n       y = \"Frequency per 1000 words\", \n       title = \"Location frequencies\") +  \n  theme_bw() +\n  guides(fill=\"none\") \n\n  ### we could now map this (geocoding first to extract xy-coordinates)\n\n#### TF-IDF Term frequency - Inverse document frequency\n  # Identifying words that are unique (or show up less often) to certain documents (speeches, periods, etc.)\n  # No need to \"stopwords\" because words that are common in all docs are unimportant here\n\ndata &lt;- data %&gt;%\n  mutate(period = case_when(\n  Year&lt;1914 ~ \"Pre-1914\",\n  Year&gt;=1914 & Year&lt;=1945 ~ \"World Wars\",\n  Year&gt;1945 ~ \"Post-1945\")) %&gt;%\n  mutate(period = factor(period, \n                         levels = c(\"Pre-1914\", \"World Wars\", \"Post-1945\"),\n                         ordered = TRUE))\n\ndata_token &lt;- data %&gt;%\n  unnest_tokens(output = words, input = Text) %&gt;%       # tokenize first\n  count(words, period, sort = TRUE) # count them\n\ntf_idf &lt;- data_token %&gt;%\n  bind_tf_idf(words, period, n) # distinguish documents by year\n  # the commonest words are weighted away (their tf-idf score is 0)\n\ntf_idf\n  # tf - term frequency\n  # idf - inverse document frequency\n  # tf_idf - identifies the most unique words in each year\n\n# Check the highest scores\ntf_idf %&gt;% arrange(desc(tf_idf)) \n\n# You could now identify which are the most unique words in each speech\n\n\n#### POS (Part of Speech)- Identify types of words: nouns, verbs, adjectives...\n\n# There is an in-built list of words identifying POS\nparts_of_speech\n\ndata_token &lt;- data %&gt;%\n  unnest_tokens(output = word, input = Text) %&gt;%  # the data we will be using (tokenised)\n  inner_join(parts_of_speech, relationship = \"many-to-many\") %&gt;%                 # join the two datasets using \"word\" as the \"matching\" field\n  count(pos)\ndata_token\n\n\n# There are more sophisticated POS tagging would require the context of the sentence structure\n# It is though beyond what we cover here and involve other packages (\"NLP\", \"openNLP\", \"tm\")\n# See here:\n# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n\n\n#### Sentiment analysis\n# install.packages(\"textdata\")\n\nlibrary(tidytext)\nlibrary(textdata)\n\n# Tidytext includes three dictionaries with \"sentiments\": afinn, bing & nrc\n\nget_sentiments(\"afinn\")\n  # the first time you will need to say yes to download of the sentiment dictionary\ntail(get_sentiments(\"afinn\"))\nget_sentiments(\"afinn\") %&gt;%\n  summary(value) \n  # summary statistics for the value column (from -5 to 5)\n\nget_sentiments(\"bing\")\n\n\n# Count words from the lexicons that appear in a text and add them all up\n\n# (positives - negatives)\ndata_token_stop &lt;- data %&gt;%\n  unnest_tokens(output = words, input = Text) %&gt;%   # tokenise\n  anti_join(stop_words, by = c(\"words\" = \"word\"))   # remove stop words\n\nsentiment &lt;- data_token_stop %&gt;%\n  rename(\"word\" = \"words\") %&gt;% # we call our new column \"word\" which makes inner_joins easier \n  inner_join(get_sentiments(\"bing\"), relationship = \"many-to-many\") %&gt;%\n  count(sentiment, year = Year) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative) \n\nsentiment %&gt;%\n  ggplot(aes(year, sentiment)) +\n    geom_line(show.legend = FALSE) +\n    geom_hline(yintercept = 0, linetype = 2, alpha = .8)\n\nsentiment %&gt;%\n  filter(year &gt; 1950 & sentiment &gt; 400)\n\n# using the afinn lexicon\ndata_token_stop %&gt;%\n  rename(\"word\" = \"words\") %&gt;% # we call our new column \"word\" which makes inner_joins easier \n  inner_join(get_sentiments(\"afinn\")) %&gt;%\n  group_by(Year) %&gt;%\n  summarize(sentiment = sum(value)) %&gt;%\n  ggplot(aes(Year, sentiment)) +\n    geom_line(show.legend = FALSE) +\n    geom_hline(yintercept = 0, linetype = 2, alpha = .8)\n\n\n# Putting both analysis together\n\nbing &lt;- data_token_stop %&gt;%\n  rename(\"word\" = \"words\") %&gt;% # we call our new column \"word\" which makes inner_joins easier \n  inner_join(get_sentiments(\"bing\"), relationship = \"many-to-many\") %&gt;%\n  count(sentiment, year = Year) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(bing = positive - negative) \n\nafinn &lt;- data_token_stop %&gt;%\n  rename(\"word\" = \"words\") %&gt;% # we call our new column \"word\" which makes inner_joins easier \n  inner_join(get_sentiments(\"afinn\")) %&gt;%\n  group_by(Year) %&gt;%\n  summarize(afinn = sum(value))\n\nbing %&gt;%\n  inner_join(afinn, by = c(\"year\" = \"Year\")) %&gt;%\n  pivot_longer(c(\"bing\", \"afinn\"), names_to = \"sentiment\", values_to = \"counts\") %&gt;%\n  ggplot(aes(x = year, y = counts, color = sentiment)) +\n    geom_line() +\n    geom_hline(yintercept = 0, linetype = 2, alpha = .8)\n\n\n\n# Topic models\n  # discovering the abstract \"topics\" that occur in a collection of documents\n  # unveils hidden semantic structures in a text body\n  # it works with matrixes of words/documents\n  # more details here: \n  # https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2\n\n# install.packages('topicmodels')\n# install.packages('stm')\n\noptions(stringsAsFactors = FALSE)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(topicmodels)\nlibrary(stm)\nlibrary(SnowballC)\n\n# transform dataframe to DTM - document term matrix\ndata_dtm &lt;- data_token_stop %&gt;%\n  mutate(word_stem = wordStem(words)) %&gt;% # stemming\n  select(President, period, word_stem) %&gt;%\n  rename(President = President, words = word_stem) %&gt;%\n  group_by(period) %&gt;%\n  count(words, sort = TRUE) %&gt;%\n  cast_dtm(period, words, n)\ndata_dtm\n\n# transform to tm (adjust the parameters)\nk = 10      # number of topics\nalpha = 2   # how many topics may dominate each text\ndata_tm &lt;- LDA(data_dtm, k = 10, alpha = 2) \n  # LDA: Latent Dirichlet Allocation (algorithm)\n  # choosing the number of topics is a bit of an art\n\n?LDA\n\n# look at the output of the topic model\nstr(posterior(data_tm))\n\n# highest words in each topic (do the topics make sense for a human?)\nterms(data_tm, 15)\n\n# transform the output back to the tidy format (so we can better work with it)\n# install.packages(\"tidyr\")\n# install.packages(\"reshape2\")\nlibrary(tidytext)\nlibrary(tidyr)\n\nterms &lt;- tidy(data_tm, matrix = \"beta\")\nterms\n  # the topics are given numbers\n  # you can later \"name\" them \n\nwords_in_topics &lt;- terms %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\nwords_in_topics\n\nwords_in_topics %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    scale_y_reordered()\n\n\n\n\n#### Co-occurence\n  # Words that appear together (although not necessarily right next to each other)\n# install.packages(\"widyr\")\nlibrary(widyr)\n\ndata_adj &lt;- data %&gt;%\n  unnest_tokens(output = words, input = Text) %&gt;%   # tokenize\n  anti_join(stop_words, by = c(\"words\" = \"word\")) %&gt;%   # drop stop words\n  pairwise_count(words, Year, sort = TRUE)\n  # Count the number of times each pair of items appear together within a group\n\ndata_adj\n\n## Exercises:\n  # Analise the evolution of the importance of \"education\" (and related words) in the speeches\n  # What time of words show up in the context of education? Do they change over time?\n\n\nCorpuscle: corpuses\nhttps://clarino.uib.no/korpuskel/home"
  },
  {
    "objectID": "r-scripts/desc_bivar.html",
    "href": "r-scripts/desc_bivar.html",
    "title": "1.3. Comparing dimensions (bivariate statistics)",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n##### Bi-variate statistics #######\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# upload basic packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\n\n# import data\ndata &lt;- read_excel(\"data/paisley_data.xlsx\")\n\ndata2 &lt;- read_rds(\"paisley_v2.rds\")\n\n\n### Qualitative variables\n\n# Cross-tabulation / contingency table\nview(data)\n\ndata |&gt;\n  count(employed)\n\ndata |&gt;  \n  count(sex, employed)\n\ndata |&gt;  \n  count(sex, employed) |&gt;\n  pivot_wider(names_from = employed, values_from = n)\n\n\ndata |&gt;  \n  count(sex, employed) |&gt;\n  pivot_wider(names_from = employed, values_from = n) |&gt;\n  mutate(total = employed + unemployed,\n         emp_perc = 100*employed/total,\n         unemp_perc = 100*unemployed/total) |&gt;\n  mutate_if(is.numeric, round, 1) |&gt;\n  relocate(sex, employed, emp_perc, unemployed, unemp_perc, total)\n\n\nlibrary(modelsummary) # ready-made solution\ndata |&gt;\n  filter(!is.na(employed)) |\n  datasummary_crosstab(sex ~ employed, data = _)\n\n\n# Visualisation\n\ndata |&gt; \n  filter(countryb == \"scotland\" & age&gt;=16) |&gt;  \n  ggplot(aes(x = lit_rank)) + \n    geom_bar() +\n    coord_flip() +\n    facet_wrap(~ sex, nrow = 1)\n\ndata |&gt; \n  filter(countryb == \"scotland\" & age&gt;=16) |&gt;  \n  ggplot(mapping = aes(x = lit_rank, y = after_stat(prop), group = sex)) + \n    geom_bar() +\n    coord_flip() +\n    facet_wrap(~ sex, nrow = 1)\n\ndata |&gt; \n  filter(countryb == \"scotland\" & age&gt;=16 & !is.na(lit_rank)) |&gt;\n  ggplot(aes(x = lit_rank, y = after_stat(prop), \n             fill = sex, group = sex)) + \n  geom_bar(position = \"dodge2\") +\n  coord_flip()\n\n?ggplot\n\n# 3 variables simultaneously\n\ndata |&gt; \n  filter(countryb == \"scotland\" & age&gt;=16 & !is.na(lit_rank)) |&gt;\n  ggplot(aes(x = lit_rank, y = after_stat(prop), \n             fill = sex, group = sex)) + \n  geom_bar(position = \"dodge2\") +\n  coord_flip() +\n  facet_wrap(~ countryb, nrow = 1)\n\n\n### Comparing numerical variables by group\n\n# Comaparing means (average)\ndata\n\ndata |&gt;\n  ggplot(aes(x = height)) + \n  geom_histogram()\n\n\n\ndata |&gt; \n  filter(countryb!=\"overseas\") |&gt;\n  group_by(countryb) |&gt;\n  summarize(\n    obs = sum(!is.na(height)),\n    mean_height = mean(height, na.rm = TRUE)) |&gt;\n  mutate(mean_height = round(mean_height, 1))\n\n# Visually (not very useful)\n\ndata |&gt; \n  filter(countryb!=\"overseas\") |&gt;\n  group_by(countryb) |&gt;\n  summarize(mean_height = mean(height, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = countryb, y = mean_height)) + \n  geom_point()\n\n# Comparing distributions\n\ndata |&gt;   \n  filter(countryb!=\"overseas\") |&gt;\n  ggplot(aes(x = height)) +\n  geom_histogram(binwidth = 5) +\n  facet_wrap(~ countryb, nrow = 1)\n  # but very different sample sizes\n\ndata |&gt;   \n  filter(countryb!=\"overseas\") |&gt;\n  ggplot(aes(x = height, y = after_stat(density))) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~ countryb, nrow = 1)\n  # difficult to eyeball differences between graphs\n\n# Overlapping histograms (kernel density graphs)\ndata |&gt;   \n  ggplot(aes(x = height, y = after_stat(density))) +\n  geom_histogram(binwidth = 5) +\n  geom_density(kernel = \"gaussian\", color = \"red\", size = 1.5)\n\ndata |&gt;   \n  filter(countryb!=\"overseas\") |&gt;  \n  ggplot(aes(x = height, colour = countryb)) +\n  geom_density()\n\n# Boxplots\n  # another way of comparing distributions\n\ndata |&gt;   \n  filter(countryb!=\"overseas\") |&gt;  \n  ggplot(aes(x = countryb, y = height)) +\n  geom_boxplot()\n\n# the solid line depicts the median value (50th percentile)\n# the box contains 50 per cent of the observations \n# (those contained between the percentiles 25th and 75th (IQR).\n# the vertical lines that extend below and above the box \n# reflect observations falling within 1.5 the interquartile range\n# the black dots refer to the extreme values that all outside that range \n\ndata |&gt;   \n  filter(countryb!=\"overseas\") |&gt;  \n  ggplot(aes(x = countryb, y = height)) +\n  geom_boxplot() +\n  geom_jitter(alpha = 0.2, width=0.10, height=0.05)\n\n\n## Compositional effects\n\ndata |&gt; \n  filter(countryb!=\"overseas\") |&gt;\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50) |&gt;\n  group_by(countryb) |&gt;\n  summarize(\n    obs = sum(!is.na(height)),\n    mean_height = mean(height, na.rm = TRUE)) |&gt;\n  mutate(mean_height = round(mean_height, 1))\n\n\n## With qualitative variables (able to write)\n\ndata |&gt; \n  mutate(write = ifelse(lit_adj==\"write\", 1, 0)) |&gt;\n  filter(countryb!=\"overseas\" & sex==\"male\" & age&gt;18) |&gt;\n  group_by(sex, countryb) |&gt;\n  summarize(\n    obs = sum(!is.na(write)), \n    write = round(mean(write, na.rm = TRUE), 3))\n\n\n## Comparing multiple groups\n\ndata |&gt; \n  group_by(sex, countryb) |&gt;\n  summarize(\n    obs = sum(!is.na(height)), \n    height = mean(height, na.rm = TRUE)) |&gt; \n  mutate_if(is.numeric, round, 2) |&gt;\n  pivot_wider(names_from = sex, values_from = c(\"obs\", \"height\")) |&gt;\n  relocate(countryb, obs_male, height_male, obs_female, height_female)\n\n\n# Visually\n\ndata |&gt; \n  group_by(sex, countryb) |&gt;\n  summarize(\n    obs = sum(!is.na(height)), \n    height = mean(height, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = countryb, y = height, color = sex)) +\n  geom_point()\n\n### Numerical variables\n\n# Tables (class intervals)\n\ndata &lt;- data |&gt;\n  mutate(weight_kg = 0.453592*weight)\n\ndata |&gt; \n  mutate(age_class = cut(age, breaks = seq(9, 89, 10))) |&gt;  \n  filter(age&gt;=20) |&gt;\n  group_by(age_class) |&gt;\n  summarise(obs = sum(!is.na(weight_kg)),\n            mean_weight = mean(weight_kg, na.rm = TRUE))\n\n# Line graph\ndata |&gt; \n  filter(age&gt;=20) |&gt;\n  mutate(age_class = cut(age, breaks = seq(20, 90, 10))) |&gt;\n  group_by(age_class) |&gt;\n  summarise(obs = sum(!is.na(weight)),\n            mean_weight = mean(weight, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = age_class, y = mean_weight, group = 1)) +\n  geom_line()\n\n## Do it by age (instead of age_class)\n\n# Scatter plot\ndata |&gt;\n  filter(age&gt;=20) |&gt;\n  ggplot(aes(x = age, y = weight)) +\n  geom_point()\n\ndata |&gt;\n  filter(age&gt;=20) |&gt;\n  ggplot(aes(x = age, y = weight_kg)) +\n  geom_point() +\n  stat_summary(geom = \"line\", fun = \"mean\", color = \"red\")   \n\n# smoothing\ndata |&gt;\n  filter(age&gt;=20) |&gt;\n  ggplot(aes(x = age, y = weight_kg)) +\n  geom_point() +\n  stat_summary(geom = \"line\", fun = \"mean\", color = \"red\") +  \n  geom_smooth(se = FALSE)\n\n# distinguishing by a third dimension\ndata |&gt;\n  filter(age&gt;=20) |&gt;\n  ggplot(aes(x = age, y = weight_kg, color = sex)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n### Evolution over time (history!)\n\n# a table\nheight_year &lt;- data |&gt;\n  mutate(year_birth = year - age) |&gt;\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) |&gt;\n  group_by(year_birth) |&gt;\n  summarise(obs = sum(!is.na(height)),   \n            av_height = round(mean(height, na.rm = TRUE), 1))\nheight_year\n\n# visually\nheight_year |&gt;\n  ggplot(aes(x = year_birth, y = av_height)) +\n  geom_point(col = \"red\") +\n  geom_line()\n\n## how to reduce the year-to-year variation (noise)? \n### bins, moving average, smoothing\n\n# creating bins\ndata |&gt; \n  mutate(year_birth = year - age) |&gt;\n  mutate(year_birth_5 = cut(year_birth, breaks =  seq(1795, 1885, 5))) |&gt;\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) |&gt;\n  group_by(year_birth_5) |&gt;\n  summarise(\n    av_height = mean(height, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year_birth_5, y = av_height)) +\n  geom_point() + geom_line(group = 1)  +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n# Given that the variable capturing the 5-year cohorts is a factor, \n# we need `group = 1` to let `ggplot` know that all the observations \n# should be treated as part of the same group, so the line can connect the points.\n\n# Moving average (lags and leads)\n\nheight_year &lt;- height_year |&gt;\n  arrange(year_birth) |&gt;\n  mutate(\n    lag1 = lag(av_height, 1),\n    lag2 = lag(av_height, 2),\n    lead1 = lead(av_height, 1),\n    lead2 = lead(av_height, 2))\nheight_year\n\nheight_year |&gt;\n  mutate(\n    ma3 = (lag1 + av_height + lead1) / 3,\n    ma5 = (lag2 + lag1 + av_height + lead1 + lead2) / 5) |&gt;\n  ggplot(aes(x = year_birth)) +\n  geom_point(aes(y = av_height)) +\n  geom_line(aes(y = av_height)) +\n  geom_line(aes(y = ma3), color = \"red\") +\n  geom_line(aes(y = ma5), color = \"blue\")\n\n\n# Smoothing the trend (polynomial)\nheight_year |&gt;\n  ggplot(aes(x = year_birth, y = av_height)) +\n    geom_point() +\n    geom_line() +\n    geom_smooth(se = FALSE)\n\n# Comparing groups\ndata |&gt; \n  mutate(year_birth = year - age) |&gt;\n  filter(countryb==\"scotland\" & age&gt;=20 & age&lt;=50) |&gt;\n  group_by(year_birth, sex) |&gt;\n  summarise(\n    av_height = mean(height, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year_birth, y = av_height, color = sex)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n  # try adjusting the smoothing parameter: span (0.25, 1.40)\n\ndata |&gt; \n  mutate(year_birth = year - age) |&gt;\n  filter(countryb==\"scotland\" & age&gt;=20 & age&lt;=50) |&gt;\n  group_by(year_birth, sex) |&gt;\n  summarise(\n    av_height = mean(height, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year_birth, y = av_height, color = sex)) +\n  geom_point() +\n  geom_smooth(span = 0.25, se = FALSE)\n\n## showing all the raw data (but careful with the scale)\ndata |&gt; \n  mutate(year_birth = year - age) |&gt;\n  filter(countryb==\"scotland\" & sex==\"male\" & age&gt;=20 & age&lt;=50) |&gt;\n  ggplot(aes(x = year_birth, y = height)) +\n  geom_point() +\n  stat_summary(geom = \"line\", fun = \"mean\") +\n  geom_smooth(se = FALSE)\n\n# Other statistics\n\ndata |&gt;  \n  mutate(year_birth = year - age) |&gt;\n  filter(age &gt;= 18 & sex==\"male\" & year_birth&gt;=1800) |&gt;\n  group_by(year_birth) |&gt;\n  summarise(sd = sd(height, na.rm = TRUE),\n            mean = mean(height, na.rm = TRUE)) |&gt; \n  mutate(cv = sd/mean) |&gt;\n  ggplot(aes(x = year_birth, y = cv)) +\n  geom_point() +\n  geom_line() + \n  geom_smooth(se = FALSE)\n\n# qualitative variables: dummy variables\n\ndata |&gt;\n  mutate(write = ifelse(lit_adj==\"write\", 1, 0)) |&gt;\n  mutate(year_birth = year - age) |&gt;  \n  filter(age&gt;=18 & age&lt;=50) |&gt;\n  group_by(year_birth, sex) |&gt;\n  summarise(\n    mean = mean(write, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = year_birth, y = mean, color = sex)) +\n  geom_smooth(se = FALSE) +\n  geom_point()"
  },
  {
    "objectID": "r-scripts/mult_reg.html",
    "href": "r-scripts/mult_reg.html",
    "title": "4.1. Multiple regression analysis",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# Install packages (if needed)\n# install.packages(\"tidyverse\")\n# install.packages(\"xlsx\")\n# install.packages(\"writexl\")\n\n# Load the packages you need\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(modelr)\nlibrary(ggplot2)\nlibrary(moderndive)\nlibrary(modelsummary)\nlibrary(marginaleffects)\n\n# Import the data\ndata &lt;- read_rds(\"data/paisley_v2.rds\")\n\n\n#### MULTIPLE REGRESSION ANALYSIS \n\n# Regression analysis goes beyond and allows:\n# (1) assessing the actual impact of X on Y: coefficient b\n# (2) computing how much of the variation in Y is explained by X (or Xs): R-squared\n# (3) allows controlling directly for the effect of other variables ***********************\n\n# Prepare the data (just in case)\ndata &lt;- data %&gt;%\n  mutate(\n    weight_kg = 0.453592*weight,\n    height = 30.48*feet+2.54*inches,\n    male = if_else(sex==\"male\", 1, 0),\n    england = if_else(countryb==\"england\", 1, 0),\n    ireland = if_else(countryb==\"ireland\", 1, 0),\n    scotland = if_else(countryb==\"scotland\", 1, 0),\n    overseas = if_else(countryb==\"overseas\", 1, 0),\n    read = recode(lit_adj,\n                  \"illiterate\" = 0,\n                  \"read\" = 1,\n                  \"write\" = 1),\n    write = if_else(lit_adj==\"write\", 1, 0),\n    urban = case_when(born==\"glasgow\" ~ 1,\n                           born==\"port glasgow\" ~ 1,\n                           born==\"edinburgh\" ~ 1, \n                           born==\"liverpool\" ~ 1,\n                           born==\"dublin\" ~ 1,\n                           born==\"london\" ~ 1,\n                           born==\"barcelona\" ~ 1,\n                           born==\"birmingham\" ~ 1,\n                           born==\"manchester\" ~ 1,\n                           TRUE ~ 0))\n\n# Multiple explanatory variables\nreg &lt;- data %&gt;%\n  filter(age&gt;=20 & countryb!=\"overseas\") %&gt;% \n  lm(weight_kg ~ age + height + male + write, data = .)\n\nget_regression_summaries(reg) %&gt;%\n  select(nobs, r_squared, adj_r_squared) # R-squared\n\nreg %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate) # regression coefficients\n\nreg %&gt;%\n  get_regression_table() %&gt;%\n  select(!statistic) # tables including standard errors, confidence intervals, etc.\n\n\n# Visualising regression results\n\ndata %&gt;%\n  filter(age&gt;=20 & countryb!=\"overseas\") %&gt;% \n  ggplot(aes(x=age, y=weight_kg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n  \n\n## two possibilities\nlibrary(marginaleffects)\nreg %&gt;%\n  plot_predictions(\"age\") # the link between one X and Y \n\nlibrary(modelsummary)\n\nmodelplot(reg, coef_omit = 'Interc') \n  # all the coefficients together\n  # all the coefficients simultaneously\n\n### What happens to the model when we add more explanatory variables??\n  # (1) it usually increases the explanatory power: R-squared \n  # (2) the effect of the other variables may change \n  # (3) the estimations may become noisier \n\n# Illustration: three regresssions explaining heights (restricted sample)\nsample &lt;- data %&gt;%\n  filter(age&gt;=20 & age&lt;=30 & countryb!=\"overseas\" & countryb!=\"ireland\")\n\nreg1 &lt;- sample %&gt;%\n  lm(height ~ write, data = .)\nreg2 &lt;- sample %&gt;%\n  lm(height ~ write + male, data = .)\nreg3 &lt;- sample %&gt;%\n  lm(height ~ write + male + england, data = .)\n\nmodelsummary(\n  list(\n    \"M1\" = reg1,\n    \"M2\" = reg2,\n    \"M3\" = reg3),\n  statistic = c(\"s.e. = {std.error}\", \"p-value = {p.value}\"),\n  stars = c(\"*\" = .1, \"**\" = .05, \"***\" = 0.01),\n  gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"),\n  fmt = 3,\n  coef_omit = \"Intercept\",\n  note = \"For simplicity, the intercept is omitted.\") \n\n## (1) Adding more variables increase the R-squared providing the new variables are relevant. \n\n## (2) Adding an additional variables to the model may change the coefficients of the other variables. \n  # omitted variable bias: if an omitted variable (Z) is correlated with both X and Y, \n    # the regression coefficient is unreliable:\n      # b not only reflects the effect of X on Y, but it also partly captures the effect of Z on Y\n    # controlling for Z (including Z in the model) addresses this issue\n\n# the effect of adding additional variables to the existing coefficients basically reflects:\n  # --the correlation between the added variable and the dependent variable (Y)\n  # --the correlation between the added variable and the previously existing explanatory variables.\n\n# correlation matrix\ndata %&gt;%\n  filter(age&gt;=20 & age&lt;=30 & countryb!=\"overseas\" & countryb!=\"ireland\") %&gt;%  \n  select(height, write, male, england) %&gt;%\n  datasummary_correlation()\n\n## (3) The estimations may become noisier\n\n# adding more variables increases the number of coefficients to be estimated with the same information (sample size). \n  # reducing the degrees of freedom\n\n# the regression coefficients are computed using only independent information (exclusive to one particular variable). \n  # If two of the explanatory variables are correlated between each other, they share information. \n  # The procedure behind regression cannot use the shared information to estimate their coefficients, \n  # so that information is discarded and $b$ is estimated using only the *independent* information remaining. \n  # Having less information increases the standard errors, thus reducing the accuracy of the estimations: \n  # confidence intervals wider and p-values become larger\n\n\n## Regression tables: very useful to compare several models in just one table  \n  # assess how our results behave under different specifications: \n    # including more/less variables\n    # differences across groups or time-periods, \n    # excluding outliers, \n    # etc. \n\nreg_all &lt;- data %&gt;%\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age + sex + height + england + ireland + overseas, data = .)\nreg_males &lt;- data %&gt;%\n  filter(age &gt;= 20 & sex==\"male\") %&gt;%\n  lm(weight_kg ~  age + height + england + ireland + overseas, data = .)\nreg_females &lt;- data %&gt;%\n  filter(age &gt;= 20 & sex==\"female\") %&gt;%\n  lm(weight_kg ~ age + height + england + ireland + overseas, data = .)\nreg_all2 &lt;- data %&gt;%\n  filter(age&gt;=20 & age&lt;=50 & countryb!=\"overseas\") %&gt;%\n  lm(weight_kg ~ age + sex + height + england + ireland + overseas, data = .)\nreg_males2 &lt;- data %&gt;%\n  filter(age &gt;= 20 & sex==\"male\" & age&lt;=50 & countryb!=\"overseas\") %&gt;%\n  lm(weight_kg ~  age + height + england + ireland + overseas, data = .)\nreg_females2 &lt;- data %&gt;%\n  filter(age &gt;= 20 & sex==\"female\" & age&lt;=50 & countryb!=\"overseas\") %&gt;%\n  lm(weight_kg ~ age + height + england + ireland + overseas, data = .)\n\nmodelsummary(\n  list(\n    \"1. All\" = reg_all,\n    \"2. Males\" = reg_males,\n    \"3. Females\" = reg_females,\n    \"4. All\" = reg_all2,\n    \"5. Males\" = reg_males2,\n    \"6. Females\" = reg_females2),\n  statistic = \"std.error\",\n  stars = c(\"*\" = .1, \"**\" = .05, \"***\" = 0.01),\n  gof_map = c(\"nobs\", \"r.squared\"),\n  coef_omit = \"Intercept\",\n  fmt = 2,\n  note = \"For simplicity, the intercept is omitted.\")\n\n\n\n## Quantifying time (controlling for other confounders)\n\nsample &lt;- data %&gt;%\n  mutate(year_birth = year - age) %&gt;%\n  filter(age&gt;=20 & year_birth&gt;1795)\n\nm1 &lt;- sample %&gt;%\n  lm(height ~ year_birth, data = .)\n\nm2 &lt;- sample %&gt;%\n  lm(height ~ year_birth + sex + england + ireland + overseas, data = .)\n\nmodelsummary(\n  list(\n    \"(1)\" = m1,\n    \"(2)\" = m2),\n  statistic = \"std.error\",\n  stars = c(\"*\" = .1, \"**\" = .05, \"***\" = 0.01),\n  gof_map = c(\"nobs\", \"r.squared\"),\n  coef_omit = \"Intercept\",\n  fmt = 2,\n  note = \"For simplicity, the intercept is omitted.\")  \n\n\n\n### Choosing which variable introduce in the analysis should be theoretically justified\n\n# Also:\n  # introducing additional *relevant* variables (Z) is helpful for two main reasons:\n    # -- adds more information, thus improving the accuracy of our estimations \n          # (if X and Z are not correlated)\n    # -- it prevents the **omitted variable bias** if X and Z are correlated. \n          # if an omitted variable (Z) is correlated with both X and Y, \n            # the regression coefficient is unreliable\n  \n  # however, adding more variables may also reduce accuracy due to:\n    # -- reducing the degrees of freedom (df=n-k-1)\n    # -- multicolinearity problems: \n      # if X and Z are correlated, coefficients are estimated with less *independent* information, \n        # which increases the standard errors and reduces the likelihood of being statistically significant \n          # (confidence intervals also become wider)\n\n  # The omitted variable bias is arguably a much worse problem than multicolinearity, \n\n\n\n### Challenges to regression analysis\n  # Non-linear relationships (functional form)\n  # Parameter stability\n  # Outliers\n\n### Correlation is not causation\n# Role of omitted variables\n# Reverse causality\n\n### Other issues\n# Number of observations (noise)\n# Categories employed\n# Garbage in, garbage out (the results are as good as the data itself)\n\n\n# Functional form:\ndata %&gt;%\n  ggplot(aes(x=age, y=weight_kg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nreg &lt;- data %&gt;%\n  mutate(age_sq = age*age) %&gt;%\n  lm(weight_kg ~ age + age_sq, data = .)\n  \ndata %&gt;%\n  ggplot(aes(x=age, y=weight_kg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2))\n\ndata %&gt;%\n  ggplot(aes(x=age, y=weight_kg)) +\n  geom_point() +\n  geom_smooth()"
  },
  {
    "objectID": "r-scripts/chall_reg.html",
    "href": "r-scripts/chall_reg.html",
    "title": "4.2 Challenges to regression",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here."
  },
  {
    "objectID": "other-data.html",
    "href": "other-data.html",
    "title": "Other historical datasets",
    "section": "",
    "text": "Below you can find a curated set of varied historical datasets that you can use for the class assignments and your own research projects. Please make sure to read the original article describing the information. Digitising historical sources constitutes a huge effort, so we should especially thanks (and properly acknowledge) those authors who make their datasets public."
  },
  {
    "objectID": "other-data.html#international-data",
    "href": "other-data.html#international-data",
    "title": "Other historical datasets",
    "section": "International data",
    "text": "International data\n\nHansard dataset: UK Parliamentary speeches between 1806 and 1911 (over 1 million parliamentary speeches). This is a huge dataset that might be computationally too demanding on some machines. Find here a random sample of 10,000 speeches. See Blaxill (2020) and Guldi (2023) for illustrations using this information.\nPopulation censuses. IPUMS International holds individual-level information from a variety of historical population censuses (Ruggles et al. 2025). The same or even more extended datasets can be requested to the respective national agencies).\nCivil and parish registers. The European Historical Samples Network provides micro-data on individuals (families and households) taken from birth (and baptisms), marriage and death records (and the like). This information allows following individuals through their life courses and study topics such as fertility, mortality, age at marriage and partner choice, social mobility and migration, among others. See Alter and Mandemakers (2014) and Quaranta (2021).\nThe Proceedings of the Old Bailey (Hitchcock et al. 2023). This dataset contains the texts of 197,752 trials held at London’s central criminal court (The Old Bailey) between 1674 and 1913. As well as the texts themselves, the data has been labeled, so it allows identifying defendants, offences, victims, verdicts and sentences. See, for instance, Hitchcock and Turkel (2016) for an application.\nVOC Dataset (Petram et al. 2024): This dataset stores the pay ledgers of the Dutch East India Company’s (VOC), primarily from the eighteenth century. It contains almost 800,000 records containing each crew member’s name, place of origin, rank, wage, etc. The raw information has been carefully curated and stored in several .csv files that can be merged together using the corresponding IDs. Read more about this source here.\nTudor Network of Power (Ahnert et al. 2023). This data contains all (surviving) items of correspondence in the Tudor State Papers (1509-1603), which are the official government records of the Tudor period in England. As explained by the authors (Ahnert and Ahnert 2023), data cleaning and curation constituted a significant effort. As well as more traditional quantitative methods, this data set is suited for the network analysis.\nAcademich scholars and literati in Medieval and Early Modern Europe (De La Croix, n.d.). Relational database on around 83,000 scholars and literati active in European Academia between 1000 and 1800. As well as place and year of birth and details, it details to which institutions these individuals belonged (universities, scientific academies, etc.). See De La Croix, Scebba, and Zanardello (2025) and De La Croix and Morault (2025) for two applications using social network analysis.\nHomicide in Chicago, 1870-1930. This dataset consists on 11,0000 homicide reports filed by Chicago Police Department during the period of study, thus allowing the study of homicide, crime, urban development, and the police themselves. See Bienen and Rottinghaus (2003) (available here) for a description of the project and the dataset.\nAfrican Names Dataset (Altis 2021). This dataset contains information on 91,491 Africans taken from captured slave ships or from African slave trading sites after 1807 as a result of the British navy’s attempt to suppress the slave trade. It includes names, stature, sex, age, country of origin, the vessel involved, year of arrival, and ports of embarkation and disembarkation.\nTatoos dataset (Project, n.d.): Almost 60,000 British convict records from 1793 to 1925. As well as other personal information (age, gender, occupation, religion), these records contain physical descriptions of convict bodies, including their tattoos and other marks (i.e. scars). See additional info here and Alker and Shoemaker (2022).\nLondon Lives (Hitchcock et al., n.d.): 240,000 manuscripts from a wide range of primary sources between 1690 and 1800 that allow studying the ordinary life of Londoners (crime, poverty, illness, apprenticeship, work, politics, etc.). See also Hitchcock and Shoemaker (2020) (available here).\nPetitioning in Early Modern England (Waddell and Howard 2022). This dataset consists of 2,847 petitions filed in England between 1573 and 1799. As well as the text itself, it includes information on date, petitioners, topic, administrative responses, etc. Petitions were a crucial mode of communication between the ‘rulers’ and the ‘ruled’, so they provide a vital source for illuminating the concerns of the people, from noblemen to paupers. The data is hosted in this repository.\nThe Google Books Project has digitised millions of books published from the late 17th century onwards (coverage is uneven) and the Google Books Ngram Viewer allows counting the number of times that a particular term or terms appear in the corpus (Michel et al. 2011). The R package ngramr mimics the functionalities of the latter but directly from within the R environment. It extracts data from the Google corpus and provides it in the form of an R dataframe, which can subsequently be treated with the tools you are familiar with. This corpus should be though used with caution. See Pechenick, Danforth, and Dodds (2015) and Schmidt, Piantadosi, and Mahowald (2021) for its biases and limitations to study socio-cultural and linguistic evolution.\nThe HathiTrust Library also contains millions of digitised texts and an online tool for single word queries (bookworm). The underlying data can be downloaded by request. The hathiTools R package (Marquez and Schmidt 2022) allows to interact directly with these resources."
  },
  {
    "objectID": "other-data.html#norwegian-data",
    "href": "other-data.html#norwegian-data",
    "title": "Other historical datasets",
    "section": "Norwegian data",
    "text": "Norwegian data\nThe National Biblioteket has million of digitised resources (books, manuscripts, newspapers, letters, etc.). As well as hosting an online norsk n-gram for quering the corpus, the DH-LAB is also creating its own tools for accessing and analysing their collections (including R packages). In any case, if you are interested in a particular period, you can directly contact them and request a particular set of texts.\nStatistics Norway has a collection of historical statistics on population, health, education, income, prices, manufacturing, transportation and communication and the environment, among many other dimensions. They usually refer to governmental reports scanned in pdf form. It is often the case that these sources have also been properly digitised.\nThe Kommunedatabasen also has digitised a huge amount of historical information on municipalities (kommuner).\nThe HistLab, hosted at the University of Tromsø, stores individual-level information extracted from population censuses (1801-1920), parish registers (baptisms, marriages and deaths) or land registers (1838, 1886). Contact them directly to request the digitised records.\nOther additional sources can be found below:\n\nThe Norwegian Parliamentary Debates Dataset (Fiva, Nedregård, and Øien 2025). This dataset includes all speeches delivered in the Norwegian Parliament between December 1945 and June 2024 (almost one million speeches). As well as the text itself, it includes information on date, speaker, political and regional affiliation, etc.). Given the size of this dataset, we include here a 5 per cent random sample. You can find the whole dataset here.\nNorwegian parliamentary elections from 1906 to 2013: candidate-level observations for all candidates from all parties since the 1906 election (2024 version; (Fiva and Smith 2017)): here in .dta format (requires read_dta() from the package haven)."
  },
  {
    "objectID": "other-data.html#miscellaneous",
    "href": "other-data.html#miscellaneous",
    "title": "Other historical datasets",
    "section": "Miscellaneous",
    "text": "Miscellaneous\nThose students with other research interests can choose their dataset on their own. The possibilities are endless. Here are just a few examples:\n\nFriends Dataset (Hvitfeldt 2020). The complete transcripts from the famous TV show (1994-2004). This dataset is available by installing and loading the R package friends. Although the object itself is not visible in the environment, the object “friends” is implicitly in your environment, so you will have access to it just by typing “friends”. You could in any case create an object with the data yourself (data &lt;- friends). The dataset does not have a explicit “time” variable but you could easily create your own using the information on “season” (and perhaps “episode”). More info here or here.\n\nAs mentioned above, I encourage you to find your own dataset."
  },
  {
    "objectID": "get-r.html",
    "href": "get-r.html",
    "title": "Intro to R",
    "section": "",
    "text": "Implementing quantitative or computational analyses to historical (or any other) data requires some sort of statistical software. We will rely on R, a open-source free statistical software widely used by practitioners in many different fields both inside and outside academia. Although it is possible to work directly in R, using an integrated interface such as RStudio makes things much easier. RStudio basically integrates a text editor with the R console, so you can write, run and see the results of your analyses more easily.\nWe will be therefore download and install both software (R and RStudio) in your computer. Here is a link to access them (the version you install depends on whether you are using Windows or Mac)."
  },
  {
    "objectID": "get-r.html#r-interface",
    "href": "get-r.html#r-interface",
    "title": "Intro to R",
    "section": "R interface",
    "text": "R interface\nFigure 1 below illustrates what RStudio looks like. As well as the different menus at the top, the interface is divided in four panels. We will use the upper-left panel to write the code that instructs R to perform the analyses. You therefore need to open a new R Script (from the menu or clicking the icon). While it is also possible to directly type the commands in the console, using a script allows easily saving and replicating our work later on. While most of the results will show up in the console in the lower-left panel, the tab plots in the lower-right panel reproduces the visualisations we implement (by contrast, the tab files shows the structure of the directory where we are working on). Lastly, the environmment, located in the upper-left panel displays the objects that we load into the system.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: RStudio Interface.\n\n\n\nAs mentioned above, we will use a script to document our work. Using scripts makes it easy to keep a record of the commands you use, which in turn facilitates replicating (or adjusting) your analyse later, which will save you so much time (you can also re-use your old scripts or borrow parts to use in other projects)."
  },
  {
    "objectID": "get-r.html#setting-up-the-stage",
    "href": "get-r.html#setting-up-the-stage",
    "title": "Intro to R",
    "section": "Setting up the stage",
    "text": "Setting up the stage\nGiven that R needs some basic information regarding which folder in our computer (or the cloud) we are working from and which set of tools we are going to use, scripts usually start in a similar way. The code below includes some useful preparatory commands. You can copy and paste it in your own script.\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# Set working directory\ngetwd() # Provides the current working directory\nsetwd(\"~/Documents/quants\") \n  # sets the working directory to the appropriate folder\n\n# Install packages\ninstall.packages(\"tidyverse\")\n\n# Load packages\nlibrary(tidyverse)\n\nNotice that the symbol # allows creating “comments” (the characters turn green). R does not “read” these lines when implementing the code, so they can be used to both better structure our scripts and comment the code itself.\nA couple of further clarifications are though in order. Firstly, it is often difficult to type the correct directory path (the one above is the one in my computer). Setting it manually through the menu helps properly selecting the folder you want to be working from: Session/Set working directory/Choose directory... Implementing this operation actually runs the necessary command in the console, so you can actually see the path to your folder or copy and paste it into the script (as it will be type in the script, you won’t need to do this again). I cannot emphasise enough how important this step is, since it indicates R where in our computer we will be working (so it is easy to access the necessary files). In this regard, it is important to have a folder structure (data, results, etc.) that facilitates navigating through your files.\nSecondly, the way that R works is by relying on tools that are contained in different packages. There are many of these packages and they not only need to be installed (only once), but also open before using (each session). This is why we need the commands install.packages() and library() to indicate which packages need to be installed and opened. The package tidyverse, in particular, gathers together different packages that are commonly used like ggplot2, dplyr, readr, etc. (it is therefore not necessary to install and load those packages individually). We will keep incorporating different packages as we need them.\nYour RStudio interface should look like something like Figure 2, except for the fact that the command setwd() should include the path to your own working directory. We can now run these lines of code selecting them and using the icon Run. Clicking it makes R go through those command lines in sequence (or through the whole script if you indicate so). It first clears the environment, then sets the working directory and lastly installs and opens the package tidyverse. The results of these commands happen in the console panel, which gives some comments that we can safely ignore for now.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Starting a R script."
  },
  {
    "objectID": "get-r.html#importing-data-sets",
    "href": "get-r.html#importing-data-sets",
    "title": "Intro to R",
    "section": "Importing data sets",
    "text": "Importing data sets\nAs well as the appropriate software (R and RStudio), this session uses two historical data sets to illustrate the concepts and methods covered here (you should have downloaded them). In order to start exploring this information, we first need to import the data into the R environment.\nAs an illustration, let’s focus on the Paisley dataset, one of the historical sources we will be exploring here. The raw data is stored as an excel spreadsheet named “paisley-data.xlsx” in the folder “data”. The command readxl() imports this excel file into the R environment. Although readxl is contained in the tidyverse package, you still need to open it explicitly using library().\n\nlibrary(readxl)\ndata &lt;- read_excel(\"data/paisley-data.xlsx\")\n\nNotice that we are instructing R to find the file that is located in a particular folder. We are using a relative path that stems from where your project is saved in (if the data is in the same working directory, you don’t need the path, just the file name).1 Notice also the symbol &lt;- (called assignment operator). It serves to create a (temporary) object, named data containing the Paisley data, which is now in the “environment” we are working with. The name of the object is up to you, we call it “data” but it could be anything else with certain restrictions (i.e. not starting with a number). One neat R feature is that you can load many objects simultaneously (with different pieces of information each) and treat them separately depending on your needs (we will see that we can “create” those objects ourselves as results of our analyses).\nOnce an object has been created containing the data frame, it is listed in the upper-right window called environment. Typing the name of the object (data in this case) provides a peak at the underlying information. As shown below, the upper left corner indicates that this data frame (referred to as a tibble in the tidyverse terminology) contains 1,000 individuals (rows) and 21 fields (columns). By default, typing the name of the object (data) only provides information on the 10 first cases in order to save memory and space (imagine that your dataset contains 10 million observations!). The number of fields that are displayed depends on how much space is available in the console in the lower part of the interface. You are nonetheless informed about the number of rows and variables that are not visible (as well as the names of those variables). Notice also that, below the variable name, R also indicates the type of variable: some are categorical (“chr” meaning character) and others are numerical (“dbl” meaning double).\n\ndata\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n# ℹ 990 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nIf you want to display more cases, you can use the function print(n = 15) and indicate the number of cases to be reported. Let’s pause for a moment to disect what the code below is doing. Notice first that it is crucial to indicate where the information you are asking for is coming from. Remember that we imported the Paisley data into the object name data. The pipe (|&gt;) here basically takes this object and uses it as input in the next line of code, which uses the function print() to request listing the first 15 cases from that object. Alternatively, you can have a look at the last 20 cases by typing tail(20).\n\ndata |&gt;  \n  print(n = 15)\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n11    11   343 may       1841 AGNES    CURRIE… fema…    35 islay scotland paisl…\n12    12   382 june      1841 ELLIZA   MUNN    fema…    18 john… scotland johns…\n13    13   425 june      1841 SARAH    BLACK … fema…    36 glas… scotland kelvi…\n14    57     3 january   1841 THOMAS   ROBERT… male     19 pais… scotland high …\n15    58    19 january   1841 JOHN     MONTGO… male     24 some… england  barra…\n# ℹ 985 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nThere are other commands that help knowing more about how the data set looks like such as glimpse() or names(). Feel free to try them out yourself. You can also have a sense of the magnitude and complexity of the whole data set by typing view(data). The latter opens up a new tab where you can explore the full data set at ease. Notice also that some values are missing. R signals them as NA (not available), meaning that no information is recorded in those fields for those observations. We will discuss the importance of missing values in due time. What it is important to stress now is that, although the Paisley data set is not especially big, scrolling up-down and left-right makes it obvious that it is extremely difficult to extract any kind of pattern by just “looking” at all this information. Here is where statistics (and R) come to the rescue. This is basically what this course will be about: a basic overview on how historians use computational methods to extract the rich information contained in our sources. If you have got this far, congrats, you are ready for it!"
  },
  {
    "objectID": "get-r.html#further-references",
    "href": "get-r.html#further-references",
    "title": "Intro to R",
    "section": "Further references",
    "text": "Further references\nAlexander, Rohan (2023), Telling stories with Data. With applications in R (CRC Press).\nIsmay, Chester and Kim, Albert Y. (2024), Statistical inference via Data Science. A ModernDive into R and the Tidyverse (CRC Press).\nSilge, Julia, and Robinson, David (2017), Text mining with R. A tidy approach (O’Reilly).\nWickham, Hadley, Çetinkaya-Rundel, Mine, and Grolemund, Garret (2023), R for Data Science (O’Reilly; 2nd edition)."
  },
  {
    "objectID": "get-r.html#footnotes",
    "href": "get-r.html#footnotes",
    "title": "Intro to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will work assuming the working directory is set to the appropriate folder (see previous step above). Although we strongly advice not to use menus, finding the appropriate path to your files is not always straightforward, so we make an exception here.↩︎"
  },
  {
    "objectID": "r-scripts/desc_num.html",
    "href": "r-scripts/desc_num.html",
    "title": "1.2. Numerical variables",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# upload basic packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\n\n# import data\ndata &lt;- read_excel(\"data/paisley_data.xlsx\")\n\ndata &lt;- read_rds(\"data/paisley_v2.rds\")\n\n############## Numerical variables ############\n\n# age, weight, height, ...\ndata\n\n# Reporting frequencies is not useful when there are many values\ndata |&gt;  \n  count(age) |&gt;              \n  mutate(perc = 100*n/sum(n)) |&gt;\n  print(n = Inf) # display all rows\n\npaisley_data |&gt; \n  count(age) |&gt;\n  print(n = Inf) # display all rows\n\n# Potential solution: Create class intervals (group values into bins)\ndata &lt;- data |&gt; \n  mutate(age_class = cut(age, breaks = 5)) # Creates groups of equal size\n\ndata |&gt; \n  count(age_class) |&gt;\n  print(n = Inf) # display all rows\n\n# or setting when the breaks happen yourself\ndata |&gt; \n  mutate(age_class = cut(age, breaks = c(0, 14, 19, 50, Inf))) |&gt;\n  count(age_class) |&gt;\n  mutate(perc = 100*n/sum(n)) # report relative frequencies as well\n\n# display the data visually with ggplot() and geom_col()\ndata |&gt;\n  mutate(age_class = cut(age, \n                         breaks = c(0, 14, 19, 50, Inf))) |&gt;\n  count(age_class) |&gt;\n  ggplot(aes(x = age_class, y = n)) +\n  geom_col()\n                         \n# or even assign \"labels\" to those groups\ndata |&gt; \n  mutate(age_class = cut(age, breaks = c(0, 14, 19, 50, Inf), \n                         labels = c(\"Children\", \"Youngters\", \"Adults\", \"Elderly\"))) |&gt;\n  count(age_class)\n  \n\n\n## Histogram (visual representation of the distribution)\ndata |&gt;   \n  ggplot(mapping = aes(x = age)) +\n    geom_histogram(binwidth = 5)\n\n# Play around with the width of the bin: accuracy vs noise\n  # ggplot uses the + operator (to add layers)\n  # 3 key aspects: data, aesthetics and type\n\ng1 &lt;- data |&gt;   \n  ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 1)\n\ng2 &lt;- data |&gt;   \n  ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 10)\n\n# put the graphs together\nlibrary(patchwork)\ng1 + g2\n\nggsave(\"output/hist_age.pdf\") # save plot\n\n# Focusing on particular subsamples\ndata |&gt;   \n  filter(sex == \"male\") |&gt; \n  ggplot() +\n  geom_histogram(mapping = aes(x = age), binwidth = 1)\n\n# Or by group\ndata |&gt;   \n  ggplot() +\n  geom_histogram(mapping = aes(x = age), binwidth = 5) +\n  facet_wrap(~ sex, nrow = 1)\n\n### Summarise: report summary statistics\n\n# count: n(), mean, median, min, max, sd, IQR, min, quantile(x, 0.25)...\n# first, nth(x, 2), last\n# !is.na(x), n_distinct(x)\n\n# average age\ndata |&gt; \n  summarize(mean_age = mean(age, na.rm = TRUE))\n  # na.rm removes missing values from the computations\n  # stands for \"NA remove\"\n  # if not included it results in NA because it cannot be computed\n  # instead of yielding missing outputs (check removing that condition)\n\n# first assign a name and then define what you want to do using functions\n\ndata |&gt; \n  summarize(\n    count = sum(!is.na(age)),   # \"!is.na\" indicates \"is not na\" (not available=missing value)\n    mean_age = mean(age, na.rm = TRUE), # the comma allows for asking for more statistics\n    min_age = min(age, na.rm = TRUE),\n    max_age = max(age, na.rm = TRUE),\n    p25_age = quantile(age, 0.25, na.rm = TRUE),\n    p75_age = quantile(age, 0.75, na.rm = TRUE)  \n  )\n\n\n# Illustration:\ndata |&gt;   \n  ggplot(mapping = aes(x = age)) +\n    geom_histogram(colour = \"grey70\", alpha = 0.2) +\n    geom_vline(aes(xintercept=29.6)) +\n    geom_vline(aes(xintercept=27)) +\n    geom_vline(aes(xintercept=9)) +  \n    geom_vline(aes(xintercept=89)) +\n    geom_vline(aes(xintercept=17)) +  \n    geom_vline(aes(xintercept=47)) + \n    geom_segment(aes(x = 29.6-12.1, y = 1, xend=29.6+12.1, yend = 1), color = \"red\", linewidth = 2) + \n    annotate(x=29.6, label=\"Mean\", y=142, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=27, label=\"Median\", y=150, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=9, label=\"Min\", y=150, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=89, label=\"Max\", y=150, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=17, label=\"p10\", y=150, colour=\"red\", geom = \"label\", size = 3) +  \n    annotate(x=47, label=\"p90\", y=150, colour=\"red\", geom = \"label\", size = 3) +\n    annotate(x=29.6, label=\"+1/-1 Standard deviation\", y=5, colour=\"red\", geom = \"label\", size = 3) +\n    scale_x_continuous(name = \"Age\", breaks = seq(0,90,10)) +\n    scale_y_continuous(name = \"Frequency\")\n\n\n# Shape of the distribution: differences\n\nlibrary(patchwork)\np1 &lt;- data |&gt;  \n  filter(age&gt;15) |&gt;\n  ggplot(mapping = aes(x = weight)) +\n  geom_histogram()\n\np2 &lt;- data |&gt;\n  filter(age&gt;15) |&gt;\n  ggplot(mapping = aes(x = age)) +\n  geom_histogram()\n\np1 + p2\n\n\n\n### Modifying and creating numerical variables\n\n# Inspect info on heights: feet, inches\n\ng1 &lt;- data |&gt;   \n  ggplot(aes(x = feet)) +\n  geom_histogram()\n\ng2 &lt;- data |&gt;   \n  ggplot(aes(x = inches)) +\n  geom_histogram()\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\ng1 + g2 # put the graphs together\n\n# something is going on\n\ndata |&gt; \n  subset(feet&gt;20) # Identify this case\n\n## Correct numerical value (typo)\ndata &lt;- data |&gt;\n  mutate(feet = replace(feet, case==99 & feet==50, 5))\n\n\n# check you dit it right\ndata |&gt; \n  subset(forename==\"FRANCIS\" & surname==\"GILFILLAN\", \n         select=c(casen, forename, surname, feet))\n\n\n# Create nex variable (height) combining both variables (feet & inches)\ndata &lt;- data |&gt;\n  mutate(height = 30.48*feet+2.54*inches)\n\nview(data)\n# a new variable has been added (last column on the right)\n\n# visualise new variable\ndata |&gt;  \n  ggplot() +\n  geom_histogram(mapping = aes(x = height), binwidth = 5)\n\n\n# Group a numerical variable into different bins\ndata &lt;- data |&gt;\n  mutate(height_bins = case_when(height &lt; 145 ~ 'low',\n                                 height &gt;=145 & height&lt; 175 ~ 'med',\n                                 height &gt;=175 ~ 'high'))\n# check\ndata |&gt; \n  group_by(height_bins) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(freq = n/sum(n))\n\n\n### Dummy variables \n\n# Allow quantifying qualitative variables\n\ndata |&gt;\n  count(literacy)\n\n  # example with case_when()\ndata &lt;- data |&gt;\n  mutate(write = case_when(\n    literacy==\"superior education\" ~ 1,\n    literacy==\"read & write well\" ~ 1,\n    literacy==\"read & write tolerably\" ~ 1,\n    is.na(literacy) ~ NA,\n    TRUE ~ 0))\n\ndata |&gt;\n  count(write)\n\ndata |&gt;\n  summarize(\n    obs = sum(!is.na(write)),\n    mean = mean(write, na.rm = TRUE),\n    sd = sd(write, na.rm = TRUE),\n    min = min(write, na.rm = TRUE),\n    max = max(write, na.rm = TRUE)) |&gt;\n  mutate_if(is.numeric, round, 3)\n\n  # examples with if_else\ndata |&gt;\n  count(countryb)\n\ndata &lt;- data |&gt;\n  mutate(scotland = if_else(countryb==\"scotland\", 1, 0)) \n\ndata |&gt;\n  select(countryb, scotland) \n\n# summary statistics\ndata |&gt; \n  summarize(\n    obs = sum(!is.na(scotland)),   \n    mean = mean(scotland, na.rm = TRUE), \n    sd = sd(scotland, na.rm = TRUE), \n    min = min(scotland, na.rm = TRUE),\n    max = max(scotland, na.rm = TRUE)) |&gt;\n  mutate_if(is.numeric, round, 3)\n\n  # the average of a dummy variable reflects the fraction (proportion) of \n    # observations belonging to that category \n  # it can also be interpreted as the probability of belonging to that group \n    # if an observation were selected at random.\n\n\n\n\n\n\n\n\n### More on graphing with ggplot\n# 3 key aspects: data, aesthetics (mapping) and type\npaisley_data |&gt;  \n  filter(age &gt;= 18) |&gt;\n  group_by(countryb) |&gt;\n  summarize(mean_height = mean(height, na.rm = TRUE)) |&gt;\n  ggplot(mapping = aes(x = countryb, y = mean_height)) +\n  geom_bar()\n# you can run chunks of code to see what the do so far\n# and identify where the problem lies\n# always consider what the unit of analysis is (when different levels)"
  },
  {
    "objectID": "r-scripts/desc_qual.html",
    "href": "r-scripts/desc_qual.html",
    "title": "1.1. Qualitative variables",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n#### Quantification in History ####\n\n# These scripts provide the code that help exploring the \"Paisley dataset\".\n# They are intended as an introductory guide to how quantification works\n# They can also be easily replicated\n\n# Notice that the symbol \"#\" allows creating \"comments\" (in green) to the code\n\n\n## Getting ready (preparatory commands)\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# Set working directory\ngetwd() # Provides the current working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\")\n  # Sets the working directory(THIS IN MINE; ***CHANGE IT TO YOURS***)\n\n  # it is sometimes difficult to type the correct directory path (the one above is mine)\n  # setting it manually through the menu helps (Session/Set working directory/Choose directory)\n  # copy and paste it into the script later, so you don't need to do it again\n\n  # once the working directory is set\n  # we use \"relative paths\" within this environment\n  # vreate a folder structure that makes working on this project easy\n    # data, results, etc.\n\n\n### Install packages (if needed; only once)\n# install.packages(\"tidyverse\")\n\n    # R works using different tools that are contained\n    # in different packages\n    # they need to be installed (only once)\n    # and open before using (each session)\n\n### Load the packages you need (you will need to do it every session)\nlibrary(tidyverse)\nlibrary(readxl)\n\n# Import data\ndata &lt;- read_excel(\"data/paisley-data.xls\")\n  # notice that we are using a \"relative path\"  \n  # if the data is in the working directory, you don't need the path (just the file name)\n  # if the data is somewhere else (not within the project environment), you need to use the absolute path\n    # i.e.-- paisley_data &lt;- read_excel(\"~/FRAN/Teaching/QM Research School/Datasets/paisley_data.xls\")\n    # going back in the folder structure: paisley_data &lt;- read_excel(\"../Datasets/paisley_data.xls\")\n  \n  # notice the \"&lt;-\" \n    # it creates a (temporary) object that is now in the \"environment\" we are working with\n      # We could properly save it (as a separate file) if we wished \n    # you can load as many \"objects\" as you wish (and they can be treated separately)\n\n# Inspecting the data\ndata       # A peek at the data\n    # Notice that some values are missing (NA; not available; the source did not provide info)\n    # It also indicates whether categorical (character) or numerical (double...)\n    # Types of variables:\n        # numeric: dbl (double; there are other types: integer...)\n        # string: chr (character)\n        # factor, ...\n\nview(data) # The whole dataset\nhead(data) # the first observations\ntail(data) # the last observations\n\ntbl_vars(data) # list the variables (fields) in the dataset\n\n## We will be using the pipe (|&gt; or |&gt;) though\n\ndata |&gt;  \n  print(15)\n\n## NO NEED TO LEARN THE COMMANDS BY HEART\n  # copy and paste from the templates\n  # you will get better with practice\n  # google is always your friend (r table frequency table)\n  # we now focus on make the commands work but we will later focus on interpreting the results\n  # (you will be able to work on the code on your own)\n  # in any case, we will only be scraping the surface (a brave new world out there)\n\n\n#################### DESCRIPTIVE STATISTICS (1 variable) ###################\n\n############## Categorical (qualitative) variables ############\n# sex, literacy, employed ...\n# each of these variables can exhibit certain values (categories)\n  # e.g. sex: male / female\n\n#### Frequency table (tabulate)\n  # number of observations falling in each category \n\n  # number of males and females\n\ndata |&gt;  # returns a table\n  count(sex)\n  # the pipe (also |&gt;) meaning \"and then\"\n  # it takes the output of a line of code and uses it as input to the next line\n\ndata |&gt;  # returns a table\n  count(employed)\n  # notice that is also reports the number of missing values\n\ndata |&gt;  # returns a table\n  count(occup) |&gt;\n  print(n = 20) # display 20 rows\n\ndata |&gt;  # returns a table\n  count(occup) |&gt;\n  print(n = Inf) # display all rows\n\ndata |&gt;  \n  count(occup, sort = TRUE) # present the data in order\n                            # important to quickly identify the most important categories\n\n## Absolute and relative frequencies\ndata |&gt; \n  count(occup, sort = TRUE) |&gt;              \n  mutate(proportion = n/sum(n))  # adding relative frequency \n\ndata |&gt; \n  count(occup, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) |&gt; # fractions\n  mutate(perc = 100*n/sum(n)) # percentages \n\n# or\ndata |&gt; \n  group_by(sex) |&gt;       # dimension we focus on\n  summarize(n = n()) |&gt;  # reporting number of observations in each group\n  mutate(rel_freq = n/sum(n)) # creating a new variable with the relative frequency \n\n  # notice that the code may look tricky but is easily replicable\n  # with literacy\ndata |&gt; \n  group_by(literacy) |&gt;\n  summarize(n = n()) |&gt;\n  mutate(freq = n/sum(n))\n\n# Adding another row with the totals:\nlibrary(janitor)\n\ndata |&gt;  \n  count(countryb, sort = TRUE) |&gt;              \n  mutate(perc = 100*n/sum(n)) |&gt;\n  adorn_totals(\"row\") |&gt;\n  as_tibble()\n\n\n\n### Focusing on subsamples (smaller groups) \n  # filter()\ndata |&gt; \n  filter(sex == \"male\") |&gt;   \n  count(literacy, sort = TRUE) |&gt;\n  mutate(prop = n/sum(n))\n  \n  # \"filter\" restricts the analysis to those observations fulfilling that condition\n  # filter() allows for complex selections using different operators:\n    # &gt;, &lt;, ==, != (distinct)\n    # Notice also that you can create more complex conditions using:\n      # and: &\n      # or: |\n      # except: - \n\ndata |&gt; \n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50) |&gt;   \n  count(literacy, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) \n\ndata |&gt; \n  filter(countryb==\"ireland\" | countryb==\"scotland\") |&gt;\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50) |&gt;   \n  count(literacy, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) |&gt;\n  mutate(prop = round(prop, 2)) # reporting only up two decimals\n\n# This allows paying attention to what it is both common and rare\ndata |&gt; \n  filter(countryb==\"ireland\" | countryb==\"scotland\") |&gt;\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50) |&gt;   \n  filter(literacy==\"superior education\") |&gt;\n  select(year, forename, surname, age, born, countryb, reside, occup)\n\n\n### Create an output that you can use later\n  # the output is not saved unless it is assigned to an object\n  # which can then be exported (saved) into different formats later\n\ntable1 &lt;- data |&gt; \n  filter(sex == \"male\") |&gt;   \n  group_by(literacy) |&gt;      \n  summarize(n = n()) |&gt;\n  mutate(freq = n/sum(n)) \n\n# Export the output (object) into excel \ninstall.packages(\"writexl\")\nlibrary(writexl)\n\nwrite_xlsx(occ, \"output/table1.xlsx\")\n  # notice that we are saving this \"object\" as an excel file in the folder \"output\"\n\n\n\n###### Plotting frequencies (graph bar)\n  # ggplot (part of the tidyverse)\n  # use \"+\" to add features to the graph\n\n# sex\nggplot(data = data) +       \n  geom_bar(aes(x = sex))\n    # \"aes\" goes for aesthetics\n\n  # or\ndata |&gt; \n  ggplot() + \n    geom_bar(aes(x = sex))\n\n# literacy\ndata |&gt; \n  ggplot() + \n    geom_bar(aes(x = literacy))\n\n# horizontal to facilitate reading the categories\ndata |&gt; \n  ggplot() + \n    geom_bar(aes(x = literacy)) +\n    coord_flip()\n\n# narrowing down the analysis  \ndata |&gt; \n  filter(countryb == \"scotland\" | countryb == \"ireland\") |&gt;\n  filter(sex == \"male\" & age&gt;18) |&gt;\n  ggplot() + \n    geom_bar(mapping = aes(x = literacy)) +\n    coord_flip()\n\n\n# proportions (instead of counts)\ndata |&gt; \n  count(literacy, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) |&gt;\n  ggplot(aes(x = literacy, y = prop)) + \n    geom_col() + # instead of geom_bar()\n    coord_flip()\n  \n    # geom_col() is a more general way of depicting graph bars\n    # we explicitly indicate what we want to depict (prop in this case)\n\n# editing the graph\ng1 &lt;- data |&gt; \n  count(literacy, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) |&gt;\n  ggplot(mapping = aes(x = literacy, y = prop)) + \n    geom_col() + \n    coord_flip() + \n    ylab(\"Proportion\") + \n    xlab(\"Literacy\")\n  # or all together: \n  # labs(title =\"\", subtitle = \"\", x = \"\", etc etc)\n  # many options for editing graphs... \n\ng1  \n# save the graph\ng1 |&gt;\n  ggsave(\"output/lit.png\", dpi = 320)\n\n?ggsave\n?ggplot\n\n\n### Re-categorising qualitative information \n# typos, aggregating categories, etc.\n\ndata |&gt;  \n  count(occup) |&gt;              \n  mutate(prop = n/sum(n)) \n\n# recoding\ndata &lt;- data |&gt;\n  mutate(occup_adj = recode(occup,\n                            \"blacksmith\" = \"black smith\", \n                            \"block print\" = \"block printer\"))\n\nView(data)\n\ndata |&gt; \n  filter(occup_adj==\"black smith\" | occup_adj==\"block printer\") |&gt;\n  select(occup, occup_adj)\n\ndata |&gt;  \n  count(literacy)\n\n  # checking what we have done\n\n# another example with literacy: iliterate, read, write\ndata &lt;- data |&gt;\n  mutate(lit_adj = recode(literacy, \n                          \"superior education\" = \"write\", \n                          \"read well\" = \"read\", \n                          \"read tolerably\" = \"read\", \n                          \"read a little\" = \"read\",\n                          \"read & write well\" = \"write\", \n                          \"read & write tolerably\" = \"write\", \n                          \"read & write a little\" = \"read\", \n                          \"cannot write\" = \"read\")\n  )\n    # no need to recode \"illiterate\" because it is the same category\n\ndata |&gt; \n  subset(select=c(forename, surname, literacy, lit_adj))\n\ndata |&gt;\n  count(lit_adj)\n\n# removing leading and trailing spaces\ndata &lt;- data |&gt; \n  mutate(occup_adj = str_trim(occup_adj))\n\n# ranking \"qualitative\" information (ordinal variables)\ndata |&gt;  \n  count(literacy) |&gt;              \n  mutate(prop = n/sum(n)) \n  # listed in alphabetical order (not always informative enough)\n\n\n# factor variables: ranking categories\ndata &lt;- data |&gt; \n  mutate(lit_rank = factor(literacy, \n                           levels = c(\"illiterate\", \n                                      \"read a little\", \n                                      \"read tolerably\", \n                                      \"read well\", \n                                      \"cannot write\", \n                                      \"read & write a little\", \n                                      \"read & write tolerably\", \n                                      \"read & write well\", \n                                      \"superior education\"), \n                           ordered = TRUE))\n\ndata |&gt;\n  count(lit_rank)\n  \n  # or visually:\ndata |&gt; \n  filter(!is.na(lit_rank)) |&gt; # exclude missing values\n  ggplot() +\n    geom_bar(aes(x = lit_rank)) +\n    coord_flip()  \n\n# re-categorising more complicate patterns\n\ndata |&gt;\n  count(marks)\n\n# install.packages(\"stringr\")\nlibrary(stringr)\n\ndata &lt;- data |&gt;\n  mutate(marks_adj = case_when(\n    str_detect(marks, \"scar\") ~ \"scar\",\n    str_detect(marks, \"cut\") ~ \"cut\", \n    str_detect(marks, \"burn\") ~ \"burn\",  \n    str_detect(marks, \"blind\") ~ \"blind\",      \n    str_detect(marks, \"no mark\") ~ \"none\",\n    str_detect(marks, \"nomark\") ~ \"none\",    \n    str_detect(marks, \"mark\") ~ \"mark\",\n    is.na(marks) ~ NA,\n    TRUE ~ \"other\"))\n\n# checking what you have done\ndata |&gt;\n  select(marks, marks_adj)\n\ndata |&gt; \n  count(marks_adj)\n\ndata |&gt; \n  filter(marks_adj==\"cut\") |&gt; \n  count(marks) |&gt; \n  print(n = Inf)\n\n# the above matches the exact sequence of characters \n  # defined within the quotation marks.\n\n# Regular expressions - trick\n  # \"^A\" those starting with A\n  # \"e$\" those ending with e\n  # “cut|scar” would match both possibilities\n  # !str_detect(mark,\"cut\") -- all except those containing “cut”. T\n  # more when explaining \"textual data\"\n\n\n##### Save the (modified) data ####\n# The data has somewhat change: correct typos, add new variables...\n# We can save the new dataset as a R file, so we can come back to that later\n\nwrite_rds(data, \"paisley_v2.rds\")\n\n\n\ndata2 &lt;- read_rds(\"paisley_v2.rds\")"
  },
  {
    "objectID": "r-scripts/uncert.html",
    "href": "r-scripts/uncert.html",
    "title": "3. Uncertainty (chance)",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# Install packages (if needed)\n# install.packages(\"tidyverse\")\n# install.packages(\"xlsx\")\n# install.packages(\"writexl\")\n\n\n# Load the packages you need\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(modelr)\n\ndata &lt;- read_rds(\"data/paisley_v2.rds\")\ndata &lt;- data %&gt;%\n  mutate(weight_kg = 0.453592*weight)\n\n\n#################### Dealing with uncertainty ###################\n\n# What we observe is influenced by chance\n# What would have happened if we would have observed a different set of records (sample)\n\n# We draw conclusion about the characteristics of the population\n  # based on a sample of observations\n  # this sample statistic however will deviate from the true value\n  # --&gt; sampling error, which depends on:\n    # the variability within the population (st. dev.)\n    # the number of observations in the sample\n  # --&gt; we use the standard error to compute:\n    # confidence intervals\n      # it will cover the true value with certain probability\n    # testing hypothesis:\n      # assume a hypothesis to be true\n      # assess the probability (p-value) of what is observed\n        # based on the previous assumption\n          # the probability that the observed outcome would\n            # have happened by chance\n\n\n\n### Standard error\ndata %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & countryb!=\"\") %&gt;%\n  group_by(countryb) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    sd = sd(height, na.rm=TRUE),\n    se = sd/sqrt(obs)) %&gt;%\n  mutate_if(is.numeric, round, 2)\n  # larger sample size, lower standard errors\n\n### Confidence intervals\n\nci &lt;- data %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  group_by(countryb) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    st.dev = sd(height, na.rm=TRUE),\n    st.error = sd/sqrt(obs), \n    lb_ci = t.test(height, conf.level = 0.95)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(height, conf.level = 0.95)$conf.int[2]) %&gt;%  # CI upper bound\n  mutate_if(is.numeric, round, 1) # add 1 decimal place\nci\n\n# Visually\nci %&gt;%  \n  ggplot(aes(x = countryb, y = mean)) +\n    geom_point() +\n    geom_errorbar(aes(x=countryb, ymin=lb_ci, ymax= ub_ci), width = 0.1)\n\n  # using sex instead\nci_sex &lt;- data %&gt;%\n  filter(age&gt;=20 & countryb==\"england\") %&gt;%\n  group_by(sex) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    st.dev = sd(height, na.rm=TRUE),\n    st.error = sd/sqrt(obs), \n    lb_ci = t.test(height, conf.level = 0.95)$conf.int[1],   # CI lower bound\n    ub_ci = t.test(height, conf.level = 0.95)$conf.int[2])   # CI upper bound\n\nci_sex %&gt;% ggplot(aes(x = sex, y = mean)) +\n  geom_point() +\n  geom_errorbar(aes(x=sex, ymin=lb_ci, ymax= ub_ci), width = 0.3)\n\n\n\n# Confidence level\n\nci99 &lt;- data %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  group_by(countryb) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    lb_ci = t.test(height, conf.level = 0.99)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(height, conf.level = 0.99)$conf.int[2]) %&gt;%  # CI upper bound\n  mutate(conf_level = \"Conf. level = 99\")\nci99\nci90 &lt;- data %&gt;%\n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  group_by(countryb) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE),\n    lb_ci = t.test(height, conf.level = 0.90)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(height, conf.level = 0.90)$conf.int[2]) %&gt;%  # CI upper bound\n  mutate(conf_level = \"Conf. level = 90\")\nci90\nci90_99 &lt;- bind_rows(ci99, ci90)\nci90_99\nci90_99 %&gt;%  \n  ggplot(aes(x = countryb, y = mean)) +\n  geom_point() +\n  geom_errorbar(aes(x=countryb, ymin=lb_ci, ymax= ub_ci), width = 0.1) +\n  facet_wrap(~ conf_level, nrow = 1)\n\n\n# Apply the same idea to changes over time\ndata %&gt;%\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50 & year&gt;1846) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    mean = mean(weight_kg, na.rm = TRUE),\n    lb_ci = t.test(weight_kg, conf.level = 0.95)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(weight_kg, conf.level = 0.95)$conf.int[2]) %&gt;%  # CI upper bound\n  ggplot(aes(x = year, y = mean)) +\n    geom_point() +\n    geom_errorbar(aes(x = year, ymin = lb_ci, ymax = ub_ci), width = 0.1)\n\n# this solution is slightly different\ndata %&gt;%\n  filter(sex==\"male\" & age&gt;=18 & age&lt;=50 & year&gt;1846) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    mean = mean(weight_kg, na.rm = TRUE),\n    lb_ci = t.test(weight_kg, conf.level = 0.95)$conf.int[1],      # CI lower bound\n    ub_ci = t.test(weight_kg, conf.level = 0.95)$conf.int[2]) %&gt;%  # CI upper bound\n  ggplot(aes(x = year, y = mean)) +\n    geom_smooth(se = TRUE) +\n    geom_point() +\n    geom_errorbar(aes(x = year, ymin = lb_ci, ymax = ub_ci), width = 0.1)\n\n\n### Confidence intervals in correlation and regression analysis\n  # the results also come from a particular sample (what if...)\n\n# Correlation\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  group_by(sex) %&gt;%\n  summarise(cor = cor(age, weight_kg, method = \"pearson\", use = \"complete.obs\"),\n            cor_ci_lb = cor.test(age, weight_kg)$conf.int[1],\n            cor_ci_ub = cor.test(age, weight_kg)$conf.int[2]) \n\n# Regression\nlibrary(modelr)\nlibrary(moderndive)\n\ndata %&gt;% \n  mutate(weight_kg = 0.453592*weight) %&gt;%\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate, std_error, lower_ci, upper_ci) %&gt;%\n  mutate_if(is.numeric, round, 2)\n\n# visually \ndata %&gt;% \n  mutate(weight_kg = 0.453592*weight) %&gt;%\n  filter(age&gt;=20) %&gt;%\n  ggplot(aes(y = weight_kg, x = age)) +\n    geom_point() +\n    geom_smooth(method = \"lm\") # se = TRUE\n\n\n### Hypothesis testing, t-tests, and p-values\n  # asking concrete questions and let the statistics reject the hypotheses or not\n    # null hypothesis (H0): reflects a conservative attitude towards the potential findings \n      # by specifying the negative form of a proposition. \n        # there is not difference between two means \n        # that there is no relationship between two variables. \n    # research (alternative) hypothesis (H1) specifies the opposite\n\n  # the p-value assess the probability that the observed outcome would be present \n    # if the null hypothesis were true \n    # (the probability that the observed outcome would have happened by chance). \n      # if the probability of having observed that outcome is very low, \n        # we should therefore consider rejecting the null hypothesis, \n          # that there is no difference between the means or \n          # that there is no correlation between two variables). \n\n  # as with confidence intervals, we reject hypotheses with a certain probability (**confidence level**). \n    # in order to decide whether we reject the null or not, \n      # the *p-value* is compared against α, the **significance level**, \n        # which basically mirrors the confidence level (α = 100 minus the confidence level):\n      # -- if p-value is large (&gt;α), the data supports the null hypothesis\n      # -- if p-value is small (&lt;α), the evidence is against the null hypothesis\n\n# Example\ndata %&gt;%\n  filter(countryb==\"scotland\" & sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  summarise(\n    obs = sum(!is.na(height)), \n    mean = mean(height, na.rm = TRUE)) # average height\n\n# One-sample test\n  # Scottish males are 171 cms on average (from other source)\n  # are our prisoners similar?\n\ndata %&gt;%\n  filter(countryb==\"scotland\" & sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%  \n  t.test(height ~ 1, mu = 171, data = .,\n         conf.level = 0.95)\n\n# Two-sample test: comparing groups\ndata %&gt;%\n  filter(countryb==\"scotland\" | countryb==\"ireland\") %&gt;%  \n  filter(sex==\"male\" & age&gt;=20 & age&lt;=50) %&gt;%\n  t.test(height ~ countryb, data = .,\n         conf.level = 0.95)\n\n\n\n## Hypothesis testing in regression analysis\n\nreg_m &lt;- data %&gt;% \n  filter(age&gt;=20 & sex==\"male\") %&gt;%\n  lm(weight_kg ~ age, data = .) \n\nreg_f &lt;- data %&gt;% \n  filter(age&gt;=20 & sex==\"female\") %&gt;%\n  lm(weight_kg ~ age, data = .) \n\nmodelsummary(\n  list(\n    \"Males\" = reg_m,\n    \"Females\" = reg_f),\n  statistic = c(\"s.e. = {std.error}\", \"p-value = {p.value}\"),\n  stars = c(\"*\" = .1, \"**\" = .05, \"***\" = 0.01),\n  gof_map = c(\"nobs\", \"r.squared\"),\n  fmt = 3,\n  coef_omit = \"Intercept\",\n  note = \"For simplicity, the intercept is not reported\")"
  },
  {
    "objectID": "r-scripts/corr_reg.html",
    "href": "r-scripts/corr_reg.html",
    "title": "2. Correlation analysis",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n# Start afresh\nrm(list=ls()) # Clear de \"Global Environment\"\n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# Install packages (if needed)\n# install.packages(\"tidyverse\")\n# install.packages(\"xlsx\")\n# install.packages(\"writexl\")\n\n\n# Load the packages you need\nlibrary(tidyverse) \nlibrary(readxl)\nlibrary(writexl)\n\n\n\n#################### CORRELATION ANALYSIS ###################\n\n# Introductory graph\nx &lt;- (1:30)     # range of x\nn &lt;- length(x)  # number of obs.\n\na = 3         # constant\nb = 0.3         # coefficient\nsd = 0.5        # standard deviation of the error term\nset.seed(1)\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data0 &lt;- data.frame(x, y)\nsim_data0 &lt;- as_tibble(sim_data0)\nsim_data0 &lt;- sim_data0 %&gt;%\n  mutate(type = \"positive\")\n\na = 7.5         # constant\nb = 0         # coefficient\nsd = 3        # standard deviation of the error term\nset.seed(222)\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data1 &lt;- data.frame(x, y)\nsim_data1 &lt;- as_tibble(sim_data1)\nsim_data1 &lt;- sim_data1 %&gt;%\n  mutate(type = \"no correlation\")\n\na = 10         \nb = -0.2         \nsd = 1.3\nset.seed(123)\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data2 &lt;- data.frame(x, y)\nsim_data2 &lt;- sim_data2 %&gt;%\n  mutate(type = \"negative\")\nsim_data2 &lt;- as_tibble(sim_data2)\n\nsim_data &lt;- bind_rows(sim_data0, sim_data1, sim_data2) %&gt;% \n  mutate(type = factor(type, \n                       levels = c(\"positive\", \"negative\", \"no correlation\"), \n                       ordered = TRUE))\n\nsim_data %&gt;%  \n  ggplot(mapping = aes(x = x, y = y)) +\n  geom_point() +\n  facet_wrap(~ type, nrow = 1)\n\n\n#### Numerical variables ####\n\ndata &lt;- read_rds(\"data/paisley_v2.rds\")\n\nview(data)\n\n# Describing the data first: age, weight\ndata %&gt;% \n  summarize(count_age = sum(!is.na(age)),   \n            mean_age = mean(age, na.rm = TRUE),\n            count_weight = sum(!is.na(weight)),   \n            mean_weight = mean(weight, na.rm = TRUE))\n\n# Let's express weight in kgs (instead of pounds)\ndata &lt;- data %&gt;%\n  mutate(weight_kg = 0.453592*weight)\n\n\n\n### Visually\n  # Scatterplot: Plotting the relationship between two variables\ndata %&gt;%  \n  ggplot() +\n  geom_point(aes(x = age, y = weight_kg)) \n\n  # focusing only on those younger than 20\ndata %&gt;%  \n  filter(age &lt;= 19) %&gt;%\n  ggplot() + \n    geom_point(mapping = aes(x = age, y = weight_kg)) + \n  scale_x_continuous(limits = c(9,19), breaks = seq(9,19, by = 1)) +\n  scale_y_continuous(limits = c(20,80), breaks = seq(20,80, by = 10))\n      # editing how the xaxis looks like\n\n  # focusing now on those aged 20 and older\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot() + \n  geom_point(mapping = aes(x = age, y = weight_kg)) \n\n\n## Pearson correlation coefficient\n  # it measures the direction and the strenght of the association between two variables\n  # it ranges between -1 to 1 being:\n    #  1 - perfect positive correlation\n    #     (strong - moderate - weak)\n    #  0 - no correlation\n    #     (strong - moderate - weak)\n    # -1 - perfect negative correlation\n\n  # it can be computed for numerical, ordinal and qualitative variables \n    # but employing different methods\n\n# compute it using age & weight (both numerical)\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  summarize(cor = cor(age, weight_kg, method = \"pearson\", use = \"complete.obs\")) \n    # -0.185\n    # \"use\" indicates how to handle missing values\n    # other options: \"everything\", \"all.obs\", \"complete.obs\", \"na.or.complete\", or \"pairwise.complete.obs\"\n\ndata %&gt;%  \n  filter(age &gt;= 20 & age &lt; 50) %&gt;%\n  group_by(sex) %&gt;%\n  summarize(cor=cor(age, weight_kg, method = \"pearson\", use = \"complete.obs\")) \n    # the negative link between age and weight is much stronger in women\n\n# visually:\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot() + \n    geom_point(aes(x = age, y = weight_kg)) +\n    facet_wrap(~ sex, nrow = 2)\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot() + \n  geom_point(aes(x = age, y = weight_kg, col = sex))\n\n\n## Illustrative graph - regression\nx &lt;- (1:30)     # range of x\nn &lt;- length(x)  # number of obs.\n\na = 0           # constant\nb = 6.2         # coefficient\nc = -0.2\nsd = 0.5        # standard deviation of the error term\nset.seed(2222)\ny = a + b*x + +c*x^2 + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data3 &lt;- data.frame(x, y)\nsim_data3 &lt;- as_tibble(sim_data3)\n\nsim_data3 %&gt;%  \n  summarize(cor = cor(x, y, method = \"pearson\", use = \"complete.obs\")) %&gt;%\n  mutate_if(is.numeric, round, 3)\n\nsim_data3 %&gt;%  \n  ggplot(mapping = aes(x = x, y = y)) +\n  geom_point() +  \n  annotate(x=25, label=\"r = -0.013\", y=41, colour=\"red\", geom = \"label\", size = 3)\n\n\n#### Regression analysis\n\n# Correlation analysis assesses the direction and strength of the relationship \n  # within a scale between 0-1 (whether two variables move together)\n\n# Regression analysis goes beyond and allows:\n  # (1) assessing the actual impact of X on Y: coefficient b\n  # (2) computing how much of the variation in Y is explained by X (or Xs): R-squared\n  # (3) allows controlling directly for the effect of other variables\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n# Regression analysis basically finds the best line to fit the data (OLS: Ordinary Least Squares)\n  # (the line minimising the sum of the deviations between the observed and predicted values)\n  #   (the deviations are squared in order to compare negative and positive deviations)\n  # \"lm\" refers to \"linear model\": Y = a + b*X   \n  # y is the variable we want to \"explain\" (weight; dependent variable)\n  # x is the explanatory variable (age)\n  # se refers to standard errors (the confidence intervals) - let's not report then for now\n\n# Estimate the regression line (intercept/slope)  \n  # R (or any other statistical software) does the job for you\n\nlibrary(modelr)\n\nlm(weight_kg ~ age, data = filter(data, age&gt;=20))\n  # where age is the explanatory variable (x) and weight_kg the dependent variable (y)\n\n# or with pipe\ndata %&gt;%\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .)\n\n  # interpret the results:\n    # y = a + b*x\n    # weight = 66.8 - 0.13*age\n    # intercept: if x = 0 -&gt; weight = 66.8\n    # slope: one-unit increase in X (age: 1 year) reduces Y (weight) by 0.13 units (kgs.)\n    # (always think about of units of measurement both in X and Y)\n\n\n# Predicted values: estimating what Y will be depending on Y\n\nreg &lt;- data %&gt;% \n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .) \n\ndata &lt;- data %&gt;%\n  add_predictions(reg, var = \"pred\")\nview(data)\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) + # deactivating this line shows only the predicted values\n    geom_point(aes(x = age, y = pred), colour = \"red\", size = 1) \n\n# Residuals: difference between the observed and predicted values\n\nlabel &lt;- expression(e[i]) # illustrative graph\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg)) +\n    geom_segment(aes(x = 69, xend = 69, y = 73, yend = 58), colour = \"red\", size = 0.5) +\n    annotate(x=71, y=66, colour=\"red\", geom = \"label\", size = 3, label=label) +\n    geom_segment(aes(x = 66, xend = 66, y = 44.5, yend = 58.5), colour = \"red\", size = 0.5) +\n    annotate(x=68, y=51.5, colour=\"red\", geom = \"label\", size = 3, label=label) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n# Add the predicted values and the residuals to the dataframe\ndata &lt;- data %&gt;%\n  add_predictions(reg, var = \"pred\") %&gt;%\n  add_residuals(reg, var = \"resid\") \n\ndata %&gt;% \n  filter(age&gt;=20 & !is.na(weight_kg)) %&gt;%\n  select(age, weight_kg, pred, resid) \n\n# Ordinary Least Squares (OLS): finds the line that best fits the data\n  # estimates the line that minimises the sum of the deviations between the observed values (the dots) and the predicted values \n    # (the deviations are squared in order to avoid negative and positive deviations cancelling each other out)\n\ndata %&gt;%  # Illustrative graph only\n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg)) +\n    geom_point() +\n    geom_abline(intercept = 62, slope = -.04, colour = \"red\") +\n    geom_abline(intercept = 65, slope = -.10, colour = \"red\") +\n    geom_abline(intercept = 63, slope = -.03, colour = \"red\") +\n    geom_abline(intercept = 65, slope = -.04, colour = \"red\") +\n    geom_abline(intercept = 68, slope = -.17, colour = \"red\") +\n    geom_abline(intercept = 68, slope = -.14, colour = \"red\") +\n    geom_abline(intercept = 68, slope = -.10, colour = \"red\") +\n    geom_smooth(method = \"lm\", se = FALSE, colour = \"blue\", linewidth = 1)  \n\n\n## R-squared\n  # Fraction of the variation on Y that is explained by the model \n  # It ranges between 0 and 1\n    # Low R2 does not neccessarily mean that the model is useless\n    # It has to be interpreted based on expectations\n    # The coefficient \"b\" can still provide useful info about the effect of X on Y\n\n# Illustrative graph\nx &lt;- (1:30)     # range of x\nn &lt;- length(x)  # number of obs.\n\na = 0.2         # constant\nb = 0.3         # coefficient\nsd = 1        # standard deviation of the error term\nset.seed(11111)\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data0 &lt;- data.frame(x, y)\nsim_data0 &lt;- as_tibble(sim_data0)\nsim_data0 &lt;- sim_data0 %&gt;%\n  mutate(type = \"high\")\n\na = 0.2         # constant\nb = 0.3         # coefficient\nsd = 2.5        # standard deviation of the error term\ny = a + b*x + sd*rnorm(n)   # the error term is normally distributed (mean = =; sd)\nsim_data1 &lt;- data.frame(x, y)\nsim_data1 &lt;- as_tibble(sim_data1)\nsim_data1 &lt;- sim_data1 %&gt;%\n  mutate(type = \"low\")\n\nsim_data &lt;- bind_rows(sim_data0, sim_data1) # combine both # reorder the levels: pos, neg, no\n\nreg0 &lt;- sim_data %&gt;% \n  filter(type==\"high\") %&gt;%\n  lm(y ~ x, data = .)\n\nreg1 &lt;- sim_data %&gt;% \n  filter(type==\"low\") %&gt;%\n  lm(y ~ x, data = .)\n\nsummary(reg0)$r.squared \nsummary(reg1)$r.squared \n\nnew_labels &lt;- c(\"high\" = \"R-squared = 0.85\", \"low\" = \"R-squared = 0.44\")\nsim_data %&gt;%  \n  ggplot(mapping = aes(x = x, y = y)) +\n  geom_point() +\n  facet_wrap(~ type, nrow = 1, labeller = labeller(type = new_labels))\nsim_data %&gt;%  \n  ggplot(mapping = aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ type, nrow = 1, labeller = labeller(type = new_labels)) # report the R2\n\n\n# Report R-squared\nreg &lt;- data %&gt;% \n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .) # it does not automatically shows up\n\nsummary(reg)$r.squared # 0.03 - the model (age) explains 3 per cent of the variation in adult weight\n\n# install.packages(\"moderndive\")\nlibrary(moderndive)\n\ndata %&gt;% \n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ age, data = .) %&gt;%\n  get_regression_summaries() %&gt;%\n  select(nobs, r_squared)\n\ndata %&gt;% \n  filter(age&lt;20) %&gt;%\n  lm(weight_kg ~ age, data = .) %&gt;%\n  get_regression_summaries() %&gt;%\n  select(nobs, r_squared) # 0.695\n\n# Regression tables: how age and adult weight are related for men and women separately?\n\n# install.packages(\"modelsummary\")\nlibrary(modelsummary)\n\nreg_males &lt;- data %&gt;%\n  filter(age&gt;=20 & sex==\"male\") %&gt;%\n  lm(weight_kg ~ age, data = .)\n\nreg_females &lt;- data %&gt;%\n  filter(age&gt;=20 & sex==\"female\") %&gt;%\n  lm(weight_kg ~ age, data = .)\n\nmodelsummary(\n  list(\n    \"Males\" = reg_males,\n    \"Females\" = reg_females),\n  statistic = NULL,\n  gof_map = c(\"nobs\", \"r.squared\"),\n  fmt = 2) \n\n    # obviously these results are contingent on the data we are using:\n      # smaller numer of female observations\n      # also, are men and women comparable in this setting (who ended up in prison)?\n        # controls? ...\n      # biological vs social? --&gt; single / married women? diets in prison?\n    # non-linearities?\n    # outliers\n\n\n# Visually:\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(mapping = aes(x = age, y = weight_kg, color = sex)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n### Qualitative variables \n  # categorical (or ordinal) variable can be used in regression analysis as dummy variables (0/1)\n    # 1 category acts as a \"reference category\"\n    # so the coefficient is interpreted \"against\" that category\n    # if you have n categories, you need (n-1) dummies\n    # i.e. sex has two categories: male/female, so you just need 1 dummy\n      # either being male or being female \n    # no need to create dummy variables to include categorical info (R does it for you)\n\ndata %&gt;% \n  filter(age&gt;=20) %&gt;%\n  group_by(sex) %&gt;%\n  summarize(\n    mean_height = mean(height, na.rm = TRUE))\n\ndata &lt;- data %&gt;% \n  mutate(male = ifelse(sex==\"male\", 1, 0)) # what to do with missing values, apparently still missing, check\ndata %&gt;%\n  count(male)\n\ndata %&gt;% \n  subset(select=c(forename, sex, male)) %&gt;% # check what you have done\n  print(n = 20) \n\ndata %&gt;%\n  filter(age &gt;= 20) %&gt;%\n  lm(height ~ male, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate) # men are 12.5 centimeters taller than women\n\ndata %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  ggplot(aes(x = male, y = height)) + # visually\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_hline(yintercept = 156.5729, color = \"red\", linetype = 2) +\n  geom_hline(yintercept = 169.0388, color = \"blue\", linetype = 2) +\n  scale_x_continuous(breaks = seq(0, 1, 1))\n\n# doing the same with country of birth\ndata %&gt;%\n  filter(age &gt;= 20 & sex==\"male\") %&gt;%\n  lm(height ~ countryb, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)\n  # no need to create the dummies yourself but you lose control of the reference category\n\n# creating the dummies yourself\ndata &lt;- data %&gt;%  \n  mutate(england = ifelse(countryb==\"england\", 1, 0)) %&gt;%\n  mutate(ireland = ifelse(countryb==\"ireland\", 1, 0)) %&gt;%\n  mutate(scotland = ifelse(countryb==\"scotland\", 1, 0)) %&gt;%\n  mutate(overseas = ifelse(countryb==\"overseas\", 1, 0))\n\ndata %&gt;% \n  subset(select=c(forename, countryb, england, ireland, scotland, overseas)) %&gt;%\n  print(n = 20) \n\ndata %&gt;%\n  filter(age &gt;= 20 & sex==\"male\") %&gt;%\n  lm(height ~ england + ireland + overseas, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)\n\n\n### Qualitative variables as dependent variables (Y)\ndata %&gt;%\n  count(lit_adj)\n\ndata %&gt;%\n  group_by(lit_adj) %&gt;%\n  summarize(n = n()) %&gt;%\n  mutate(freq = n/sum(n))\n\ndata &lt;- data %&gt;%\n  mutate(write = ifelse(lit_adj==\"write\", 1, 0)) %&gt;%\n  mutate(male = ifelse(sex==\"male\", 1, 0))\n\ndata %&gt;%\n  count(write)\n\ndata %&gt;% summarize(\n  mean_write = mean(write, na.rm = TRUE))\n\ndata %&gt;%\n  lm(write ~ male, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)\n  # being male (going from 0, female, to 1, male) is associated with an increase in literacy of 0.07 \n    # (or an increase of 7 percentile points if measured as percentages)\n\n\n\n## Comparing the relative importance of different variables\n\nreg_height &lt;- data %&gt;% \n  mutate(height = 30.48*feet+2.54*inches) %&gt;% # in case not previously created\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ height, data = .) \n\nreg_sex &lt;- data %&gt;% \n  mutate(male = if_else(sex==\"male\", 1, 0)) %&gt;% # in case not created before\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ male, data = .) \n\nmodelsummary(\n  list(\n    \"M1\" = reg_height,\n    \"M2\" = reg_sex),\n  statistic = NULL,\n  gof_map = c(\"nobs\", \"r.squared\"),\n  fmt = 3)\n\n\ndata %&gt;% \n  mutate(height = 30.48*feet+2.54*inches) %&gt;% \n  mutate(height_mts = height/100) %&gt;%\n  filter(age&gt;=20) %&gt;%\n  lm(weight_kg ~ height_mts, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)\n  # changing the units of measurement does not change the results (only their scale) \n\n\n## Time as explanatory variable\n\ndata %&gt;%\n  mutate(year_birth = year - age) %&gt;%\n  filter(age &gt;= 20) %&gt;%\n  lm(height ~ year_birth, data = .) %&gt;%\n  ggplot(aes(y = height, x = year_birth)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    annotate(x=1780, y=167, label=\"Y = 118.1 + 0.026*X\", colour=\"red\", geom = \"label\", size = 3)\n\n# individual years as explanatory variables\ndata %&gt;%\n  mutate(year_birth = year - age) %&gt;%\n  mutate(year_birth = as_factor(year_birth)) %&gt;%  \n  filter(age &gt;= 20) %&gt;%\n  lm(height ~ year_birth, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate) %&gt;% \n  print(n = Inf)\n\n# grouping years into age groups\ndata &lt;- data %&gt;%\n  mutate(year_birth = year - age) %&gt;%\n  mutate(year_birth_10 = cut(year_birth, breaks =  seq(1775, 1885, 10)))\ndata %&gt;%\n  count(year_birth_10)\n\ndata %&gt;%\n  filter(age&gt;=20 & year_birth&gt;1795) %&gt;%\n  lm(height ~ year_birth_10, data = .) %&gt;%\n  get_regression_table() %&gt;%\n  select(term, estimate)"
  },
  {
    "objectID": "r-scripts/mapping.html",
    "href": "r-scripts/mapping.html",
    "title": "1.5. Mapping (spatial information)",
    "section": "",
    "text": "The script used in this session is included below. You can also downloaded it from here.\n\n\n###### Mapping ######\n\n# clear de \"Global Environment\"\nrm(list=ls()) \n\n# set working directory\nsetwd(\"/Volumes/francijb/Documents/FRAN/Teaching/QM_2024/session\") \n\n# upload basic packages\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggplot2)\n\n# install.packages(\"sf\")\n# install.packages(\"tmap\")\n# install.packages(\"geodata\")\nlibrary(sf)\nlibrary(tmap)\nlibrary(geodata)\n\n\n##### Shapefiles: polygons, lines, points\n\n### Polygons\n\n# Import files\ndist_sh &lt;- read_sf(\"data/mapping/educ_1860/dist_1860.shp\") # Spanish districts\ndist_sh\n\n# Map it: t_shape() + tm_polygons() (or tm_borders, tm_fill...)\ndist_sh |&gt; \n  tm_shape() +\n  tm_polygons(fill = \"lightblue\", col = \"grey\", lwd = 0.5)\n\n# Map the info contained in particular fields (variables)\ndist_sh |&gt;\n  tm_shape() +\n    tm_polygons(fill = \"literacy_m\", lwd = 0.5)\n\n?tm_polygons\n\ndist_sh |&gt;\n  tm_shape() +\n    tm_polygons(fill = \"literacy_m\", lwd = 0.5,\n                fill.scale = tm_scale_intervals(\n                  style = \"fixed\",\n                  breaks = c(0, 15, 30, 45, 60, Inf),\n                  values = \"brewer.reds\"), # color palette\n                fill.legend = tm_legend(\n                  title = \"\") + # Remove the legend title\n    tm_scale_bar(position = c(\"right\", \"bottom\"))\n\n# Multiple maps\ndist_sh |&gt;\n  tm_shape() +\n  tm_polygons(fill = c(\"literacy_m\", \"literacy_f\"),\n              fill.scale = tm_scale_continuous(\n                ticks = c(0, 10, 20, 30, 40, 50, 60, 70),\n                values = \"brewer.reds\"),\n              fill.legend = tm_legend(\n                title = \"\"\n                , orientation = \"landscape\"),\n              fill.free = FALSE) +\n  tm_layout(panel.labels = c(\"Men\", \"Women\"),\n            panel.label.bg.color = \"white\",\n            panel.label.frame = FALSE)\n\n# exporting maps\nmap &lt;- dist_sh |&gt;\n  tm_shape() +\n  tm_polygons(fill = \"literacy_m\")\n\n\ntmap_save(map, \"output/map_lit_1860.png\", dpi = 600)\n  # save the map as a .png file (with resolution = 600 pixels)\n\n# Categorical (qualitative) variables\n\ndist_sh |&gt;\n  tm_shape() +\n    tm_polygons(fill = \"province\",\n                fill.scale = tm_scale_categorical(\n                  n.max = 48)) # when many categories (avoid recycling colors)\n\n### Point shapefiles\n\nmun_sh &lt;- read_sf(\"data/mapping/mun_1860_1930/mun_1860_1930.shp\")\nmun_sh\n\n# Map it: tm_shape() + tm_dots() (or tm_bubbles...)\nmun_sh |&gt;\n  filter(ccau!=5) |&gt;\n  tm_shape() +\n    tm_dots(fill = \"blue\", size = 0.01)\n\n# Adding contour for references\nspain &lt;- read_sf(\"data/mapping/ESP_adm0/ESP_adm0_pr_peninsula.shp\") # import Spanish boundaries\n  # import shapefile with the contour first\n\nmun_sh |&gt;\n  filter(ccau!=5) |&gt;\n  tm_shape() +\n    tm_dots(col = \"blue\", size = 0.01) + \n  tm_shape(spain) +\n    tm_borders(col = \"grey70\", lwd = 0.5)\n\n# Adjusting the size of the dots according to a particular field\n  # population in 1860 here (pop1860)\nmun_sh |&gt;\n  filter(ccau!=5) |&gt;\n  tm_shape() +\n  tm_dots(fill = \"blue\", fill_alpha = 0.5, \n          col = \"blue\", col_alpha = 0.5, \n          size = \"pop1860\",\n          size.scale = tm_scale_continous(\n            ticks = c(500, 1000, 5000, 10000, 20000, 50000, 100000, 200000))) + \n  tm_shape(spain) +\n    tm_borders(lwd = 0.05, col = \"grey70\")\n\n# Adding labels to the features: tm_text()\nmun_sh |&gt;\n  filter(ccau!=5) |&gt;\n  tm_shape() +\n  tm_dots(\n    fill = \"blue\", fill_alpha = 0.4,\n    col = \"blue\", col_alpha = 0.4,\n    size = \"pop1860\") +\n  tm_text(\"municipio\", just = \"left\", xmod = 0.5, size = 0.5) +\n  tm_shape(spain) +\n  tm_borders(lwd = 0.5, col = \"grey70\")\n\ncities &lt;- mun_sh |&gt;\n  filter(pop1860&gt;=50000)\n\nmun_sh |&gt;\n  filter(ccau!=5) |&gt;\n  tm_shape() +\n  tm_dots(\n    fill = \"blue\", fill_alpha = 0.4,\n    col = \"blue\", col_alpha = 0.4,\n    size = \"pop1860\") +\n  tm_shape(spain) +\n  tm_borders(lwd = 0.5, col = \"grey70\") +\n  tm_shape(cities) +\n  tm_text(\"municipio\", just = \"left\", xmod = 0.5, size = 0.5)\n\n  # you can also add labels to polygons if needed\n\n# Temporal variation\n\nmun_sh |&gt;\n  filter(ccau!=5) |&gt;\n  tm_shape() +\n  tm_dots(\n    fill = \"blue\", fill_alpha = 0.4,\n    col = \"blue\", col_alpha = 0.4,\n    size = c(\"pop1860\", \"pop1900\", \"pop1930\"),\n    size.legend = tm_legend(\n      title = \"\", \n      orientation = \"landscape\"),\n    size.free = FALSE) +\n  tm_facets(nrow = 1) +\n  tm_layout(panel.labels = c(\"1860\", \"1900\", \"1930\"),\n            panel.label.bg.color = \"white\") +\n  tm_shape(spain) +\n  tm_borders(lwd = 0.5, col = \"grey70\")\n\n\n# or using facets but we need to structure the data differently\nmun_sh_long &lt;- mun_sh |&gt;\n  pivot_longer(cols = starts_with(\"pop\"), \n               names_to = \"year\", \n               names_prefix = \"pop\", \n               values_to = \"pop\") |&gt;\n  filter(year==\"1860\" | year==\"1900\" | year==\"1930\") |&gt; # select only the years I am interested in\n  filter(ccau!=5) # excluding Canarias\nmun_sh_long\n\nmun_sh_long |&gt;\n  tm_shape() +\n  tm_dots(fill = \"blue\", fill_alpha = 0.5,\n          size = \"pop\",\n          size.legend = tm_legend(\n            title = \"\"\n            , orientation = \"landscape\")) +\n  tm_facets(by = \"year\", nrow = 1) +\n  tm_layout(legend.outside.position = \"bottom\") +\n  tm_shape(spain) + tm_borders()\n\n\n\n\n### A brief note on coordinate systems and projections\n\n# install.packages(\"spData\")\nlibrary(spData)\n\nm0 &lt;- tm_shape(world, projection = 4326) + tm_polygons() + tm_credits(\"WGS 84\", position = c(\"LEFT\", \"BOTTOM\")) + tm_layout(asp = 4)\nm1 &lt;- tm_shape(world, projection = 8857) + tm_polygons() + tm_credits(\"Equal Earth\", position = c(\"LEFT\", \"BOTTOM\")) + tm_layout(asp = 4)\nm2 &lt;- tm_shape(world, projection = \"+proj=moll\", ) + tm_polygons() + tm_credits(\"Mollweide\", position = c(\"LEFT\", \"BOTTOM\")) + tm_layout(asp = 4) \nm3 &lt;- tm_shape(world, projection = \"+proj=wintri\", ) + tm_polygons() + tm_credits(\"Winkel Tripel\", position = c(\"LEFT\", \"BOTTOM\")) + tm_layout(asp = 4)\n\ntmap_arrange(m0, m1, m2, m3, ncol = 2)\n\n# distortions either in shape, area, distance or direction\n\n# Spatial objects usually have the adequate CRS already defined\n# Combining spatial objects with different CRSs is problematic\n\n## Some CRSs (authority:code)\n# WGS 84 (short for World Geodetic System 1984 (EPSG:4326)\n# WGS 84 / World Mercator (EPSG:3395) -- used by Google Maps\n# WGS 84 / Pseudo-Mercator (EPSG:3857)\n# LAEA Europe (EPSG:3035) -- Lambert Azimuthal Equal Area\n# UTM projections are especially suited for working with small areas. \n  # The earth is divided into 60 tiles (North/South the Equator). \n  # You should choose the one that covers your area of study. \n  # For Spain: ETRS 1989 UTM Zone 30N (\"EPSG:25830\").\n\n# check for help when choosing CRSs: \n# https://jjimenezshaw.github.io/crs-explorer/\n\n## Retrieving the CRS: authority:code -- summary()\nspain                         # ETRS89 / UTM zone 30N\nspain |&gt; summary(\"geometry\") # epsg:25830\nspain |&gt; st_crs()\n  # provides all the information needed to properly identify the CRS.\n\n## Changing the CRS\nspain2 &lt;- st_set_crs(spain, \"EPSG:3035\") # set CRS (LAEA Europe)\nspain2 |&gt; summary(\"geometry\")\nspain2 &lt;- st_transform(spain, \"EPSG:3035\") # set CRS\n\n\nm0 &lt;- tm_shape(spain) + tm_polygons() + tm_credits(\"ETRS89 / UTM zone 30N\", position = c(\"RIGHT\", \"BOTTOM\"))\nm1 &lt;- tm_shape(spain2) + tm_polygons() + tm_credits(\"LAEA Europe\", position = c(\"RIGHT\", \"BOTTOM\"))\ntmap_arrange(m0, m1, ncol = 2)\n\n\n#### Mapping historical (or otherwise) data\n\n## Rely on existing GIS files\n# Search online for what you are looking for\n# The Historical GIS Research Network\n  # http://www.hgis.org.uk/resources.htm\n# Geospatial Historian\n  # https://geospatialhistorian.wordpress.com/finding-data/\n# Historical gazetteers\n  # World Historical Gazetteer: https://whgazetteer.org\n# Use contemporary files (and adapt them if necessary)\n  # Natural Earth: https://www.naturalearthdata.com/features/\n  # GADM: https://www.gadm.org (administrative boundaries)\n  # National agencies\n\n\n## (1) Import them using read_sf()\n  # regardless whether shapefiles are historical or contemporary\n  # use filter() if necessary to extract the features you are interested in\n\n## (2) merge them with the information you have gathered \n  # from other the archive or other sources\n\n## Illustration using Paisley\n\nlibrary(readxl) \npaisley &lt;- read_excel(\"data/paisley_data.xlsx\") # Paisley data\npaisley_born &lt;- paisley |&gt;\n  filter(countryb==\"scotland\") |&gt;\n  count(born, sort = TRUE)\npaisley_born\n\nlocations &lt;- read_sf(\"data/mapping/Localities2020centroids/Localities2020_Centroids.shp\") # import the spatial object (shapefile)\nlocations # shapefile with Scottish locations\n\nscotland &lt;- read_sf(\"data/mapping/scotland/scotland.shp\") # import the spatial object (shapefile)\n\nlocations |&gt; summary(\"geometry\") # epsg:27700\nscotland |&gt; summary(\"geometry\") # epsg:4326\nscotland &lt;- st_transform(scotland, \"EPSG:27700\") # set CRS\n# or\nscotland &lt;- st_transform(scotland, st_crs(locations)) \n  # using the crs in the object \"locations\"\n\ntm_shape(scotland, bbox = locations) + tm_borders() +\n  tm_shape(locations) + tm_dots(col = \"blue\")\n\n## Clean the Paisley locations\npaisley &lt;- paisley |&gt;\n  mutate(born = str_trim(born)) |&gt;       # removes leading/trailing spaces\n  mutate(born = str_to_lower(born)) |&gt;   # all to lower letters\n  mutate(born_adj = recode(born,          # homogenising categories\n                           \"campsey\" = \"campsie\",                     \n                           \"bridge of wier\" = \"bridge of weir\",\n                           \"n kilpatrick\" = \"new kilpatrick\"))\n\npaisley &lt;- paisley |&gt;\n  mutate(born_adj = str_replace(born_adj, \"shire\", \"\")) # removing \"shire\"\n\npaisley_born &lt;- paisley |&gt;\n  filter(countryb==\"scotland\") |&gt;\n  count(born_adj)\npaisley_born\n\n# Merge both objects: shapefile - paisley places \nlocations_ext &lt;- locations |&gt;\n  mutate(name = str_to_lower(name)) |&gt;   # converts to lower case\n  full_join(paisley_born, by = join_by(name == born_adj))\nlocations_ext |&gt; \n  select(code, name, n)\n\nlocations_ext |&gt;\n  filter(!is.na(n)) & is.na(code))\n\n# map the number of prisoners \n  # assuming we are satified with the matching\ntm_shape(scotland, bbox = locations_ext) + tm_borders() +\n  tm_shape(locations) + tm_dots(fill = \"grey\", size = 0.05) +\n  tm_shape(locations_ext) +\n  tm_bubbles(fill = \"red\",\n             size = \"n\",\n             size.scale = tm_scale_continuous(\n               ticks = c(1, 5, 10, 25, 50, 100, 200)),\n             size.legend = tm_legend(\n               title = \"Number of prisoners, by origin\"))\n  # most of our Scottish prisoners were born relatively near the prison\n\n## extracting features from larger spatial objects\nprov_shp &lt;- read_sf(\"data/mapping/ESP_adm2/ESP_adm2.shp\")\nprov_shp |&gt;\n  tm_shape() +\n  tm_borders()\n\nzgz_shp &lt;- prov_shp |&gt;\n  filter(NAME_1==\"Zaragoza\")\nzgz_shp |&gt;\n  tm_shape() +\n  tm_borders()\n\n## Adding XY coordinates: st_as_sf()\n\nlibrary(readxl) \nzgz_mun &lt;- read_excel(\"data/mapping/mun_zgz_1860.xlsx\")\nzgz_mun \n\nzgz_mun_shp &lt;- st_as_sf(zgz_mun, coords = c(\"lat\", \"lon\"), crs = 3042)\nzgz_mun_shp\n\nzgz_mun_shp |&gt;\n  tm_shape() + tm_dots() +\n  tm_shape(zgz_shp) + tm_borders()\n\n\n## Geocoding\n\npaisley_born |&gt; arrange(-n)\n\n# install.packages(\"tidygeocoder\")\nlibrary(tidygeocoder)\n\nplaces_geo &lt;- paisley_born |&gt;\n  geocode(born_adj, method = \"osm\", \n          full_results = TRUE)\nplaces_geo\nview(places_geo)\n\n  # the method refers to the geocoding service you are requesting\n    # `osm` refers to the *Open Street Map Nominatim API\n    # others: arcgis, census, google maps, etc.; \n    # see the package documentation:\n      # https://cran.r-project.org/web/packages/tidygeocoder/tidygeocoder.pdf\n      # the Google Maps Geocoding API requires an API key, so it might not be free\n\n  # some locations are not found\n  # others are found in other countries: US, Canada, Australia\n\n# improve the geocoding by adding more info (country)\n\npaisley_born &lt;- paisley_born |&gt;\n  mutate(born_adj = str_to_title(born_adj)) |&gt;            # capitalise the first letter\n  mutate(born_adj = paste(born_adj, \", Scotland\", sep = \"\"))  # add string\npaisley_born\n\nplaces_geo &lt;- paisley_born |&gt;\n  geocode(born_adj, method = \"osm\", full_results = TRUE)\nplaces_geo\n\nplaces_geo |&gt;\n  filter(is.na(lat))\n  # correct typos\n  # finding the coordinates (lat, long) manually and add them using mutate()\n\n# transform it into a spatial object (including CRSs)\nplaces_geo_sf &lt;- places_geo |&gt;\n  filter(!is.na(lat)) |&gt;\n  st_as_sf(coords = c(\"lat\", \"long\"), crs = 4326) # WGS 84 4326\n  \n# map it\nplaces_geo_sf |&gt;\n  tm_shape() +\n  tm_bubbles(fill = \"red\",\n             size = \"n\",\n             size.scale = tm_scale_continuous(\n               ticks = c(1, 5, 10, 25, 50, 100, 200))) +\n  tm_shape(scotland) + tm_borders()\n\n\n## Digitise your own maps -- ArcGIS / QGIS"
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Below are the instructions to prepare the session in advance:\n1. Install R and RStudio. R is a (free) statistical software and RStudio is an integrated interface that makes working with R easier. Bear in mind that you have to download and install both softwares: R and RStudio. The version you install depends on whether you are using Windows or Mac. Here is a link to access them.\n2. Create a dedicated folder to this session in your computer. Name it quants.\n3. Download the following materials. Store these files in separate folders (named data and scripts, respectively) within the folder quants:\n\nThe datasets we will be working with: here and here.\nThe R scripts containing the code to extract information from them (available in Blackboard).\n\n4. Get familiar with R and RStudio by opening RStudio and following the instructions in the Intro to R section.\n5. Read the following texts in advance:\n\nThe background information for the two case-studies we will be exploring: The Paisley Prisoners’ Dataset and The State of the Union Presidential Speeches.\nThe article Quantifying history (available in Blackboard). This text outlines the advantages and disadvantages of using quantitative and computational methods in history. We will be discussing it at the beginning of the first session.\n\n6. Do not forget to bring your laptop!\nDo not hesitate to contact me if you have any question!"
  },
  {
    "objectID": "paisley.html",
    "href": "paisley.html",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "",
    "text": "This case-study introduces basic tools to systematize and extract information from historical sources, regadless it is qualitative or numerical. Displaying frequency tables, plotting histograms and reporting summary statistics (i.e. the mean, the minimum and maximum values, the standard deviation, etc.) helps characterising how our data looks like. Making sense of the source using descriptive statistics can actually get you a long way in your understanding of the historical setting you are studying."
  },
  {
    "objectID": "paisley.html#setting-the-stage",
    "href": "paisley.html#setting-the-stage",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Setting the stage",
    "text": "Setting the stage\nAs explained in the Intro to R section, we need to include some preliminary commands in our script so we (1) get rid of other objects that could be in the R environment from previous sessions, (2) set the working directory, (3) load (and install if necessary) the packages we pla to use, and (4) import the dataset.\n\n# Clear de \"Global Environment\"\nrm(list=ls()) \n\n# Sets the working directory\nsetwd(\"~/Documents/quants\") \n\n# Install/load packages\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(readxl)\ndata &lt;- read_excel(\"data/paisley-data.xlsx\")"
  },
  {
    "objectID": "paisley.html#inspecting-the-data",
    "href": "paisley.html#inspecting-the-data",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Inspecting the data",
    "text": "Inspecting the data\nOnce the data is imported into the R environment as an object, we can start inspecting it as shown in the Intro to R section. As shown in the Global Environment, in the upper left corner, this this data frame (referred to as a tibble in the tidyverse terminology) contains 1,000 individuals (rows) and 21 fields (columns). Typing the name of the object (data) only provides information on the 10 first cases in order to save memory and space. Notice also that, below the variable name, R also indicates the type of variable: some are categorical (“chr” meaning character) and others are numerical (“dbl” meaning double).\n\ndata\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n# ℹ 990 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nIf you want to display more cases, you can use the function print(n = 15) and indicate the number of cases to be reported. Let’s pause for a moment to disect what the code below is doing. Notice first that it is crucial to indicate where the information you are asking for is coming from. Remember that we imported the Paisley data into the object name data. The pipe (|&gt;) here basically takes this object and uses it as input in the next line of code, which uses the function print() to request listing the first 15 cases from that object. Alternatively, you can have a look at the last 20 cases by typing tail(20).\n\ndata |&gt;  \n  print(n = 15)\n\n# A tibble: 1,000 × 21\n   casen    no month     year forename surname sex     age born  countryb reside\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; \n 1     1    17 january   1841 AGNES    M'INTY… fema…    24 pais… scotland colin…\n 2     2    45 january   1841 CATHERI… CARLIN… fema…    30 irvi… scotland paisl…\n 3     3    68 january   1841 JEAN     WRIGHT  fema…    17 pais… scotland paisl…\n 4     4    91 february  1841 MARGRET  M'HAFF… fema…    18 glas… scotland paisl…\n 5     5    93 february  1841 JANET    M'LEAN  fema…    25 cath… scotland strab…\n 6     6   263 april     1841 ELIZA    DUNCAN  fema…    34 belf… ireland  paisl…\n 7     7   280 april     1841 ANN      RYLEY   fema…    45 sligo ireland  glasg…\n 8     8   299 april     1841 MARGRET  M'LEOD  fema…    40 gree… scotland green…\n 9     9   300 april     1841 MARY     MILLAR… fema…    19 gree… scotland green…\n10    10   310 april     1841 JEAN     M'KINL… fema…    29 glas… scotland pollo…\n11    11   343 may       1841 AGNES    CURRIE… fema…    35 islay scotland paisl…\n12    12   382 june      1841 ELLIZA   MUNN    fema…    18 john… scotland johns…\n13    13   425 june      1841 SARAH    BLACK … fema…    36 glas… scotland kelvi…\n14    57     3 january   1841 THOMAS   ROBERT… male     19 pais… scotland high …\n15    58    19 january   1841 JOHN     MONTGO… male     24 some… england  barra…\n# ℹ 985 more rows\n# ℹ 10 more variables: feet &lt;dbl&gt;, inches &lt;dbl&gt;, weight &lt;dbl&gt;, occup &lt;chr&gt;,\n#   employed &lt;chr&gt;, literacy &lt;chr&gt;, marks &lt;chr&gt;, offence &lt;chr&gt;, sentence &lt;chr&gt;,\n#   source &lt;chr&gt;\n\n\nThere are other commands that help knowing more about how the data set looks like such as glimpse() or names(). Feel free to try them out yourself. You can also have a sense of the magnitude and complexity of the whole data set by typing view(data). The latter opens up a new tab where you can explore the full data set at ease. Notice also that some values are missing. R signals them as NA (not available), meaning that no information is recorded in those fields for those observations. We will discuss the importance of missing values in due time.\n\nview(data)\n\nWhat it is important to stress now is that, although the Paisley data set is not especially big, scrolling up-down and left-right makes it obvious that it is extremely difficult to extract any kind of pattern by just “looking” at all this information. Here is where statistics (and R) come to the rescue."
  },
  {
    "objectID": "paisley.html#categorical-qualitative-variables",
    "href": "paisley.html#categorical-qualitative-variables",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Categorical (qualitative) variables",
    "text": "Categorical (qualitative) variables\nWe will start extracting information from the data set by focusing on categorical (qualitative) variables, those defined with words instead of numbers, such as sex, country of birth, occupation, etc. Each of these variables can exhibit certain values (categories) and it is important to stress that the difference between these values is merely qualitative (one category is no more or better than any other).\nThe first step is to assess how the distribution of values (categories) looks like, that is, to quantify their relative importance. A frequency table reports the number of observations (usually referred to as n) falling into each category, an information that can easily be retrieved using count() and indicating which variable you want to get information on.\n\ndata |&gt;  \n  count(sex)\n\n# A tibble: 2 × 2\n  sex        n\n  &lt;chr&gt;  &lt;int&gt;\n1 female   284\n2 male     716\n\n\nAs you will have guessed, the code above takes the object data that contains the Paisley data set and implements the function count() on the field sex. The results shows that the data contains 284 female prisoners and 716 male prisoners (the categories are presented in alphabetical order: “female” and “male”).\nWe could do the same with any other variable. Notice that the function count() also reports the number of missing values (if any). Reporting a frequency table of the variable employed yields three different categories: employed, unemployed and NA. In this case, the data set did not record the emplyment status of 821 prisoners. Analysing variables with missing values presents especial challenges because their accuracy depends on the reasons behind its “missingness”.\n\ndata |&gt;  \n  count(employed)\n\n# A tibble: 3 × 2\n  employed       n\n  &lt;chr&gt;      &lt;int&gt;\n1 employed     101\n2 unemployed    78\n3 &lt;NA&gt;         821\n\n\nAs with other commands listing information, count() only reports the first 10 categories by default. Some variables, such as occupation (occup) has many categories, so you probably want to inspect all of them. You can just indicate explicitly how many categories you want to display by using the function print(): while indicating n = 15 displays information on the first 15 categories, typing n = Inf reports all rows. Notice also that this command is now 3 lines long and we are using the pipe (|&gt;) in each line to implement this sequence of instructions: the pipe takes the output from that line and uses it as an input in the following line.\n\ndata |&gt;  \n  count(occup) |&gt;\n  print(n = 15)\n\n# A tibble: 209 × 2\n   occup             n\n   &lt;chr&gt;         &lt;int&gt;\n 1 at school         1\n 2 baker            15\n 3 barber            1\n 4 black smith      10\n 5 blacksmith        1\n 6 bleacher         13\n 7 block maker       1\n 8 block print       1\n 9 block printer     1\n10 boat builde       1\n11 boatman           7\n12 boatyard          1\n13 boiler make       3\n14 boiler maker      1\n15 boilermaker       2\n# ℹ 194 more rows\n\n\nAs the previous example using occupations show, frequency tables are not that useful when the variable contains a large number of categories. Also, by default, count() reports the different categories in alphabetical order which is often not particularly useful. In such case, it is better to present the categories according to their relative importance in order to quickly identify the most important categories. This is achieved by introducing the option sort and set it up as TRUE. The results indicate that the most common occupation was “labourer”, followed by “prostitute” and the rest. We also have 22 prisoners whose occupation was not recorded. You may wonder whether they did not want to report it or they did not have one. The same output can be achieved using the function arrange(desc(n)), which enables presenting the data in descending order based on the column n.\n\ndata |&gt;  \n  count(occup, sort = TRUE)\n\n# A tibble: 209 × 2\n   occup            n\n   &lt;chr&gt;        &lt;int&gt;\n 1 labourer       181\n 2 prostitute      81\n 3 weaver          40\n 4 carter          28\n 5 hawker          28\n 6 miner           28\n 7 house keeper    23\n 8 seaman          22\n 9 &lt;NA&gt;            22\n10 house keepe     16\n# ℹ 199 more rows\n\n\nThese simple descriptions of the data not only allow identifying what is typical, but also what is rare. In this regard, we can use arrange() to report those occupations that are less common among prisoners. Note that, by default, arrange() sorts the data in ascending order, so this command only needs the name of the field that serves to sort the data in ascending order (in the previous examples, we explicitly indicated that we wanted the data to be presented in descending order using the desc() argument). As seen below, it is also possible to extend the pipe (|&gt;) and use print() to report more categories if needed.\n\ndata |&gt;  \n  count(occup) |&gt; \n  arrange(n) |&gt; \n  print(n=15) \n\n# A tibble: 209 × 2\n   occup              n\n   &lt;chr&gt;          &lt;int&gt;\n 1 at school          1\n 2 barber             1\n 3 blacksmith         1\n 4 block maker        1\n 5 block print        1\n 6 block printer      1\n 7 boat builde        1\n 8 boatyard           1\n 9 boiler maker       1\n10 boot binder        1\n11 brick maker        1\n12 brothel keeper     1\n13 butcher            1\n14 cab driver         1\n15 cabinet making     1\n# ℹ 194 more rows\n\n\nLikewise, apart from the absolute number of observations falling in each category, it is useful to report the relative frequency. Frequency tables in fact routinely report both values. As explained above, count() effectively transforms the paisley data set into something different by summarising the info contained in a particular field. If we want to report the relative frequency, we need to add another column computing it. Therefore, we need to create a new variable using the command mutate() and instruct R how to populate that new field. As you will soon find out, mutate() is an extremely important command. In the example below, it takes the table created by count() as input and creates a new variable named prop (or something else; you decide which name you give to the new field). The value of the new field is something that the researcher sets up. In this case, we want to compute the relative frequency of each occupational category, that is, the number of cases falling into each category divided by the total number of observations. While the first value is the result of the function count() as reported in the field n, the second value can be retrieved by using the appropriate function. The operation n/sum(n) will therefore achieve what you need.\n\ndata |&gt;  \n  count(occup, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) \n\n# A tibble: 209 × 3\n   occup            n  prop\n   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt;\n 1 labourer       181 0.181\n 2 prostitute      81 0.081\n 3 weaver          40 0.04 \n 4 carter          28 0.028\n 5 hawker          28 0.028\n 6 miner           28 0.028\n 7 house keeper    23 0.023\n 8 seaman          22 0.022\n 9 &lt;NA&gt;            22 0.022\n10 house keepe     16 0.016\n# ℹ 199 more rows\n\n\nCrucially, you can narrow your analysis by focusing on particular subsamples of the data (or excluding outliers). This is achieved using filter() which allows “filtering” the data set according to the conditions that you specify. Imagine, for instance, that you are interested in knowing the educational background of the female prisoners. By specifying that the variable sex should be equal to “female”, filter() restricts the analysis to those observations (rows) fulfilling this condition. The results below show that, while many women in the Paisley data are either “illiterate” or “read a little”, only 7 “read & write well”.\n\ndata |&gt; \n  filter(sex==\"female\") |&gt;   \n  count(literacy, sort = TRUE) |&gt;              \n  mutate(prop = n/sum(n)) \n\n# A tibble: 10 × 3\n   literacy                   n    prop\n   &lt;chr&gt;                  &lt;int&gt;   &lt;dbl&gt;\n 1 read a little             94 0.331  \n 2 illiterate                79 0.278  \n 3 read & write a little     61 0.215  \n 4 &lt;NA&gt;                      18 0.0634 \n 5 cannot write              11 0.0387 \n 6 read & write well          7 0.0246 \n 7 read & write tolerably     5 0.0176 \n 8 read well                  4 0.0141 \n 9 read tolerably             3 0.0106 \n10 superior education         2 0.00704"
  },
  {
    "objectID": "paisley.html#numerical-variables",
    "href": "paisley.html#numerical-variables",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Numerical variables",
    "text": "Numerical variables\nLet’s now explore variables that are expressed using numerical values. In the Paisley dataset, we only have three numerical variables: age, height and weight. Their specific properties advice to analyse them using a wider set of statistics. In this regard, simply reporting frequencies is often not very useful, especially when these variables include a large range of values. See, for instance, what happens when construct frequency table using now the variable age. The results report the number of observations falling in each category: 2 prisoners aged 9 years, 3 aged 10, etc. (we have also used mutate() to create an additional column with the relative frequency expressed as percentages). For questions of space, we just report the first 10 rows but the point is clear. You can display the full table, containing 62 rows, by adding the option print(n=Inf). The full table obviously provides interesting insights but it is simply too large for being useful as an interpretative tool.\n\ndata |&gt;  \n  count(age) |&gt;              \n  mutate(perc = 100*n/sum(n))\n\n# A tibble: 62 × 3\n     age     n  perc\n   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1     9     2   0.2\n 2    10     3   0.3\n 3    11     4   0.4\n 4    12     9   0.9\n 5    13     6   0.6\n 6    14    18   1.8\n 7    15    17   1.7\n 8    16    18   1.8\n 9    17    30   3  \n10    18    48   4.8\n# ℹ 52 more rows\n\n\nA useful way of exploring and reporting numerical variables is by using a histogram. This type of plot provides a visual representation of the distribution of values of the variable that we are analysing. The command ggplot() easily allows constructing histograms. You first need to indicate which variable you want to depict in the x-axis and then decide which type of plot you want. We have also included a line of code that establishes how the x-axis is labelled (in this case in multiples of 10 starting at age 0 and ending at age 90). As with any other function, we can also first use filter() to narrow down the analysis to particular subsamples of our data (i.e. gender, country of birth, etc.).\n\ndata |&gt;   \n  ggplot(aes(x = age)) +\n    geom_histogram(binwidth = 5) +\n    scale_x_continuous(breaks = seq(0, 90, 10))\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nHistograms present visual representations of the distribution of numerical variables. They are extremely useful because they provide an all-encompassing view of the data under analysis. However, it is also important to report specific values that help accurately describing the distribution. Descriptive statistics reduce complex distributions to more simple and intelligible numbers, thus making comparing distributions easier. The mean (or the average) is the most popular descriptive statistic but, depending on the researcher’s aim, other statistics may prove even more important.\nThe command summarise() allows computing all these descriptive statistics, also known as summary statistics. The following, for instance, compute the prisoners’ average age. This computation will generate a variable (we have assigned it here the name mean_age but you can choose any other name) that is equal to the function we specify. In this case, we want to compute the average, so we use the function mean(). Notice that we are also including the parameter na.rm = TRUE, which stands for “NA remove”, in order to exclude missing values from the computations and ensure accurate results (otherwise it results in NA because it cannot be computed; you can check the results removing that condition). As shown below, our prisoners are relatively young, on average.\n\ndata |&gt; \n  summarize(mean_age = mean(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_age\n     &lt;dbl&gt;\n1     29.6\n\n\nCalling different functions within summarize() allows calculating other statistics. In the example below, we also compute the number of observations with information of age and the minimun and maximum values. We are using the functions sum(), mean(), min() and max() to compute the corresponding statistics.1 This exercise tells us that the average age (mean) of the 999 prisoners reporting age (obs) is 29.6 years. It also indicates that there is at least a prisoner as young as 9 years old (min) and at least another one as old as 89 (max).\n\ndata |&gt; \n  summarize(\n    obs = sum(!is.na(age)),\n    mean = mean(age, na.rm = TRUE), \n    min = min(age, na.rm = TRUE),\n    max = max(age, na.rm = TRUE)) \n\n# A tibble: 1 × 4\n    obs  mean   min   max\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   999  29.6     9    89"
  },
  {
    "objectID": "paisley.html#bivariate-statistics",
    "href": "paisley.html#bivariate-statistics",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Bivariate statistics",
    "text": "Bivariate statistics\nHistorians and other social scientists routinely base their narratives on comparisons across different dimensions (gender, age, socio-economic groups, regions, etc.). Let’s then try to describe two variables simultaneously.\nImagine, for instance, that we want to provide information on age and adult weight. Notice that weight is expressed in pounds. For those of us more used to deal with the metrical system, it is advisable to change the unit of measurement. This can be done by creating another variable (weight_kg) that makes the conversion (1 kgs. = 0.453592 pounds; the operator &lt;- instruct R to modify the object data accordingly).\n\ndata &lt;- data |&gt;\n  mutate(weight_kg = 0.453592*weight)\n\nComing back to our main purpose of simultaneously looking at age and weight, it would not make sense to report a table listing the average height for each age (i.e. 20, 21, 22, etc.), so we first use mutate() to create a variable grouping age into different class intervals (in 10-year cohorts starting at age 9) and then compute the average for each group using group_by() and summarise(). Given that we want to focus on adult weight, we are restricting the analysis to those age 20 or older. The results clearly show that older prisoners tend to have lower weights on average.2\n\ndata |&gt; \n  filter(age&gt;=20) |&gt;\n  mutate(age_class = cut(age, breaks = seq(19, 89, 10))) |&gt;  \n  group_by(age_class) |&gt;\n  summarise(obs = sum(!is.na(weight_kg)),\n            mean_weight = mean(weight_kg, na.rm = TRUE))\n\n# A tibble: 7 × 3\n  age_class   obs mean_weight\n  &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;\n1 (19,29]     331        63.7\n2 (29,39]     167        62.4\n3 (39,49]     104        62.6\n4 (49,59]      49        59.7\n5 (59,69]      21        57.2\n6 (69,79]       4        55.1\n7 (79,89]       1        44.5\n\n\nThe same information could be depicted using a line graph by using geom_line() in ggplot() and indicating to plot the variables we just computed (age and mean_weight) in the x- and y-axes, respectively. Notice that instead of grouping ages into 10-year intervals, we are using the full distribution of ages (the trade-off is the higher variation arising from the low number of observations for each individual age). Contrary to long tables containing all the categories, plots allow presenting all the information in a more concise way. The graph below tracks the relationship between these two dimensions and clearly suggests that getting older is associated with losing weight (or, at least, that the older prisoners in our data set are lighter than the younger ones).\n\ndata |&gt; \n  filter(age&gt;=20) |&gt;\n  group_by(age) |&gt;\n  summarise(mean_weight = mean(weight_kg, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = age, y = mean_weight, group = 1)) +\n    geom_line()\n\n\n\n\n\n\n\n\nWe could obviously refine our analysis to take into account other dimensions of our data. Using filter(), for instance, would allow to focus on particular subsamples of our data (i.e. males or females) or exclude outliers, that is, observations with extreme values that may distort our results (i.e. very old prisoners; see chapter X for a more detailed discussion of outliers). We can also go beyond the previous graph and simultaneously consider other dimensions in the visualisation itself. The code below replicates the previous plot but distinguishing by sex and excluding the prisoner who are really old (80+). This exercise not only makes clear that women are in general lighter, but also that they seem to lose weight more rapidly than men: the slope of the line tracking the relationship between age and weight is steeper.\n\ndata |&gt;\n  filter(age&gt;=20 & age&lt;80) |&gt;\n  group_by(age, sex) |&gt;\n  summarise(mean_weight = mean(weight_kg, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = age, y = mean_weight, color = sex)) +\n    geom_point() +\n    geom_line()\n\n\n\n\n\n\n\n\nWe don’t have time for more. This is though just the very tip of the iceberg. Quantitative tools allow extracting information from historical sources in a powerful way, regardless whether the information is numerical or qualitative."
  },
  {
    "objectID": "paisley.html#footnotes",
    "href": "paisley.html#footnotes",
    "title": "Case-Study 1: The Paisley Prison dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is important to notice the comma after each function since it allows for computing additional statistics (the last one thus does not need the comma). Note also that the brackets need to be balanced. Likewise, the function requesting the number of observations (sum()) is structured differently than the rest. This is because we need to count the number of prisoners reporting age (the option !is.na effectively ask to only consider those observations who are not reporting a missing value in the variable age). If we had used the code sum(age), na.rm = TRUE as in the other command lines, we would have obtained the sum of all the prisoners’ ages (29,562 years; you can test it yourself).↩︎\nThere are only a few very really old prisoners, so their average weight is also very much influenced by chance.↩︎"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "The course revolves around three main themes:\n\nClassic Statistics: Descriptive statistics, uncertainty, correlation and regression analysis.\nComputational Text Analysis: Digital corpus management, word frequency and dictionary methods, text classification, topic models and sentiment analysis.\nVisualising spatial information using GIS (Geographic Information Systems) tools.\nSocial Network Analysis: visualising social networks, global and local metrics and community detection."
  },
  {
    "objectID": "further-readings.html",
    "href": "further-readings.html",
    "title": "Further reading",
    "section": "",
    "text": "For additional background on quantification in history, check the texts below:\n\nGraham, Shawn, Milligan, Ian, Weingart, Scott and Martin, Kim (2022), The joys of big data for historians, in Exploring Big Historical Data. The Historians Macroscope (World Scientific; 2nd Edition), pp. 1-34.\nFourie, Johan (2023), Quantitative history and uncharted people, in Quantitative history and uncharted people. Case studies from the South African Past (Bloomsbury), pp. 1-32.\nLemercier, Claire and Zalc, Claire (2021), Back to the sources. Practicing and teaching quantitative history in the 2020s, Capitalism. A Journal of History and Economics, vol. 2, no. 2, pp. 473-508.\nBlaxill, Luke (2023), Why do historians ignore digital analysis. Bring on the Luddites, The Political Quarterly, vol. 94, no. 2, pp. 279-289.\nJockers, Matthew L. (2013), Macroanalysis. Foundation, in Macroanalysis. Digital Methods and Literary History (University of Illinois Press), pp. 3-32."
  }
]